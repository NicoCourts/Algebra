\documentclass[12pt]{article}

\usepackage{setspace}

\usepackage{amsmath, amsfonts, amssymb, graphicx, color, fancyhdr, lipsum, scalerel, stackengine, mathrsfs, tikz-cd, mdframed, enumitem, framed, adjustbox, bm, upgreek, x	color}
\usepackage[framed,thmmarks]{ntheorem}
\usepackage[mathscr]{euscript}

%set up theorem/definition/etc envs
%Problems will be created using their own counter and style
\theoreminframepreskip{0pt}
\theoreminframepostskip{0pt}
\newframedtheorem{prob}{Problem}[section]
\newenvironment{hwprob}[1]
{\renewcommand{\theprob}{#1}%
 \addtocounter{thm}{-1}%
 \begin{prob}}
{\end{prob}}

%solution template
\theoremstyle{nonumberbreak}
\theoremindent0.5cm
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath\spadesuit}
\newtheorem{sol}{Solution}

%Theorems, Lemmas, and Corollaries
\theoremstyle{changebreak}
\theoremseparator{}
\theoremsymbol{}
\theoremindent0.5cm
\theoremheaderfont{\color{violet}\bfseries} 

\newtheorem{thm}{Theorem}[subsection]
\theoremheaderfont{\bfseries}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

%Create a new env that references a theorem and creates a 'primed' version
%Note this can be used recursively to get double, triple, etc primes
\newenvironment{thmprime}[1]
  {\renewcommand{\thethm}{\ref{#1}$'$}%
   \addtocounter{thm}{-1}%
   \begin{thm}}
  {\end{thm}}

\setlength\fboxsep{15pt}

%Shade definitions
\theoremindent0cm
\theoremheaderfont{\normalfont\bfseries} 
\def\theoremframecommand{\colorbox[rgb]{.9,.8,1}}
\newshadedtheorem{defn}[thm]{Definition}

%Shade definitions
\theoremindent0cm
\theoremheaderfont{\normalfont\bfseries} 
\def\theoremframecommand{\colorbox[rgb]{.9,.9,.9}}
\newshadedtheorem{ex}[thm]{Example}

%Man, that's really good! Let's use the same thing for definitons.
\newenvironment{defprime}[1]
  {\renewcommand{\thethm}{\ref{#1}$'$}%
   \addtocounter{thm}{-1}%
   \begin{defn}}
  {\end{defn}}

%proofs
\theoremstyle{nonumberbreak}
\theoremindent0.5cm
\theoremheaderfont{\sc}
\theoremseparator{}
\theoremsymbol{\ensuremath\spadesuit}
\newtheorem{prf}{Proof}
\newtheorem{conj}{Conjecture}

%remarks
\theoremstyle{change}
\theoremindent0.5cm
\theoremheaderfont{\sc}
\theoremseparator{:}
\theoremsymbol{}
\newtheorem{rmk}[thm]{Remark}

%Replacement for the old geometry package
\usepackage{fullpage}

%Put page breaks before each part
\let\oldpart\part%
\renewcommand{\part}{\clearpage\oldpart}%

%Center each figure by default
\makeatletter
\g@addto@macro\@floatboxreset{\centering}
\makeatother

%header stuff
\setlength{\headsep}{24pt}  % space between header and text
\pagestyle{fancy}     % set pagestyle for document
\lhead{Notes on Lie Algebras} % put text in header (left side)
\rhead{Nico Courts} % put text in header (right side)
\cfoot{\itshape p. \thepage}
\setlength{\headheight}{15pt}
\allowdisplaybreaks

%Set of Integers
\newcommand*{\Z}{
\mathbb{Z}
}
%Set of Natural Numbers
\newcommand*{\N}{
\mathbb{N}
}
%Set of Real Numbers
\newcommand*{\R}{
\mathbb{R}
}
%Set of Complex Numbers
\newcommand*{\C}{
\mathbb{C}
}
%Rationals
\newcommand*{\Q}{
\mathbb{Q}
}

%Section break
\newcommand*{\brk}{
\rule{2in}{.1pt}
}

\DeclareMathOperator{\Aut}{Aut}

%raise that Chi!
\DeclareRobustCommand{\Chi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} 

%Image
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}

%Coker
\DeclareMathOperator{\coker}{coker}

%characteristic
\DeclareMathOperator{\ch}{char}

%rank
\DeclareMathOperator{\rank}{rank}

%identity map
\DeclareMathOperator{\id}{id}

%some nice shortcuts
\DeclareMathOperator{\calB}{\mathcal{B}}

%Lie algebra stuff
\DeclareMathOperator{\gl}{\mathfrak{gl}}
\let\sl\relax
\DeclareMathOperator{\sl}{\mathfrak{sl}}
\DeclareMathOperator{\so}{\mathfrak{so}}
\let\sp\relax
\DeclareMathOperator{\sp}{\mathfrak{sp}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Rad}{Rad}
\let\t\relax
\DeclareMathOperator{\t}{\mathfrak{t}}
\DeclareMathOperator{\n}{\mathfrak{n}}
\DeclareMathOperator{\h}{\mathfrak{h}}
\DeclareMathOperator{\g}{\mathfrak{g}}
\DeclareMathOperator{\diag}{diag}

%fix tilde
\let\tilde\relax
\newcommand*{\tilde}[1]{\widetilde{#1}}

% Enumerate will automatically use letters (e.g. part a,b,c,...)
\setenumerate[0]{label=(\alph*)}

\begin{document}
%make the title page
\title{Lie Algebras and Groups\vspace{-1ex}}
\author{A course by: Monty McGovern\\
Notes by: Nico Courts}
\date{Winter 2019}
\maketitle

\renewcommand{\abstractname}{Introduction}
\begin{abstract}
	These notes are my best attempt at following along with our \textit{Math 508 --
	Lie Algebras} course at UW. This is my first time trying to type my 
	notes on-the-fly in class so we'll see how well this goes. The course reference
	is Humphreys' \textit{Introduction to Lie Algebras and Representation Theory.}

	The course description follows:
	
	\brk

	This is the second course in the Algebraic Structures sequence. I will classify 
	finite-dimensional complex semisimple Lie algebras, also proving some structural 
	results on general Lie algebras along the way. Although one usually first 
	encounters Lie algebras in a manifolds course, the treatment (following the text) 
	will be entirely algebraic.
\end{abstract}

\section{January 7, 2019}
The homework is posted on Monty's website. :) 
\subsection{Lie algebras}

This course will be studying Lie algebras, but as opposed to their treatment in manifolds, 
we will be studying them from a purely algebraic point of view. The book (Humphreys)
actually never defines a Lie group.

\begin{defn}
	A \textbf{Lie Algebra} $L$ or $\mathfrak{g}$ over a field $k$ is a $k$-vector space (usually f.d.)
	along with a \textit{bracket operation} $[vw]:L\times L\to L$ such that $[\cdot\cdot]$ is 
	\begin{itemize}
		\item anticommutative,
		\item bilinear,
		\item $[x[yz]]=[[xy]z]+[y[xz]]$
	\end{itemize}
\end{defn}

\begin{rmk}
	The last principle above is actually equivalent to the \textit{Jacobi identity:}
	\[[x[yz]]+[y[xz]]+[z[xy]].\]
	This follows from bilinearity and anticommutativity of the bracket.
\end{rmk}
The most natural place for these to arise is as \textit{derivations} on an algebra!
\begin{defn}
	A \textbf{$k$-derivation} $d:A\to A$ on an algebra $A$ over $k$ is a $k$-linear map
	satisfying the Leibniz rule.
\end{defn}
\begin{rmk}
	Some key facts about derivations (for us):
	\begin{itemize}
		\item Given a fixed $a\in A$, the map $d_a$ sending $b\mapsto ab-ba$, the \textbf{commutator}
		$[ab]$ is a derivation.
		\item If $d,e$ are derivations, then so is $[de]=de-ed$, where $de$ is the \textit{composite}
		of $d$ and $e$ as opposed to the product.
	\end{itemize}
\end{rmk}

\subsection{Examples}

A main source of Lie algebras is (associative) algebras! \textit{Any associative $k$-algebra $A$}
becomes a Lie algebra over $k$, taking $[ab]=ab-ba.$ In particular, one obvious choice for $k$-algebra
is $M_n(k)=\gl_n(k)$, the (Lie) algebra of $n\times n$ matrices over $k$.

\textbf{Lie subalgebras} are what you'd expect (including closure under brackets). Notice
that if $L'\le L$, then they \textbf{must both be over the same field.}

If $L$ is a $k$-Lie algebra and $I\lhd L$ is an ideal of $L$, then the quotient space $L/I$ becomes a 
Lie algebra with $[x+I,y+I]=[xy]+I$ as the bracket.

A \textbf{Lie algebra homomorphism} is a map $\varphi:L\to L'$ such that $\varphi$ is $k$-linear
and $\varphi([xy])=[\varphi(x)\varphi(y)].$

We get the usual first isomorphism theorem $L/\ker\varphi\cong \varphi(L).$

\brk

Associative algebras are not the only source of Lie algebras, however! One example is 
$\sl(n,k)=\{n\times n\text{ matrices over } k\text{ with trace zero}\}$

Note that this is \textbf{not closed under product} since $\tr(AB)\ne\tr A\tr B$ but $\tr(AB)=\tr(BA)$
so $\tr(AB-BA)=\tr(AB)-\tr(BA)=0$.
\begin{defn}
	We call this algebra (or, in fact any subalgebra of $\gl(n,k)$) \textbf{linear}. Think 
	``Linear'' means ``of matrices.''
\end{defn}

We say that $\sl(n,k)$ has \textbf{type} $A_{n-1}$. Eventually we will see seven types
$A-G$ of semisimple Lie algebras. The shift in index will emerge later.

$\sl(n,k)$ is, in fact, a simple Lie algebra: for $k=\C$, $\sl(n,\C)$ has no ideals
apart from the trivial ones. 

\brk

Other non-associative examples include $k^n$ with a bilinear form $(\cdot,\cdot)$ which
is either symmetric or skew-symmetric and (in either case) is nondegenerate.
\begin{defn}
	$(\cdot,\cdot)$ is \textbf{nondegenerate} if the map $v\mapsto (v,\cdot)$ is injective. Equivalently
	there is no $v\in V$ such that $(v,w)=0$ for all $w\in V$.
\end{defn}

Given $V=k^n$ and a bilinear form on $V$, we can look at all $X\in \gl(n,k)=\gl(V)$ such that 
$(Xv,w)=(v,Xw)$. Then $X$ is \textbf{adjoint} with respect to the form. There is a similar definition for when
$X$ is \textbf{skew-adjoint.} One can check that 
$[XY]$ is skew-adjoint whenever both $X$ and $Y$ are.

\subsection{Generating (skew) symmetric forms}
It ends up that the dot product (which is a symmetric form) is misleadingly simple -- thus
we will look elsewhere.

If $M\in \gl(n,k)$ is symmetric, so that $M^t=M$, then $(v,w)=v^tMw$ is a symmetric. If 
instead $M$ is skew-symmetric, then the same definition yields a skew-symmetric form. 
This actually induces a one-to-one correspondence between matrices and forms.

In both cases, if $M$ is invertible, then the form will be nondegenerate. As a consequence, 
since skew-symmetric matrices are always singular in odd dimensions, we see that 
nondegenerate skew-symmetric forms (over $\ch k\ne 2$ where the two families of forms
coincide) exist only in even dimensions.

\subsection{A peek at classifications}
If we have a nondegenerate symmetric form where $n=2m$ is going to give us an algebra
of type $D_m$. If $n=2m+1$, then it is of type $B_m$. Both of these cases are called
\textbf{orthogonal.}

If instead we have a skew-symmetric form and $n=2m$, then this is of type $C_m$, and we 
call this algebra \textbf{symplectic.}

\brk

We will make a particular choice for our matrix $M$ and then study the resulting
Lie algebras in much more detail next time. The choices will be: 
\begin{itemize}
	\item For type $D_m$:
	\[\begin{pmatrix}
		0 & I_m\\
		I_m & 0
	\end{pmatrix}\]
	\item For type $C_m$:
	\[\begin{pmatrix}
		0 & -I_m\\
		I_m & 0
	\end{pmatrix}\]
	\item For type $B_m$:
	\[\begin{pmatrix}
		1 & 0 & 0\\
		0 & 0 & I_m\\
		0 & I_m & 0
	\end{pmatrix}\]
\end{itemize}

\section{January 9, 2019}
Today we will be looking deeply into the stucture of linear Lie algebras of types A-D.

\subsection{Linear Lie Algebras Revisited}
Recall that the \textbf{matrix unit} $e_{ij}$ is the matrix with 1 in the $(i,j)$ entry and 
zero elsewhere. And then $e_{ij}e_{kl}=\delta_{jk}e_{il}$ and furthermore
\[[e_{ij}e_{kl}]=\delta_{jk}e_{il}-\delta_{li}e_{kj}.\]
This is especially nice when $j=i$, called the \textbf{diagonal matrix unit}.

Then we look at type $A_{n-1}$ ($\sl(n,k)$). Let $D$ be the set of diagonal matrices in this algebra.
Notice the dimension is $n-1$ since then $n^{th}$ term on the diagonal is determined as the
negative of the sum of the other $n-1$ terms. Let $A=\operatorname{diag}(d_1,\dots,d_n)$. Then consider the eigenvalues
associated with $e_{ij}$: 
\[[Ae_{ij}]=(d_i-d_j)e_{ij}=(E_i-E_j)Ae_{ij}\]
where $E_i$ is the linear functional selecting the $i^{th}$ entry in $A$. Moreover, $D$ is abelian as a Lie algebra, so $D$
acts diagonally on $L=\sl(n,k)$ by commutation with eigenvalues $E_i-E_j$ and zero
for $1\le i,j\le n$ and $i\ne j$.

In the other classical cases B-D, there is always a matrix $M$ which defines the form $(v,w)=v^tMw$ 
as we saw yesterday. In all three cases, the Lie algebra exists consists of all skew-adjoint matrices
$X$ relative to the form. $B_m=\mathfrak{so}(2m+1,k)$ as well as $D_m=\mathfrak{so}(2m,k)$ and $C_m=\mathfrak{sp}(2m,k)$.

This condition translates to the form of matrices in the above Lie groups and the condition is always $Mx=-x^tM$ in all cases.

Type B:
\[\begin{pmatrix}
	0 & b_1 & b_2\\
	c_1 & m & n\\
	c_2 & p & q
\end{pmatrix}\]
where $c_1=-b_2^t$, $c_2=-b_1^t$, $q=-m^t$, $n^t=-n$ and $p^t=-p$.

Type C:
\[\begin{pmatrix}
	m& n\\ p & q
\end{pmatrix}\]
where $n^t=n, p^t=p,$ and $m^t=-q$.

Type D:
\[\begin{pmatrix}
	m& n\\ p & q
\end{pmatrix}\]
where $n^t=-n, p^t=-p,$ and $m^t=-q$.

Looking at the eigenvalues of elements of $D$ associated to vectors $e_{ij}-e_{m+i,m+j}$. Look at photos

Using a similar analysis, we can look at types $B$ and $D$. We define the functions $E_i$ similarly on the space of diagonalmatrices and gives a rise to the following collection of linear functions:
in $B_m:$ $\pm E_i$ and $\pm(E_i\pm E_j)$ and $D_m$ gives us $\pm(E_i\pm E_j)$.

This collection of functions in each case is called the \textbf{root system of the Lie algebra},
Then any complex simple finite-dimensional Lie algebra is classified by its root system.
The (perhaps surprising) fact is that this already encompasses all but finitely many of these
things up to ismorphism: the classical Lie algebras. Eventually we will learn more about the
\textbf{exceptional Lie groups.}

This section was a little hard to follow and the handling in Humphreys is easier to follow, 
but delays speaking about root systems and actually deriving the eigenfunctions (is that the right word?)
until significantly later. Monty seemed to think it was acceptable to delay the understanding of this a bit.

\subsection{Derivations and $\exp$}
Look an an arbitrary Lie algebra over a field $k$ where $\ch k=0$ (which we will mostly be assuming from here on)
. Let $\delta$ be a derivation of $L$, so that $[\delta x,y]+[x\delta y]=\delta[x,y]$. Assume that $\delta$ is nilpotent.

Then the ``power series'' (polynomial) is
\[\exp\delta=\sum_{i>0}\frac{\delta^i}{i!}\]
\begin{prob}
	This is a good exercise to go through: Check that
	\[[(\exp\delta)x,(\exp \delta)y)=[xy]\]
	for each $x,y\in L$
\end{prob}

\begin{rmk}
	This actually shows that $\exp\delta$ is an automorphism of $L$. Furthermore you'd find that
	\[(\exp\delta)(\exp(-\delta))=1.\]
\end{rmk}

What if $k=\R$ or $\C$? Then the power series (even when $\delta$ is not nilpotent!) always converges
and defines an automorphism as before.

\begin{lem}
	For all \textbf{complex} semisimple Lie algebras $L$ it turns out that the group
	generated by $\exp\ad x$ ($\ad x(y)=[xy]$) coincides with the group generated by all 
	nilpotent $\ad x$.
\end{lem}
\subsection{Adjoint group}
The last thing for today is to define the adjoint group:
\begin{defn}
	Let $L$ be a Lie algebra, then 
	\[\operatorname{Int}(L)=\exp\ad L\]
	is the \textbf{Adjoint group} of $L$. It is a subgroup (so we believe) of the Lie
	Group associated to $L$.
\end{defn}
\begin{rmk}
	Actually after talking to Monty, $\exp\ad L$ \textit{is} (essentially) the Lie group
	corresponding to $L$. Such a group is not unique, however.
\end{rmk}

Some examples of adjoint groups:
\begin{itemize}
	\item If $L=\sl(n,\C)$, then $\operatorname{Int}(L)=PSL(n,\C)=SL(n,\C)/\text{center}$
	\item If $L=\so(n,\C)$, then $\operatorname{Int}(L)=PSO(n,\C)$
	\item If $L=\sp(2n,\C)$ then $\operatorname{Int}(L)=PSp(2n,\C)$.
\end{itemize}

%%%% HW 1 - Due Jan 18 %%%%
\newpage

\section*{HW 1 -- Due January 18}
Do problems 1.9, 2.1, 3.8. 3.9, and 4.3.

\begin{hwprob}{1.9}
	When $\ch k=0$, show that each classical Lie algebra $A_l,B_l, C_l$ and $D_l$ satisfies $[LL]=L$. (This shows again that each algebra consists of trace zero matrices.)
\end{hwprob}
\begin{sol}
	\subsubsection*{Type $A_k$:}
	Recall that a basis for $A_l$ is the collection $\mathcal{B}=\{e_{ij}|i\ne j\}\cup\{e_{ii}-e_{i+1,i+1}|1\le i\le l\}$. Using linearity of the bracket,
	it then suffices to check that $\calB$ is contained in the algebra generated by $[\calB \calB]$. To see this, we use that
	\[[e_{ij}e_{kl}]=\delta_{jk}e_{il}-\delta_{li}e_{kj}.\]

	First, notice we have
	\[[e_{i,i+1}e_{i+1,i}]=e_{ii}-e_{i+1,i+1},\]
	so it suffices to show we also have the $e_{ij}$ for $i\ne j$.
	This is clear whenever $k\ge 2$ (so that we have three columns in our matrices). Let $i\ne j$ be arbitrary
	and $l$ be a (the) third index. Then 
	\[e_{il}e_{lj}]=e_{ij}.\]

	If $k=1,$ then 
	\[e_{21},e_{11}-e_{22}]=e_{21}+e_{21}=2e_{21}\]
	and so $e_{21}$ is in the space generated by $[\calB \calB]$ and an analagous proof
	gives us the rest of the basis.

	\subsubsection*{Type $B_k$:}
	This one is getting a bit long and is low on my priority list so might not be finished.
\end{sol}

\begin{hwprob}{2.1}
	Prove that the set of all inner derivations $\ad x$ for $x\in L$ is an ideal of $\Der L$.
\end{hwprob}
\begin{sol}
	Let $I=\ad L$. $I\subseteq \Der L$ since $\ad x$ is a derivation. This is a subspace since $\ad 0=0$ and since the bracket is bilinear
	\[\ad x(ay+bz)=[x,ay+bz]=[x,ay]+[x,bz]=a[xy]+b[xz]=a\ad x(y)+b\ad x(z).\]

	Take any $x\in I$ and let $f\in\Der L$ be arbitrary. But then for $y,z\in L$
	\begin{align*}
		[\ad x,f](yz)&=(\ad x f-f\ad x)(yz)\\
		&=[x,f(yz)]-f[x,yz]\\
		&=[x,f(y)z+yf(z)]-f([x,y]z+y[x,z])\\
		&=[x,f(y)z]+[x,yf(z)]-f([x,y]z)-f(y[x,z])\\
		&=[x,f(y)]z+f(y)[x,z]+[xy]f(z)+y[x,f(z)]-f([x,y])z\\
		&\hspace{2in} -[x,y]f(z)-f(y)[x,z]-yf[x,z]\\
		&=([x,f(y)]-f[x,y])z+y([x,f(z)]-f[x,z])\\
		&=([\ad x, f](y))z+y([\ad x,f](z))
	\end{align*}
	so $[IL]\subseteq I$ whence $I$ is an ideal of $L$.
\end{sol}

\begin{hwprob}{3.8}
	Let $L$ be nilpotent. Prove that $L$ has an ideal of codimension 1.
\end{hwprob}
\begin{sol}
	Assume that $L$ is nilpotent. That is, defining $L^0=L$, and by induction $L^i=[LL^{i-1}]$, this 
	means that $L^n=0$. 
	
	We proceed by induction on $\dim L$: for the base case, if $L=0$ the result doesn't really mean anything,
	and when $\dim L=1$, the trivial subspace has codimension 1.

	Assume now that $\dim L=i$ and that every Lie algebra of dimension less than $i$ has a codimension 1 ideal. 
	Then we have two subcases to consider. If $L$ is commutative then
	every vector subspace is an ideal, so take a codimension one subspace and we are finished.
	
	Finally assume $[LL]\ne 0$ and consider the quotient $Q=L/[LL]$. By 
	our assumptions (and since $L$ is nilpotent $L\ne[LL]$) we get that
	\[0<\dim Q<\dim L\]
	and that, in fact, by rank-nullity
	\[\dim Q = \dim L - \dim[LL].\]
	But then by the induction hypothesis there is an ideal $J\lhd Q$ with dimension $\dim Q - 1$
	and pulling this back through the quotient yields the ideal $\bar J\lhd L$ that has dimension $\dim Q-1+\dim[LL]=\dim L-1$. 
	This is our codimension one ideal and we complete the proof.
\end{sol}

\begin{hwprob}{3.9}
	Prove that every nilpotent Lie algebra L has an outer derivation (see 1.3) as follows: Write $L=K+Fx$
	for some ideal $K$ of codimension 1. Then $C_L(K)\ne 0$ (why?). Choose $n$ so that $C_L(K)\subseteq L^n$,
	$C_L(K)\not\subseteq L^{n+1}$ and let $z\in C_L(K)-L^{n+1}$. Then the linear map $\delta$ sending $K$ to zero
	and $x$ to $z$ is an outer derivation.
\end{hwprob}
\begin{sol}
	Using the result above, we can write every nilpotent $L$ as $K+Fx$ for some $K\lhd L$.
	Now since $K\le L$, and since $L$ is simple, for some $n$: $L^n=[L^{n-1}L^{n-1}]=0$,
	or equivalently every $n$-fold bracket in $L$ is zero.

	Assuming that $n$ is minimal, this means that one can construct a $(n-1)$-fold bracket
	\[x=[x_1[x_2[x_3\cdots[x_{n-2}x_{n-1}]\cdots]]]\]
	which is nonzero. But then $[xL]=0$ so in particular $[xK]=0$ whence $x\in C_L(K).$

	Therefore we can choose an $m$, as suggested, such that $C_L(K)\in L^m\setminus L^{m-1}$
	and let $\delta$ be the linear map sending $K$ to zero and $x$ to some $z\in C_L(K)\setminus L^{m+1}$.
	We will now establish this is an outer derivation.

	Let $a,b\in L$ be arbitrary with $a=k_a+l_ax$ and $b=k_b+l_bx$ and compute
	\begin{align*}
		\delta(ab) &= \delta(k_a+l_ax)(k_b+l_bx)\\
		&= \delta(k_ak_b+k_al_bx+l_axk_b+l_axl_bx)\\
		&=0+k_a\delta(l_bx)+\delta(l_ax)k_b+\delta(l_ax)l_bx+l_ax\delta(l_bx)\\
		&=(k_a+l_ax)\delta(l_bx)+\delta(l_ax)(k_b+l_bx)\\
		&=a\delta(k_b+l_bx)+\delta(k_a+l_ax)b\\
		&=a\delta(b)+ \delta(a)b
	\end{align*}
	so $\delta$ is a derivation. It remains to show it is not inner.

	Assume by contradiction that it were. Then $\delta =\ad w$ for some $w\in L$. But then notice
	that $[wK]=0$ so $w\in C_L(K)\subseteq L^m$. But then bracketing with one more thing puts us in $L^{m+1}$, so
	$[w,x]\in L^{m+1}$. But then
	\[[w,x]=\ad w(x)=\delta(x)=z\in L^{m+1}\]
	a contradiction. Thus $\delta$ is an outer derivation.
\end{sol}

\begin{hwprob}{4.3}
	This exercise illustrates the failure of Lie's theorem when $F$ is allowed to have positive characteristic.
	Consider the $p\times p$ matrices
	\[x=\begin{pmatrix} 
		0 & 1 & 0 & \cdots & 0\\
		0 & 0 & 1 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & 1\\
		1 & 0 & 0 & \cdots & 0
	\end{pmatrix}\quad y=\diag(0,1,\dots, p-1)\]
	Check that $[xy]=x$, hence that $x$ and $y$ span a two dimensional solvable subalgebra $L$ of $\gl(p,F)$. Verify that $x$ and $y$ have no common eigenvector.
\end{hwprob}
\begin{sol}
	Compute
	\[[xy]=\begin{pmatrix} 
		0 & 1 & 0 & \cdots & 0\\
		0 & 0 & 2 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & p-1\\
		0 & 0 & 0 & \cdots & 0
	\end{pmatrix}-\begin{pmatrix} 
		0 & 0 & 0 & \cdots & 0\\
		0 & 0 & 1 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & p-2\\
		p-1 & 0 & 0 & \cdots & 0
	\end{pmatrix}=\begin{pmatrix} 
		0 & 1 & 0 & \cdots & 0\\
		0 & 0 & 1 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & 1\\
		1 & 0 & 0 & \cdots & 0
	\end{pmatrix}=x\]
\end{sol}
Thus if $L$ is the vector space spanned by $x$ an $y$, since it is closed under bracket, 
it is a Lie subalgebra of $\gl(p,F)$. Now because of this computation it is easy to see that
$[LL]=Fx$, so $L^2=0$, whence $L$ is solvable.

Notice that multiplication by $x$ on the right acts by permuting the columns of a matrix so $x^p=\id$,
whence the eigenvalues of $x$ are roots of $x^p-1=(x-1)^p$ since $\ch F=p$. Let $v$ me any element of $L$ 
fixed by the action of $x$ via $\ad x$ on $L$. Then
\[bx=b[x,y]=[x,ax+by]=[x,v]=xv-vx=v\]
but then since the only eigenvalue can be 1, $v=x$. But $[y,x]=-x$, which cannot be a scalar multiple of $y$,
so $x$ and $y$ can have no common eigenvector.
\newpage
%%%% End HW 1 %%%%

\section{January 11, 2019}
A few more things about $\Int L$ in classical cases:

Begin with $n\times n$ matrices $M$ over $\R$ or $\C$. Given any such $M$, we have
\[e^M=\sum_{i>0}\frac{M^i}{i!}\]
always converges (that is, the series for each entry converges). Futhermore $e^Me^{-M}=I$.
\begin{rmk}
	Notice that over \textit{any} field over characteristic zero whenever $M$ is nilpotent 
	this still holds.
\end{rmk}

Now if $M$ is skew-adjoint with respect to a bilinear form $(\cdot,\cdot)$, then 
\[(M^iv,w)=(-1)^i(v,M^iw)\]
whence
\[(e^Mv, w)=(v,e^{-M}w)\]
and so 
\[(e^Mv,e^Mw)=(v,w)\]
for all $v,w\in\R^n$ or $\C^n$.

But notice that then $e^M$ is an \textbf{isometry of} or \textbf{preserves} the form. Moreover, 
if we set $A=\ad M$, then $A$ is given as the difference of two \textit{commuting} linear maps, 
left and right multiplication by $M$.

\brk

Using the binomial theorem: $e^AN=e^MNe^{-M}$. This, in a nutshell, is why the formulas for
$\Int L$ all involved modding out by the center, since conjugation by a scalar matrix is trivial! :)

\subsection{Combining Ideals}
Now we return to the case of general Lie algebras over arbitrary fields (no longer assuming $\ch k=0$). 
We have seen that a subspace $I\le L$ is an ideal if $[IL]\subseteq I$.

If $I,J\lhd L$, then $I+J$ is an ideal, as well as
\[[IJ]=\{\sum_1^n [x_iy_i]|x_i\in I, y_i\in J\}.\]

We also are sometimes interested in the direct sum of ideals $\oplus L_i$ where $[L_iL_j]=0$ when $i\ne j$.

\subsection{Examples}
$0$ and $L$ are ideals as well as the \textbf{center} $Z(L)$.

\subsection{Characterizing Lie Algebras as Linear}
We now want to realize many Lie algebras as (isomorphic to) linear ones. For now, note that the adjoint map $\ad$
is a Lie algebra homomorphism by the Jacobi identity. Then $\ker\ad=Z(L)$, so when $Z=0$, then $L$ is isomorphic to 
a linear Lie algebra. 

\begin{defn}
	If $Z(L)$, and the map $\ad:L\to L$ is an isomorphism, we say $L$ \textbf{acts faithfully}
	on itself (via $\ad$).
\end{defn}

\begin{defn}
	If we have a homomorphism $L\to\gl(n,k)$, then we call $k^n$ an $L$-\textbf{module}.
	We define the action of $L$ on $K^n$ through the homomorphism.
\end{defn}

Notice that $L$ is always an $L$ module over itself.

One can check, using the realization of $\sl(n,k)$ we saw last time that (when $\ch k\nmid n$)
that $\sl(n,k)$ is simple. This is actually the Lie algebra analogue of the fact that $M_n(K)$ is simple
as a ring over any field $K$.

\begin{rmk}
	Notice that if $\ch k |n$$,$ then \textit{any} scalar matrix $aI\in\sl(n,k)$
	so $Z(\sl(n,k))$ is nontrivial.
\end{rmk}

\brk

In any event you can look up the proof for $\sl(2,k)$ in the book. The thing to glean here is that
\[[xy]=h,\quad [hx]=2x,\quad [hy]=-2y\]
for $h=e_{11}-e_{22}$, $x=e_{12}$ and $y=e_{21}$. This will keep popping up throughout the course.

\subsection{Ideals of Linear Lie Algebras}
Many subalgebras of $\sl(n,k)$ or $\gl(n,k)$ have many ideals (which happen to be ring-theoretic ideals).

\begin{defn}
	Some notation:
	\[\t(n,k)=\{\text{upper-triangular matrices}\}\] 
	also 
	\[\n(n,k)=\{\text{strictly-upper-triangular matrices}\}\]
\end{defn}

\subsection{Derived Series}
For any Lie algebra, we can define
\begin{defn}
	The \textbf{derived series} of a Lie algebra $L$ is defined to be
	\[L^{(0)}=L,\quad L^{(1)}=[LL],\quad L^{(n+1)}=[L^{(n)}L^{(n)}]\]
	which form an ascending chain of ideals.
\end{defn}

\begin{defn}
	If $L^{(n)}=0$ for some $n$, and thus for all sufficiently large $n$, we call $L$ \textbf{solvable.}
\end{defn}
\begin{rmk}
	Note that actually there is a cooresponding definition for groups using commutators $[G,G]$.
	This was defined first and ported to Lie algebras. It actually happened in the reverse direction for:
\end{rmk}

\begin{defn}
	The \textbf{central series} of $L$ is 
	\[L^0=L,\quad L^1=[LL],\quad L^2=[L,L^1],\dots\]
	and if $L^n=0$ for some $n$, then we call $L$ \textbf{nilpotent.}
\end{defn}
\begin{rmk}
	Equivalently, we can say that $L$ is nilpotent if \textit{any $n$-fold bracket} in $L$ is zero.
\end{rmk}
\begin{rmk}
	Notice that every nilpotent Lie group is solvable, but the converse fails: $t(n)$ is solvable by not nilpotent.
\end{rmk}

\subsection{Examples of solvable algebras}
One can easily check that every ideal and homomorphic image of a solvable algebra is solvable. Therefore if $I$ and $J$ are solvable ideals of $L$, then 
so is $(I+J)/J\cong I/(I\cap J)$ is solvable, as is $I+J$. Furthermore if $I$ and $L/I$ are solvable, then $L$ is.

\begin{defn}
	$\Rad L$, the \textbf{radical of $L$}, is the unique largest solvable ideal of $L$.
\end{defn}
\begin{rmk}
	Any finite dimensional algebra has a radical (taken to be the sum of all solvable ideals in $L$).
\end{rmk}

Using this definition, we are able to give two different characterizations:
\begin{defn}[Semisimple Lie Algebra]\label{def-semisimple}
	$L$ is semisimple if $\Rad L=0$.
\end{defn}
\begin{defprime}{def-semisimple}[Semisimple Lie Algebra]
	$L$ is semisimple if it is a direct sum of simple Lie algebras.
\end{defprime}
This equivalence will not become clear until we learn about the Killing form, but at least it jives
with our experience with representation theory and semisimple modules.

\section{January 14, 2019}
I missed this day for an event. I will try to catch up if I have time.

\section{January 16, 2019}
We are continuing from last time when we were showing that as solvable Lie subalgebra of $\gl(n,k)$
(when $\ch k=0$ and $\bar k=k$) necessarily admits a vector $v\in k^n\setminus 0$ that is
a common eigenvector:
\[xv=\lambda(x)v\]
for some \textbf{linear} function $\lambda:L\to k$.

We argues (as in the parllel case of a Lie subalgebra of $\gl(n,k)$ consisting of nilpotent matrices
by induction on dimension and found the same result (except it is zero). In this case when $\dim L=1$,
since $k$ is algebraically closed, $L$ is spanned by asingle matrix $x$ and any eigenvector of $x$ does the trick.

Now assume that the result holds for solvable Lie algebras of dimension $<d$. Then since $L$ is sovable, we know
$[LL]\subsetneq L$. Let $I$ be any subspace containing $[LL]$ and properly contained in $L$ of codimension one.
Then $I$ is an ideal since $[LI]\subseteq[LL]\subseteq I$, so by the inductive hypothesis $I$ has a common eigenvector, 
so that there is a linear function $\lambda$ such that the \textbf{weight space}
\[V_\lambda=\{v\in k^n:xv=\lambda(x)v,\,\forall x\in I\}\]
is nonzero.

We know that $L=I+kx$ for some $x\in L$, so we must show that $x$ preserves $V_\lambda$: then any eigenvector
of $x$ on $V_\lambda$ will do the job. Given $v\in V_\lambda$, we must show that 
\[yxv=xyv+[yx]v=\lambda(y)xv+\lambda([yx])v\]
so we must show that $\lambda([xy])=0$.

Given $v\in V_\lambda$, look at hte powers $v,xv, x^2v,\dots$. Let $m$ be the least power such that the
$v,xv,\dots,x^mv$ are linearly independent and let $W_i$ be the span of the first $i$ of these.

Then one checks by induction for an $y\in I$ that $yx^iv=\lambda(y)x^iv\pmod{v,xv,\dots,x^iv}$
It follows that $y$ was in the subspace $W_m$ (oh no missed some stuff).

So the trave of $y$ on $W_m$ is $m\lambda(y)$. Now for any $y\in I$, the matrix $[xy]$ acts on $W_m$ ascendingthe commutator of two matrices having trace zero, whence $m\lambda([xy])=0$,
so since $\ch k=0$, $\lambda([xy])=0$, as desired.

\brk

As a consequence, given a solvable Lie subalgebra of $\gl(n,k)$ here is a chain of subspaces
\[0=V_0\le V_1\le\cdots\le V_n=k^n\]
or a \textbf{flag} such that $LV_i\subseteq V_i$ for all $i$: $L$ acts by upper triangular 
matrices with respect to a suitable basis.

Another way of putting this, using module language, is that given any \textbf{finite dimensional} module over
a solvable Lie algebra (over an algebraically closed characteristic zero field) has a one-dimensional submodules $N$.

Equivalently, the only finite-dimensional irreducible modules over a solvable Lie algebra are one-dimensional. We 
will use this later in the context of semisimple Lie algebras with large solvable subalgebra.

\begin{rmk}
	For the record, Lie's theorem \textit{definitely fails} when $k$ has positive characteristic.
	We see one in our homework, and another example comes from $\sl(2,k)$ where $\ch k=2.$

	We know that $\sl(2,k)$ has a basis: $(\begin{smallmatrix}
		1&0\\0&-1
	\end{smallmatrix})=I_2$, $x=e_{12}$, $y=e_{21}$. Here $[hx]=0=[hy]$ and $[xy]=h$. It is shown in the book
	and is easy to see that when $\ch k\ne 2$, $\sl(2,k)$ is simple, but here $\sl(2,k)$ is nilpotent and its
	bracket relations become $[xy]=z$, $[xz]=[yz]=0$.

	It ends up we can define (actually over any field) the \textbf{Heisenberg} Lie algebra, spanned
	by $x=e_{12},y=e_{23},z=e_{13}$ or more simply the $3\times 3$ strictly-upper-triangular matrices.

	The matrices in $\sl(2,k)$ for any $k$ have no common eigenvector, so our proof of Lie's theorem must
	fail somewhere? We indeed have an ideal $I$ of coimension one spanned by $h$ and $x$ and there is acommon eigenvector for $I$, 
	namely $e_1$. Then $I$ acts diagonally on $ke_1$ via the linear function sending $h$ to 1, $x$ to $1$. 
	Here $y$ plays the role of $x$ in the proof, but is outside the ideal. That is,
	\[\lambda[yx]=\lambda(h)\ne 0.\]
\end{rmk}

Returning to the good case of $k$, we can now apply our result on a linear solvable Lie algebras to arbitrary ones:
uGIven any solvable Lie algebra $L$, there is a flag $0=L_0\le L_1\le\cdots\le L_n=L$ of ideas of $L$
so that $[LL_i]\subseteq L_i$ Also $[LL]$ is nilpotent, since it acts on $L$ by strictly upper triangular matrices.

Both results fail for general solvable Lie algebras when the basefield is not nice. If a solvable Lie algebra $L$ 
\textbf{does} have a chain of ideals $0=L_0\le L_1\le\cdots\le L_n=L$ where $\dim L_i=i$, then we call $L$ \textbf{completely solvable.}

\subsection{Returning to Linear Algebra}
At this point we will take some time to return to an important idea from Linear algebra, namely the \textbf{Jordan canonical form}
of a square matrix. Recall Jordan blocks and similarity when eigenvalues lie in the field.

If $M$ is already in this form, then we can write $M=M_s+M_n$ where $M_s$ consists of the diagonal entries of each
block (zeroes elsewhere) and $M_n$ the off-diagonal entries. Then $M_s$ is diagonal (or for general $M$
diagonalizable), or \textbf{semisimple} and $M_n$ is nilpotent and $[M_sM_n]=0$ (they commute).

\begin{defn}
	$M=M_s+M_n$ is called the \textbf{Jordan decomposition of $M$.} We call $M_s$ and $M_n$ the \textbf{semisimple} 
	and \textbf{nilpotent} parts of $M$.
\end{defn}
\begin{rmk}
	Each are uniquely determined for $M$ and (and this is not generally proved in the first year 
	algebra sequence) each are equal to polynomials in $M$ with no constant term.x`
\end{rmk}

\section{January 18, 2019}
\begin{prob}
	The ``board problem'' for this week: Show that a semisimple Lie algebra over $\C$ is generated as a Lie
	algebra by two elements.
\end{prob}

\subsection{Returning to Jordan Decomposition}
Given an $n\times n$ over an algebraically closed field $k$ of any characteristic, let $T$ be the corresponding linear
transformation on $k^n$. We know from the proof of the Jordan canonical form that 
\[V=k^n=\oplus_{a\in k}\ker(T-aI)^n\]
the sum of the \textbf{generalized eigenspaces} of $T$ on $V$. We then use the CRT:
there is a polynomial $p(T)$ such that $p(T)=a\pmod (T-a)^n$ for all eigenvalues $a$ and
$p(T)=0\pmod{T}$.

This is clear since the eigenvalues are distinct, so that all moduli are pairwise relatively prime.
SO then $p(T)$ acts on $\ker(T-A)^n$ so it is \textbf{semisimple} (diagonal) and $T-p(T)$, acting by $T-a$ on
$\ker(T-a)^n$, acts nilpotently.

Hence $p(x)$ and $x-p(x)$ are polnomials in $x$ without constant term such that as matrices they commute
and $x$ is semisimple (diagonal) and $x-p(x)$ is nilpotent. Moreover, tere is only one way to write $x$ as $s+n$ where $s$ is semisimple, $n$ is nilpotent, and $[sn]=0$.
This is since given $x=s+n$ where $s,n$ both commute with $x$, so with $p(x)$ and $x-p(x)$, we get
\[p(x)-s+(p(x)-x)+n\]
(check this) with the left side the sum of \textbf{commuting} semisimple matrices is semisimple and
the right side is the sum of \textbf{commuting} nilpotent matrices whence nilpotent, but the only matrix
which is both semisimple and nilpotent is the zero matrix.
\begin{rmk}
	Notice that commutativity above is vital to the statement.
\end{rmk}

Why is this important? We want to study the representations of Lie algebras, even the linear ones, and we would like to know that the semisimple and nilpotent parts $x_s, x_n$ of
any $x$ lying in such an algebra \textbf{continue} to act semisimply and nilotently in the representation 
(and that the parts $x_s$ and $x_n$ lie in L if $x$ does, which is true when $L$ is semisimple).

We have already seen that if $d$ is diagonal, then 
<FILL THIS IN>

Now we want to head towards a criterion in terms of traces alone for a matrix to be nilpotent.
Eventually we want to determine when a Lie algebra is solvable.

\begin{lem}
	Suppose that $k$ is algebraically closed of characteristic zero and that $B\subseteq A$ are subspaces of $\gl(n,k)$ and that $x(\in M?)$ is a matrix such that 
	$\tr(xy)=0$ for all $y\in M=\{z\in\gl(n,k)|[zA]\subseteq B\}$. Then $x$ is nilpotent.
\end{lem}
\begin{prf}
	Write $x=s+n$, the Jordan decomposition. Since $x\in M, s,n\in M$ since $s$ and $n$ are polynomials in 
	$x$ without constant term. We must show that $s=0$ -- that all eigenvalues of $s$ are zero.

	Let $a_1,\dots,a_n$ be the eigenvalues of $s$. These all lie in $k$, so they span a finite-dimensonal $\Q$-subspace $E$
	of $k$. To see this, we will show that any $\Q$-linear function $f:E\to\Q$ must be zero. Given $f$, 
	choose a polynomial $r(s)$ such that $r(a_i-a_j)=f(a_i)-f(a_j)$ for all $i$ and $j$ (using the Lagrange interpolation theorem).

	This is consistent since $a_i-a)j=a_k-a_l$ (using the linearity of $f$) implies the image of these under $r$ are equal.
	Then looking at $r(\ad s)$, we find that its eigenvalues are $r(a_i-a_j)$ as well as $f(a_i)-f(a_j)$,
	whence $\sum f(a_i)a_i=0=\tr(sy)$ for asuitable $y$.

	Applying $f$, we get $\sum f(a_i)^2=0$ whence all $f(a_i)=0$ (since $f(a_i)\in\Q$) and $f=0$, as desired.
\end{prf}

\subsection{Cartan's Criterion}
How can we turn this into something useful for us in the Lie algebra world?
\begin{thm}[Cartan's Criterion]
	Let $L$ be a linear Lie algebra over (alg. closed and char zero) $k$. Suppose that $\tr(xy)=0$
	for all $x\in L$ and $y\in[LL]$. Then $L$ is solvable. 
	
	As a consequence, suppose that $L$ is a Lie algebra such that 
	$\tr((\ad x)(\ad y))=-$ for all $x$ and $y$ above. Then $L$ is solvable.
\end{thm}
\begin{rmk}
	Notice that the consequence really does follow from the theorem: if $\tr((\ad x)(\ad y))=0$
	then $\ad L\cong L/Z$ is solvable where $Z$ is the center of $L$ whence $L$ is also solvable
	since $Z$ is abelian.

	To prove that $L$ is solvable, under our hypothesis, it is enough to show that $[LL]$ is nilpotent, 
	but (due to Engel's theorem) it suffices to show that $[LL]$ consists of nilpotent matrices.
\end{rmk}
To prove this criterion, we need a fact about traces (the \textbf{associativity of traces}):
\[\tr([xy]z)=\tr(x[yz]).\]
\begin{prf}
	We know that $\tr(xy)=0$ for all $x\in L$, $y\in [LL]$ and any such $x$ maps $L$ to $[LL]$ under $\ad$.
	We would like to know that for any $x\in M$, that $\tr(xz)=0$ for all  $x\in[LL]$. We know this is true if $z\in L$
	so by the associativity of trace, this follows.

	Returning to the Jordan decomposition, let $A$ be any finite-dimensional algebra over $k$ (not necessarily associative)
	and let $d$ be a derivation of $A$. THen the semisimple parts $d_s$ and $d_n$ of $d$ are also derivations.

	To see this, we write $A$ as the direct sum of the generalized eigenspaces $A_a$ of itself under the action of $d$. Since $d$ is a derivation,
	it ends up becoming clear that $A_aA_b\subseteq A_{a+b}$, so that the transformation from $A$ to itself action by $a$ on $A_a$ is a derivation.
	But this is $d_s$. Then $d_n=d-d_s$ is also a derivation.
\end{prf}

\section{January 23, 2019}
Some clarifications from last time:
\begin{lem}
	Let $A\subseteq B$ be subspaces of $\gl(n,l)$ for some algebraically closed field of char zero.
	Let $M=\{x\in\gl(n,k)|[xB]\subseteq A\}$. Suppose that $x\in M$ such that $\tr(xy)=0$ for all $y\in M$.
	Then $x$ is nilpotent.
\end{lem}
\begin{rmk}
	This used the fact that if $x=x_s+x_n$ then $\ad x=\ad x_s+\ad x_n$ is the Jordan decomposition for $\ad x$.

	Furthermore, when applying this lemma to Cartan's criterion for sovability of Lie subalgebras of $\gl(n,k)$, 
	we claimed that, if $\tr(xy)=0$ for all $x\in L$ and $y\in [LL]$, then $L$ is solvable. This follows by our lemma
	since if $x\in M=\{z\in\gl(n,k)|[zL]\subseteq[LL]\}$ and if $y,z\in L$ then $\tr(x[yz])=\tr([xy]z)=0$ by hypothesis.

	So by the lemma $[LL]$ consists of nilpotents, so is nilpotent and thus solvable.
\end{rmk}

\begin{defn}
	If $L$ is any Lie algebra over $k$, then $\kappa(x,y)=\tr((\ad x)(\ad y))$, is a bilinear 
	form on $L$ and is called the \textbf{Killing form.} 

	It is symmetric and associative ($\kappa([xy],z)=\kappa(x,[yz])$).
\end{defn}
\begin{rmk}
	Now in the context of \textit{general (as opposed to linear) Lie algebras} we can restate
	Cartan to say:
\end{rmk}
\begin{thm}[Cartan Redux]
	If $\kappa(x,y)$ for all $x\in [LL]$ and $y\in L$, then $L$ is solvable.
\end{thm}

\subsection{Semisimple Lie Algebras}
What we are interested in, actually, is the polar opposite of solvable algebras: namely,
\textbf{semisimple} Lie algebras, having no nonzero solvable ideals.
\begin{thm}
	$L$ is semisimple if and only if the Killing form $\kappa$ is nondegenerate.
\end{thm}
\begin{rmk}
	This sort of naturally follows from our discussion above -- we want the opposite property for our algebra
	and we require that the form be as far from zero as possible.
\end{rmk}
\begin{rmk}
	If $I\lhd L$, we note that, if $\kappa_I$ is the Killing form for $I$ and $\kappa$ the Killing form for
	$L$, then $\kappa_I=\kappa|_{I\times I}$. This is because if $x,y\in I$ then $\ad x$ and $\ad y$ both map $L$ into $I$.
	Thus if $M$ is any $n\times n$ matrix over $k$ mappting $V=k^n$ to a proper subspace $W$, then $\tr M=\tr M|_W$, since 
	extending a basis from $W$ to $M$ is such that the added basis vectors make no contribution to the trace of $M$.
\end{rmk}

\begin{prf}
	If $\kappa$ is nondegenerate and $S$ is a solvable ideal of $L$, then $\kappa|X=\kappa S$ has
	\textbf{radical} $\{z\in S:\kappa(z,y)=0,\forall y\in S\}$ which is at least $[SS]$, and then $K$ would have a 
	radical as well.

	More precisely, if $S$ is a solvable ideal of $L$ then there is a nonzero abelian ideal $A$ of $L$
	(the last nonzero term in the derived series for $L$) and then if $x\in A$ and $y\in L$, then $\ad x\ad y$ maps $L$
	into $A$ and $(\ad x\ad y)^2=0$, so $\kappa$ has radical at least $A$.

	Conversely, if $L$ is semisimple, then the radical of $\kappa$ as defined aobve would be a solvable ideal of $L$,
	by associativity and Cartan's criterion, so $\operatorname{Rad}(\kappa)=0$.
\end{prf}

The nondegeneracy of the Killing form is \textbf{the key} result that makes the classification
of semisimple Lie algebras tractable. For instance, in the case of finite simple groups the classification required 
10,000 pages of journal papers and it boils down to not having any kind of simple criterion like this.

\subsection{Applications}
As a first step, we show 
\begin{thm}
	A Lie algebra $L$ is semisimple if and only if it is a finite direct sum of simple ideals $I$.
\end{thm}
\begin{prf}
	This is clear since if $I$ is an ideal then $I^\perp=\{y\in K:\kappa(y,x)=0\forall x\in I\}$
	is again an ideal of $L$ and $I\cap I^\perp=0$ by Cartan. Then we can keep doing this, pulling off minimal ideals.
\end{prf}

As a second application, we will show that ever derivation $d$ of a semisimple Lie algebra $L$ is inner.
\begin{prf}
	$L$ is semisimple, so $L=[LL]$ and $Z(L)=0$, so $\ad L\cong L$. Now if $D=\operatorname{Der} L$, then
	$L\lhd D$ (as in homework). If $L\ne D$, then take the orthogonal of $L$ under the killing form in $D$.

	Thus $L\cap L^\perp=0$ and $\dim L^\perp\ge\dim D-\dim L$, but $[LL^\perp]=0$, which says exactly that if $m\in L^\perp$, then $m(x)=0$
	for all $x\in L$, so $L^\perp=0$ and $L=D$.
\end{prf}

Next we can extend the Jordan decomposition to semisimple Lie algebras. We have already seen that if $L$ is semisimple then $\Der L$ is closed
under Jordan decomposition but $\Der L=L$, so any $x\in L$ can be written as $x_s+x_n$ \textbf{FOR $x_s,x_n\in L$ (important)} where $\ad x_s$ and $\ad x_n$
act semisimply and nilpotently on $L$ and that $[x_sx_n]=0.$

\begin{rmk}
But what if $L$ is a semisimple linear Lie algebra over $k$? Then if $x\in L$, we have  as above $x_s,x_n\in L$
but also $x=s+n$ for some $s,n\in\gl(n,k)$ where $s$ is semisimple and $n$ is nilpotent and $[sn]=0$. Then we can ask:
do $s$ and $n$ lie in $L$? The answer is yes, but requires more machinery Once we know this, there are immediate applications to 
representation theory of $L$ (maps $\varphi:L\to\gl(n,k)$):

Given any such representation $\varphi$ and $x\in L$ with $x=x_s+x_n$, we get that $\varphi(x_s)$ and $\varphi(x_n)$
\textbf{continue to act semisimply and nilpotently} on $k^n$. This is what allows us to get a real handle on the representation theory.
\end{rmk}

\begin{thm}
	Given any \textbf{finite dimensional} module $M$ over a finite dimensional $M$ over a semisimple Lie
	algebra $L$ is completely reducible.
\end{thm}
\begin{rmk}
	We will prove this next time, using also certain constructions on modules as well as the matrix
	representations of elements.

	This result ends up being extremely useful not only in the (more obvious) representation theory of $L$, 
	but also in the structure theory of $L$ by considering the adjoint action of $L$ on itself.
\end{rmk}

\section{January 25, 2019}
\subsection{Weyl's Theorem}
TOday we'll prove Weyl's theorem:
\begin{thm}[Weyl]
	Any finite dimensional module over a semisimple Lie algebra is complete reducible ( a finite direct sum of simple modules).
\end{thm}
To prove this, we need some preparation: namely a couple of methods of producing new moduels over any Lie algebra $L$ from old ones.

First of all, if $M,N$ are $L$-modules, then $M\otimes_k N$ is an $L$ module of dimension the product
of the dimensions of $M$ and $N$. Here we define the action
\[x\cdot(m\otimes n)=x\cdot m\otimes n+m\otimes x\cdot n\]
where we can quickly check that 
\[[xy]\cdot(m\otimes n)=x(y(m\otimes n))-y(x(m\otimes n)).\]

Similarly, $\Hom_k(M,N)$ is an $L$ module via
\[(xf)(v)=xf(v)-f(xv)\]
and again we can check this defines a module structure, noting that $xf=0$ for all $x\in L$ if and only if $f$ is an $L$ module homomorphism.

\begin{lem}[Shur's Lemma]
	If $M$ is an irreducible $L$ module, then the only $L$ module maps in $\operatorname{End}(M)$ are scalars.
\end{lem}

Last we need
\begin{defn}
	We have the \textbf{trace form} on $L$:
	\[(x,y)=(\tr\varphi(x))(\tr\varphi(y))\]
	and by associativity and Cartan's criterion, we know (assuming that $L$ acts faithfully) that $(-,-)$ is nondegenerate.
\end{defn}
Recall that given any basis $x_i$ of $L$, we can define the dual basis $y_i$ such that $(x_i,y_j)=\delta_{ij}$.
\begin{defn}
	The \textbf{Casimir element} attached to a homomorphism $\varphi:L\to\gl(n,k)$, for some $n$, to be the matrix
	\[c_\varphi=\sum_1^m\varphi(x_i)\varphi(y_i).\]
\end{defn}
\begin{rmk}
	This matrix commutes with $\varphi(x)$ for all $x\in L$. This can be done by showing $a_{ij}=-b_{ji}$

	Then in computing $[xc_\varphi]$, we find that the coefficients of $\varphi(x_i)\varphi(y_j)$
	is $a_{ij}+b_{ji}=0$. Thus $c_\varphi$ commutes with the matrices of $\varphi(L)$.

	Hence if $V=k^n$ ws te module being acted upon and if $V$ is irreducible, then $c_\varphi$ must be a scalar (Schur).
	What is this scalar? Well since $\tr c_\varphi=m=\dim L$, we get
	\[\frac{\dim L}{\dim V}I=\frac{m}{n}I.\]
\end{rmk}
\begin{prf}
	The proof here will basically follow Humphreys, but will include a comment that \textit{should have appeared} in the text
	(much to Monty's chagrin).

	Notice that we must show that every submodule $N$ of $M$ has a complement $N'$ so that $M$ is the direct sum of $N$ and $N'$ and $N'$ is a submodule.

	For the rest of it, peek at the book. :)
\end{prf}
\begin{rmk}
	This result fails for semisimple Lie algebras in positive characteristic, although we won't really be dealing with this much.
	This parallels the fact that Maschke's theorem fails when $\ch k$ divides the order of $G$.
\end{rmk}
\subsection{Structure}
We now consider the simples case of a semisimple Lie algebra, namely $\sl(2,k)$. Here we know that we have the basis
$h=e_{11}-e_{22}$, $x=e_{12}$ and $y=e_{21}$ where
\[[hx]=2x\quad [hy]=-2y\quad [xy]=h\]

Next time we will see that the diagonal matrix $hin L$ continues to act diagonally on \textit{any} finite dimensional representation of $L$.
Thus the first thing to look at here is how the representation $V$ decomposes as a sum of its eigenspaces
\[V=\oplus_{\lambda\in k}V_\lambda\]
but since $V$ is assumed to be finite dimensional, this is actually a finite sum. In particular, there is $\lambda$ such that 
$V_{\lambda+2}=0$.
\begin{defn}
	We call the $\lambda$ where $V_\lambda\ne 0$ \textbf{weights} of $V$.
\end{defn}
So there is a weight $\lambda$ such that $\lambda+2$ is not a weight. In general, due to the relations above on $L$,
\[xV_\mu\subseteq V_{\mu+2}\quad yV_\mu\subseteq V_{\mu-2}\]
and we call $x$ and $y$ (or their matrices) the \textbf{raising and lowering operators}.

Then we can pick $\lambda$ a weight such that $\lambda+2$ is not and pick $v_0\in V\setminus 0$. Then set
\[v_i=\frac{y^iv_0}{i!}\]
so 
\[hv_i=(\lambda-2i)v_i,\quad yv_i=(i+1)v_{i+1}\quad xv_i=(x-i+1)v_{i-1}\]
where this can be proved by induction.

But the nonzero vectors $v_i$ are independent, so there is a least $m$ with $v_m\ne 0$ and $v_{m+1}=0$. 
The last equation with $i=m+1$ then forces $\lambda-m=0$, so $\lambda=m$ is a nonnegative integer!

It follows that if $V$ is irreducible that $\dim V=m+1$ and $v_0,\dots,v_m$ are a basis for $V$.

This ends up capturing the situation with \textit{any} irreducible representation of $\sl(2,k)$. We get integer eigenvalues, all of the same parity, from $-m$ to $m$.

\section{January 28, 2019}
We'll start off with tying up some loose ends from last time: 
\begin{lem}
	Let $L$ be a semisimple subalgebra of $\gl(n,k)$ (once more, $k=\bar k$ and $\ch k=0$). Let $x\in L$. Then 
	the semisimple and nilpotent parts $x_s$ and $x_n$ of $x$ lie in $L$. Thus the Jordan decompositions of $x$ 
	(as a matrix and as an element in $L$) coincide.
\end{lem}
\begin{prf}
	To see this, recall that we know that $\ad x_x$ and $]ad x_n$ are polynomials with zero constant term in $\ad x$ and
	so bracket $L$ into $\gl(V)$. So if $x_s,x_n\in N=\{y\in\gl(n,k):[yL]\subseteq L\}$ and $L$ is an ideal of $N$.

	We want to show that $L-N$ but this is false! we need to get the scalar matrices out. So replace $N$ by those elements 
	such that if $y\in N$ then $y$ restricted to any irreducible submodule $k^n$ of $L$ has trace zero.

	We have $L\subseteq N$... (snip)

	So by Weyl's thoerem we can write $N=L\oplus M$ as an $L$ module. But $L$ is an ideal of $N$ and $[LM]=0$, so the matrices in $M$
	all commute with $L$. By Schur's lemma, they act by scalars on any irreducible submodule $W$ of $k^n$, which must be zero, by the trace condition.

	Since $k^n$ is the direct sum of irreducible modules, we get $M=$, so $L=N$.
\end{prf}

Hence we have fully justified our characterization of irreducible $(m+1)$-dimensional $\sl(2,k)$ modules $V$:
\begin{itemize}
	\item We always have a basis $v_0,\dots,v_m$ with $v_{-1}=v_{m+1}=0$ such that:
	\item $hv_i=(m-2i)v_i$
	\item $yv_i=(i+1)v_{i+1}$
	\item $xv_i=(m-i+1)v_{i-1}$
\end{itemize}
we can check that the bcracket relations are satisfied starting from this as a definition.
\begin{rmk}
	In fact, if you replace $m$ by any $\lambda\in k$ and let all $v_i$'s form a basis for all non-negative $i$ (that is easing
	the restriction $v_{m+1}=0$), you get an infinite dimensional $L$-module. Whenever $\lambda\in\N$, this module is \textbf{not}
	completely reducible.
\end{rmk}
\subsection{One more $\sl(2,k)$ Computation}
\[(\exp\ad ax)(\exp\ad hy)(\exp\ad ax)=\begin{pmatrix}
	1+ab & a(2+ab)\\
	b & 1+ab
\end{pmatrix}=\begin{pmatrix}
	0 & -b^{-1}\\
	b & 0
\end{pmatrix}\]
if $ab=-1$. Multiplying by its transpose, we get
\[\begin{pmatrix}
	b^{-2} & 0 \\
	0 & b^2
\end{pmatrix}\]
So whenever $K$ is algebraically closed, we get a set of generators of $\sl(2,k)$ as products of matrices in $\Int L$ which is therefore 
isomorphic to $\operatorname{PSL}(2,k)$.

The action of $L$ on any irreducible module $V$ always lifts to $\operatorname{SL}(2,k)$ and lifts to $\operatorname{PSL}(2,k)$ if and only if $\dim V$ is odd.

\brk

The moral to draw from our classification of $\sl(2,k)$ moduoes of finite dimension is that the weights ($h$-eigenvalues) always occur in strings
of integers. In particular the weight $i$ occurs overall with the same multiplicity (eigenspace dimension) as $-i$.

Also for an irreducible module either the weight zero or the weight 1 occurs with multiplicity one (but not both).

\subsection{Return to semisimple Lie algebras}
Consider decomposing such an $L$ as a module over itself via the adjoint representation. If $L$ had only nilpotent elements, it itself would be nilpotent (it isn't)
so $L$ has a \textbf{toral subalgebra} $T$ consisting of only semisimple elements.

Then, in fact, $T$ is abelian, since otherwise we would have $[xy]=ky$ for $x,y\in T$ and $k\ne 0$.
But on the other hand, since $\ad y$ is semisimple, but on the other hand $[yx]$ is a sum of eigenvectors of $\ad y$
each with nonzero eigenvalues, a contradiction.

\subsection{Decomposing $L$}
Now let's fix a \textbf{maximal toral subalgebra} $\h$ (that is, maximal among toral subalgebras). Then we have
\[L=\oplus_{\alpha\in\h^*}L_\alpha\]
where $L_\alpha=\{x\in L:[xh]=\alpha(h)x\}$, since any commuting set of semisimple (diagonalizable) matrices are simultaneously diagonalizable.

Now by Jacobi, $[L_\alpha L_\beta]\subseteq L_{\alpha+\beta}$ and bringing back in the Killing form,
\[\kappa(L_\alpha,L_\beta)=0,\quad\text{if}\quad \alpha+\beta\ne 0\]
so that $\kappa$ is nondegenerate on $L_0$:
\[\kappa(L_0,L_\alpha)=0\quad\forall \alpha\ne 0.\]

\begin{lem}
	$H=L_0$, or equivalently, $H$ is its own centralizer.
\end{lem}

The proof can be found in the book, but using this, we can rewrite our decomposition as
\[L=\h\oplus\bigoplus_{\alpha\in\h^*}L_\alpha\]
and this is called the \textbf{root space decomposition} of $L$ in terms of $\alpha\in\h^*$.
The terms $\alpha\in\h^*$  occurring above are called the \textbf{roots of $L$} (relative to $\h$). By definition \textbf{no root is zero.}

Notation: Write $\Phi$ to be the set of all (nonzero) roots in $\h^*$.

Since the restriction of $\kappa$ to $\h$ is nondegenerate, we get an identification of $\h^*$ with $\h$:
\[\alpha\in\h^*\Leftrightarrow t_\alpha\in \h\]
where $\alpha(h)=\kappa(t_\alpha,h)$. Moreover we can transfer $\kappa$ to $\h^*$:
\[(\alpha,\beta)=\kappa(t_\alpha, t_\beta)\]
which gives us a symmetric bilinear form on $\h^*$.

\brk

\textbf{Next Time: } we will place the form on a real vector space of dimension $\dim_k\h$. Best of all, 
$(-,-)$ will be positive definite, giving us a nice orthonormal basis.

\section{January 30, 2019}
Let $L$ be a semisimple Lie algebra, $\h$ a maximal toral subalgebra. Last time we saw the root space decomposition above. Note
we actually have $L=\h\oplus \bigoplus_{\alpha\in\Phi}L_{\alpha}$ where $\Phi\in\h^*$ are the values that give you nontrivial weight spaces,
called the \textbf{root system} of $L$ (relative to $\h$) with elements called \textbf{roots.} It ends up we can embed these root systems in
a Euclidean space, giving us a context in which to write some relatively simple axioms.

\subsection{Root Systems}
First,
\begin{lem}
	$\Phi$ spans $\h^*$.
\end{lem}
\begin{prf}
	Othersiwe, some nonzero $h\in \h$ would have $\alpha(h)=0$ for all $alpha\in\Phi$, but then $h$ would be in the center of $L$, a contradiction.
\end{prf}
\begin{lem}
	If $\alpha\in \Phi$, then $-\alpha\in\Phi$.
\end{lem}
\begin{prf}
	This is due to the fact that otherwise $L_\alpha$ would lie in the radical of the Killing form. Thus
	$[L_\alpha L_{-\alpha}]=\kappa t_\alpha$ where $\alpha(g)=\kappa(t_\alpha,h)$. Then you can follow the text.
\end{prf}
The takeaway from the discussion, which I admittedly slacked on typing, was that in considering $L_\alpha$ and $L_{-\alpha}$ we find that the structure
has to be precisely the structure of $\sl(2,k).$ 

\brk

Then $L$ becomes a module for $S_\alpha\cong\sl(2,k)$ under the adjoint action. Look at the submodule
\[M_\alpha=\h\oplus\bigoplus_{c\in k^*}L_{c\alpha}\]
Clearly $M_\alpha$ is stable under the $S_\alpha$ action. Weights of $h_\alpha$ are integers, so we know that 
$L_{c\alpha}=0$ if $c\notin\Z\cup\frac{1}{2}\Z$.

But also all occurrences of the weight zero come from $\h$, a codimension one space, namely $\ker\alpha$, dies under the action of $x_\alpha$ and $y_\alpha$.
The only other occurrence is $h_\alpha$ itself, which leads to $x_\alpha$ on the action $x_\alpha$ then dies (what)? Thus the weights occurring are $0,\cdots,0,2,-2$.

So the takeaway is that the weight 4 doesn't occur and $L_{2\alpha}=L_{\alpha/2}=0$, so the weight 1 never occurs.
Finally we get that $L_{c\alpha}=0$ if $c\ne \pm 1$ and $\dim L_{\alpha}=1$.

\brk

By the way, we also know that $[L_\alpha L_\beta]=L_{\alpha+\beta}$ by the way $x_\alpha$ and $y_\alpha$ act on any irreducible $S_\alpha$ module.

Finally we also know that if $\alpha$ and $\beta$ are roots, then 
\[\beta-\beta(h_\alpha)\alpha=\beta-\frac{2(\alpha,\beta)}{(\alpha,\alpha)}\alpha\]
is a root as well and $\frac{2(\alpha,\beta)}{(\alpha,\alpha)}\in\Z$. The equation above is just a reflection in Euclidean space.

\subsection{Rationality Properties}
We know that $\Phi$ spans $\h^*$: choose a basis $\alpha_1,\dots,\alpha_n$ of $\h^*$ consisting of roots. Then any root $\beta$ takes the form 
$\sum_i c_i\alpha_i$ for $\alpha_i\in k$. Apply $(-,\alpha_i)$ and divide by $(\alpha_i,\alpha_i)$, then we get
\[\frac{2(\beta,\alpha_i)}{(\alpha_i,\alpha_i)}=\sum_j c_j\frac{2(\alpha_j,\alpha_i)}{(\alpha_i,\alpha_i)}.\]

Finally we can do some row operations to find the unique solutions for the $c_j$ must lie entirely in $\Q$, so \textbf{all roots} lie in the $\Q$ subspace $E_\Q$
of $\h^*$ whose dimension over $\Q$ is $n=\dim_k\h^*=\dim_k\h$.

Moreover, for any $\alpha,\beta\in\h$,
\[(\beta,\alpha)=\kappa(t_\beta,t_\alpha)=\sum_{\gamma\in\Phi}\gamma(t_\alpha)\gamma(t_\beta)=\sum (\gamma,\alpha)(\gamma,\beta)\]
so in particular $(\beta,\beta)=\sum (\beta,\gamma)^2$ and dividing both sides by $(\beta,\beta)^2$ we get that $(\beta,\alpha)\in \Q$ for all roots and so more generally
for all $\alpha,\beta\in E_\Q$. Apply our formula one last time to see
that $(\lambda,\lambda)>0$ when $\lambda\ne 0$, so our form $(-,-)$ is symmetric, bilinear, and positive definite on $E_\Q$.

We know set up a formal vector space $E_\R$ as the $\R$ span of a basis of $\h^*$ fixed earlier consisting of roots. Then we know that
$\Phi\in\h^*$ satisfies the following properties:
\begin{enumerate}
	\item $\Phi$ spans $\R^n$
	\item If $\alpha\in\Phi$, the only multiples of $\alpha$ also in $\Phi$ are $\pm\alpha$.
	\item If $\alpha,\beta\in\Phi$, then $\beta-\frac{2(\beta,\alpha)}{)\alpha,\alpha)}\alpha\in\Phi$.
	\item $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\in\Z$ for all roots.
\end{enumerate}
\begin{defn}
	Any system $\Phi$ satisfying properties (a)-(c) above is an \textbf{(abstract) root system in $\R^n$}. If, furthermore,
	$\Phi$ satisfies $(d)$, we call it a \textbf{crystallographic root system.} In this case, the $\Z$-span of $\Phi$ is called the
	\textbf{root lattice} $\Lambda$, which is stable under all reflections $s_\alpha$ for $\alpha\in\Phi$.
\end{defn}
\begin{rmk}
	We note that for any root system $\Phi$ that the subgroup of $O_n(\R)$ generated by all $s_\alpha$ is a finite group $W$
	(finite since it acts on a finite set $\Phi$) called the \textbf{Weyl group.} $W$ consists of automorphisms of $\Phi$ -- that is,
	linear maps on $\R^n$ sending $\Phi$ to $\Phi$ and preserving the dot product.
\end{rmk}
We will classify (mostly) all finite groups $W$ arising in this way, both in the crystallographic and non-crystallographic cases, although we are primarily interested in the former.

\brk

We end with an example.

Look at $S_n$, the symmetry group on $n$ letters acting on $\R^n$ by coordinate permutations. Any coordinate flip, 
say the $i^{th}$ and $j^{th}$ coordinates, is the reflection $S_{e_i-e_j}$. These coordinate flips generate all of $S_n$, 
so $S_n$ is a Weyl group.

\newpage

\section*{Homework 2 -- Due Feb 1}
%4.5; 7.4,6,7; board problem: show that a semisimple Lie algebra is generated as a Lie algebra by two elements
\begin{hwprob}{4.5}
	If $x,y\in\End V$ commute, prove that $(x+y)_s=x_s+y_s$ and $(x+y)_n=x_n+y_n$.
	Show by example that this can fail if $x,y$ fail to commute. [Show first that $x,y$ semisimple (nilpotent) implies their sum is.]
\end{hwprob}
\begin{sol}
	Following the hint, we will first establish:
	\begin{lem}
		If $x$ and $y$ in $\End V$ commute and are semisimple (resp. nilpotent) then their sum $x+y$ is semisimple (resp. nilpotent).
	\end{lem}
	\begin{prf}
		Assume first that $x$ and $y$ are semisimple. Then $\ad x$ and $ad y$ can both be diagonalized. In particular, since they commute
		they can be diagonalized \textit{simultaneously} via some matrix $A$ to become diagonal matrices $D_x$ and $D_y$. But then their sum
		conjugated by $A$ is
		\[A[(\ad x)+(\ad y)]A^{-1}=A(\ad x)A^{-1}+A(\ad y)A^{-1}=D_x+D_y\]
		which is clearly diagonal, thus $x+y$ itself is semisimple.

		\brk

		Assume now that $x$ and $y$ are nilpotent, with $(\ad x)^\alpha=(\ad y)^\beta=0$. Then since
		\[[\ad x,\ad y]=\ad [xy]=\ad 0=0,\]
		we have that $\ad x=X$ and $\ad y=Y$ commute. So consider 
		\[(\ad(x+y))^{\alpha+\beta-1}=(X+Y)^{\alpha+\beta-1}=\sum_{i=0}^{\alpha+\beta-1}\binom{\alpha+\beta-1}{i}X^iY^{\alpha+\beta-1-i}=0\]
		where the last equality follows since the degree of each monomial is $\alpha+\beta-1$, which either forces the power of $X$ to be greater than $\alpha$
		or the power of $Y$ greater than $\beta$ (since if $i<\alpha,$ we get $\alpha+\beta-1-i>\beta-1$).

		Thus $x+y$ is nilpotent as well, completing the proof of the lemma.
	\end{prf}

	But then the statement itself follows simply by the uniqueness of the decomposition of $x+y$ into semisimple and nilpotent components: since 
	\[x+y=x_s+x_n+y_s+y_n=(x_s+y_s)+(x_n+y_n)\]
	is such a decomposition of $x+y$, these are \textit{the} semisimple and nilpotent components.
\end{sol}
\newpage
In the following three problems, let $L=\sl(2,k)$.
\begin{hwprob}{7.4}
	The irreducible representation of $L$ of highest weight $m$ can also be realized ``naturally''
	as follows. Let $X,Y$ be a basis for the two dimensional vector space $k^2$, on which $L$ acts as usual. Let $\mathscr{R}=k[X,Y]$
	be the polynomial algebra in two variables, and extend the action of $L$ to $\mathscr{R}$ by the derivation rule
	$z\cdot fg=(z\cdot f)g+f(z\cdot g)$. Show that this extension is well defined and that $\mathscr{R}$ becomes an $L$ module. Then show that
	the subspace of homogeneous polynomials of degree $m$, with basis $X^m,X^{m-1}Y,\dots,Y^m$ is invariant under $L$ and irreducible of highest weight $m$.
\end{hwprob}
\begin{sol}
	We simply extend the action linearly both for $\mathscr{R}$ and $L$ -- that is
	\[(\alpha x+\beta y) f=\alpha(x\cdot f)+\beta(y\cdot f)\]
	and
	\[x\cdot (\alpha f+ \beta g)=\alpha(x\cdot f)+\beta(x\cdot g)\]
	so by the linearity of derivations (and the bracket in $L$), all we have to check is that the bracket acts the right way under the derivation rule:
	\begin{align*}
		[xy]\cdot fg &= (xy-yx)\cdot fg\\
		&= xy \cdot fg - yx\cdot fg\\
		&= (xy\cdot f)g+f(xy\cdot g)-(yx\cdot f)g-f(yx\cdot g)\\
		&= (xy\cdot f - yx\cdot f)g + f(xy\cdot g - yx\cdot g)\\
		&= ([xy]\cdot f)g +f([xy]\cdot g)
	\end{align*}
	so this does in fact define an action on $L$. 

	Since the action is defined to be $k$-linear, we need only check that the $L$ action preserves the degree of a monomial in $X$ and $Y$.
	\[z\cdot X^\alpha Y^\beta = (z\cdot X^\alpha)Y^\beta+ X^\alpha(z\cdot Y^\beta)=\alpha(z\cdot X)X^{\alpha-1}Y^\beta+\beta(z\cdot Y)X^\alpha Y^{\beta-1}\]
	but since $z\in L$, $z$ takes $X$ and $Y$ to some linear combination of themselves (perhaps zero!), so the total degree of each of the
	above monomials is $\alpha+\beta$. Thus these elements span an $L$-module.

	Using $A=(\begin{smallmatrix}0&0\\1 & 0\end{smallmatrix})$ and $B=(\begin{smallmatrix}0&1\\0 & 0\end{smallmatrix})$ in $L$,
	we can get between any of these basis vectors since 
	\[A\cdot X^aY^b=aX^{a-1}Y^{b+1}\quad\text{and}\quad B\cdot X^aY^b=bX^{a+1}Y^{b-1}\]
	so this module is irreducible,

	Finally by considering the action of $C=(\begin{smallmatrix}1&0\\0 & -1\end{smallmatrix})$ on the basis, we see 
	\[C\cdot X^aY^b=aX^aY^b-bX^aY^b=(a-b)X^aY^b\]
	so the weights are $a+b=m,m-2, \dots, 2-m, -m$, so this is of highest weight $m$.
\end{sol}
\begin{hwprob}{7.6}
	Decompose the tensor product of the two $L$ modules $V(3)$ and $V(7)$ into the sum of irreducible submodules:
	$V(4)\oplus V(6)\oplus V(8)\oplus V(10)$. Try to develop a general formula for the decomposition of $V(m)\otimes V(n)$.
\end{hwprob}
\begin{sol}
	Using the fact that, if $v_i$ and $w_i$ are bases for $V(3)$ and $V(7)$, respectively, then $v_i\otimes w_j$ form a basis for $V(3)\otimes V(7)$ and that
	\begin{align*}
		h\cdot v_i\otimes v_j&=(h\cdot v_i)\otimes v_j+v_i\otimes(h\cdot v_j)\\
		&=(3-2i)v_i\otimes v_j+(7-2j)v_i\otimes v_j\\
		&=(10-2(i+j))v_i\otimes v_j
	\end{align*}
	Some quick analysis of the possible values $a_{ij}=10-2(i+j)$ can attain for $0\le i\le 3$ and $0\le j\le 7$
	tells us $-10\le a_{ij}\le 10$. Counting the number of $(i,j)$ pairs giving us each weight yields the list
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c}
			0 & $\pm 2$ & $\pm 4$ & $\pm 6$ & $\pm 8$ & $\pm 10$\\\hline
			4 & 4 & 4 & 3 & 2 & 1
		\end{tabular}
	\end{center}

	Take $v_0\otimes w_0$, the basis element of highest weight in the tensor product. We have characterized irreducible $\sl_2$ modules,
	so we know that the $\sl_2$-span of this element must be a copy of $V(10)$, consisting of the $k$-span of a single vector in each weight space.
	
	I am certain one can find a vector $v$ in $L_8$ such that $y^9\cdot v=0$, but I had a difficult time finding one.
	So rather than demonstrating that each of these simple modules whose sum (whence direct sum) is all of $V(3)\otimes V(7)$, I appeal
	to theorem 5.2 in Humphreys (the Lie analogue of Maschke from group reps): since $V(10)$ is simple and we're over a characteristic zero field there is a submodules $W$ of $V(3)\otimes V(7)$ such that
	\[V(3)\otimes V(7)=V(10)\oplus W.\]
	
	Since $V(10)$ spans a one-dimensional subspace of each weight space, the dimension of each must decrease by one in $W$, giving us 
	dimensions (for weight spaces of $W$):
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c}
			0 & $\pm 2$ & $\pm 4$ & $\pm 6$ & $\pm 8$ & $\pm 10$\\\hline
			3 & 3 & 3 & 2 & 1 & 0
		\end{tabular}
	\end{center}
	Then we can pick any nonzero vector in the highest-weight space $L_8$ and under $\sl_2$ we will generate a copy of $V(8)$.
	Iterating this process we get a copy of $V(6)$ and $V(4)$ until (by dimension considerations, if you like), the process terminates and
	\[V(3)\otimes V(7)=V(10)\oplus V(8)\oplus V(6)\oplus V(4).\]

	\brk

	After gaining some intuition for how these decompositions work in practice, I believe this general theory holds:
	\begin{conj}
		Let $m$ and $n$ be integers and let $V=V(m)\otimes V(n)$. Without loss of generality, assume that $m\le n$.
		Then 
		\[V\cong V(m+n)\oplus\cdots\oplus V(m+n-2\cdot k)\oplus\cdots\oplus V(n-m)\]
	\end{conj}
\end{sol}
\begin{hwprob}{7.7}
	In this exercise, we construct certain infinite dimensional $L$-modules. Let $\lambda\in k$ be
	an arbitrary scalar. Let $Z(\lambda)$ be the vector space over $k$ with countably-infinite basis $(v_0,v_1,\dots)$.
	\begin{enumerate}
		\item Prove that the formulas (a)-(c) of Lemma 7.2 define an $L$-module structure on $Z(\lambda)$, and that every nonzero $L$-submodule of $Z(\lambda)$
		contains at least one maximal vector.
		\item Suppose $\lambda+1=i$ is a nonnegative integer. Prove that $v_i$ is a maximal weight vector (e.g. $\lambda=-1$, $i=0$).
		This induces an $L$-module homomorphism $Z(\mu)\xrightarrow{\varphi}Z(\lambda)$ for $\mu=\lambda-2i$ sending $v_0\mapsto v_i$.
		Show that $\varphi$ is a monomorphism, and that $\im\varphi$ and $Z(\lambda)/\im\varphi$ are both irreducible $L$-modules
		(but $Z(\lambda)$ fails to be completely reducible when $i>0$).
		\item Suppose $\lambda+1$ is not a nonnegative integer. Prove that $Z(\lambda)$ is irreducible.
	\end{enumerate}
\end{hwprob}
\begin{sol}
	\textbf{Part (a):}
	
	To see that these relations define an $L$-module structure, we can rely on the fact that all vectors are
	\textit{finite} linear combinations of the $v_i$. This enables us to look ``locally'' at the $L$-module structure to confirm it 
	satisfies all required properties on a case-by-case basis.

	More specifically, for any $g\in L$ and any element $\sum_i c_i v_i\in Z(\lambda)$, take 
	\[S=\{i,g(i,k)|c_i,a^i_k\ne 0\}\] 
	where we define $g(i, k)$ and $a^i_k\in k$ to be such that 
	\[g\cdot v_i=\sum_k a^i_k v_{g(i,k)}\]

	Then $|S|<\infty$, so we can take $N=\max S$ and all $v_i$ that appear in the computation $g\cdot \sum c_i v_i$ are contained in 
	$V(N)$, so the $L$ structure follows from the $L$ structure there.

	\brk

	To see the last claim, notice that if $L'$ is a nonzero submodule of $Z(\lambda)$, then if $0\ne v_i\in L'$ then 
	since $x^{i+1}\cdot v_i=0$, there is some minimal $j$ such that $x^{j}\cdot v_i=0$. Then $v_{i-j+1}$ is a maximal
	vector.

	Furthermore examining the structure of $L'$, we get that we get $v_k\in L'$ for all $k\ge i$, which follows
	trivially from $y\cdot v_i=(i+1)v_{i+1}$.

	\textbf{Part (b):}

	Let $i=\lambda+1$ for some scalar $\lambda\in k$. Then by applying $x$, we get
	\[x\cdot v_i=(\lambda-i+1)v_{i-1}=(i-1-i+1)v_{i-1}=0\]
	so $v_i$ is a maximal weight vector in $Z(\lambda)$.

	Define $\varphi:Z(\lambda-2i)\to Z(\lambda)$ to be the map sending $v_0\to v_i$. Then we get
	\[\varphi(v_{1})=\varphi\left(y\cdot v_0\right)=y\cdot\varphi(v_i)=y\cdot v_i=(i+1)v_{i+1}\]
	and more generally 
	\begin{align*}
		\varphi(v_k)&=\varphi\left(\frac{1}{k!}y^k\cdot v_0\right)\\
		&=\frac{1}{k!}y^k\cdot v_i\\
		&=\frac{(i+1)(i+2)\cdots (i+k)}{k!} v_{i+k}\\
		&=\frac{(i+k)!}{i!k!}v_{i+k}\\
		&=\binom{i+k}{k}v_{i+k}
	\end{align*}

	But then the $\varphi(v_j)$ form a basis for the image (clearly) so if 
	\[\varphi\left(\sum c_k v_k\right)=\sum c_k\varphi(v_k)=0\]
	this implies that all $c_k=0$, so $\varphi$ in a monomorphism.

	Notice that $Z(\lambda)/\im\varphi\cong V(i-1)$, so is irreducible. Now since $\im\varphi\cong Z(\lambda-2i)$
	the problem reduces to showing the latter is irreducible. From (a) we know that any nonzero submodule of $Z(\mu)$ has a maximal vector $v_j$.
	Consider that in $Z(\mu)$
	\[x\cdot v_j=(\lambda-2i-j+1)v_{j-1}=0\]
	thus either $v_{j-1}=0$ (whence $j=0$ and this submodule is all over $Z(\mu)$) or else
	\[\lambda=2i+j-1=i-1\quad\Rightarrow\quad i+j=0\]
	but $v_{-i}=0$ for positive $i$, so this forces $i=j=0$, so the submodule is the entire module.

	\textbf{Part (c):}

	Assume now that $\lambda+1$ is not a nonnegative integer. Similar to above, any nonzero submodule has a maximum vector $v_j$.
	Then the equation we saw before
	\[2i+j-1=i-1\]
	has no solution, which forces $v_{j-1}=0$ and so $j=0$ and so the submodule is all of $Z(\lambda)$.
\end{sol}
\begin{hwprob}{(Board)}
	Show that a semisimple Lie algebra is generated (as a Lie algebra) by two elements.
\end{hwprob}
\begin{sol}
	Consider the decomposition (as a vector space) of any semisimple Lie algebra $\g$ with Cartan subalgebra $\h$ 
	into the sum of its root spaces
	\[\g = \h\oplus \bigoplus_{\alpha\in\Phi}\g_\alpha\]
	and chose $h\in\h$ and from each $\g_\alpha$ choose a nonzero $v_\alpha$. Define $v =h+\sum_{\alpha\in\Phi}v_\alpha$. I claim that 
	this vector, along with another vector generates everything. I'll keep thinking about this. :)
\end{sol}
\newpage

\section{February 1, 2019}
\begin{prob}
	This assignment's board problem: Show, using large tensor powers of the adjoint representation and $\sl_2$ theory,
	that any semisimple Lie algebra $L$ has irreducible modules of arbitrarily high degree.
\end{prob}

\subsection{Revisiting classical Lie Algebras}
In all cases the diagonal matrices form a maximal toral subalgebra with roots given by (relative the standard basis $e_i$ for $\R^n$):
\begin{itemize}
	\item $A_{n-1}$ is spanned by $e_i-e_j$ for $i\le i,j\le n$
	\item $B_{n}$ is spanned by $e_i-e_j$ and $e_i+e_j$ as well as $e_i$
	\item $C_{n}$ is spanned by $e_i-e_j$ and $e_i+e_j$ as well as $2ei$
	\item $D_{n}$ is spanned by (only) $e_i-e_j$ and $e_i+e_j$.
\end{itemize}

We can check in all cases that the axioms of a root system are satisfied: the differences, sums, and monomials act
on the roots by flipping, flipping and changing the sign, and just changing the sign, respectively.
In all cases, the root system is obtained from a lattice ($\Z$-span of an $\R$-basis) as follows:
\begin{itemize}
	\item $A_{n-1}$ the vector space is spanned by $e_1-e_2,\dots$ and the lattice is generated by the same vectors.
	Then the root system is realized as the set of such vectors of square length 2.
	\item $B_n$ we take the $\Z^n\subseteq\R^n$ vectors of square length 1 or 2.
	\item $D^n$ is the same, but just square length two.
	\item $C_n$ is a bit sketchy in terms of lengths -- we can't quite say all vectors of length 2 or 4. So instead
	we derive it from $B_n$ by replacing all $e_i$ by $2e_i$.
\end{itemize}
What are the Weyl groups of these sweet babies?
\begin{itemize}
	\item $A_{n-1}$: $S_n$ action on $\R^n$ by coordinate permutations. This is also the symmetry group of the unit simplex in $\R^n$.
	\item In $B_n$ and $C_n$ (reflections act the same way irrespective of magnitude) is the group of all permutations and sign changes 
	of the coordinates in $\R^n$. This can also be recognized as the hypercube or hyperoctahedron in $\R^n$ with vertices $\pm e_i$. This is 
	sometimes called the hyperoctahedral group.
	\item For $D_n$, this consists of all permutations \textit{even} sign changes of coordinates in $\R^n$. This has no 
	standard name. This can also be viewed as the group of symmetries of a polytope whose vertices are a subset of 
	those of a hypercube, but with \textit{evenly-many} negative. Again, there is no consensus about what to call these.
\end{itemize}
Later we will see the exceptional root systems (non-classical) and again they are taken from a lattice by taking all vectors of one or two specified square lengths.
\subsection{Some Non-Crystallographic Examples}
Here there are some $\alpha$ and $\beta$ where $\frac{2(\alpha,\beta)}{(\alpha,\alpha)}\notin\Z$.
\begin{ex}[$I_2(m)$]
	Start off with a regular $m$-gon in $\R^2$. The dihedral group of symmetries of this $m$-gon is not a Weyl group, 
	but it is a generalization: a \textbf{Coxeter group}: a finite group generated by reflections.

	To find the roots: if $m$ is odd, place the $m$-gon so its center is at $(0,0)$ and take $\pm v$ as $v$ runs over the vertices. If, instead,
	$m$ is even, center the $m$-gon at $(0,0)$ and take $\pm v$ for vertices $v$ along with $\pm 2m$, where the $m$ are 
	midpoints of edges.

	Why $2m$? Well in some special cases ($m=3,4,6$), this actually becomes a crystallographic system -- but not otherwise.

	There is also a degenerate case when $m=2$. Then the group is $S_2\times S_2$, generated by two orthogonal reflections.

	In all cases, the dihedral group is generated by just \textbf{two} reflections, corresponding to two roots which make as large an angle as possible.
\end{ex}

In general, the root system described in the above example is denoted $I_2(m)$ and the corresponding Coxeter group is
$D_m$, the dihedral group on an $m$-gon.

\subsection{Exceptional Root Systems}
\begin{ex}[$E_8$]
	The biggest such system is $E_8$. We begin with vector space $\R^8$ and a lattice spanned by all $e_i\pm e_j$
	as well as $(\frac{1}{2},\cdots,\frac{1}{2})$ and take all vectors in the lattice of square length 2.

	The lattice consists of all $(a_1,\dots,a_8$ where all $a_i\in\Z$ OR all $a_i\in \Z+\frac{1}{2}$ AND $\sum a_i\in 2\Z$.

	The roots are $e_i\pm e_j$ and vectors of the form $(\pm\frac{1}{2},\cdots,\pm\frac{1}{2})$ with \textbf{evenly many} $-\frac{1}{2}$. 
	There are 240 of them.
\end{ex}
Then we can define:
\begin{itemize}
	\item $E_7$ to be the set of vectors in $E_8$ orthogonal to $e_7+e_8$ (120 roots)
	\item $E_6$ to be the set of vectors in $E_8$ orthogonal to $e_7+e_8$ and $e_6+e_8$ (72 roots).
\end{itemize}

\begin{ex}[$F_4$]
	Vector space $\R^4$, with lattice spanned by $e_1,e_2,e_3,$ and $\frac{1}{2}(1,1,1,1)$.
	Then we get all $(a_i)\in\R^4$ with all $a_i\in\Z$ or all $a_i\in \Z+\frac{1}{2}$. Then we take all vectors of square length one or two.

	We get $\pm e_i$, $\pm(e_i\pm e_j)$ and $(\pm\frac{1}{2},\cdots,\pm\frac{1}{2})$ to get a total of 48 roots.
\end{ex}
\begin{ex}[$G_2=I_2(6)$]
	Vectors space is the one spanned by $e_1-e_2$ and $e_2-e_3$ in $\R^3$ and lattice is spanned by these and the roots 
	consist of these vectors as well as all vectors of square length 2 or six:
	\[(\pm (-2,1,1),\pm (1,-2,1),\pm (1,1,-2).\]
\end{ex}
\subsection{Positive and Negative Roots}
Given a root system $\Phi$, we can cut it in half by realizing $\R^n$ as an ordered space by saying $(v_1,\dots,v_n)$ is positive
if and only if the smallest index $i$ with $v_i\ne 0$ has $v_i>0$. Then the sum of two positive vectors is positive, as is a positive multiple.

Then define
\begin{defn}[Positive Roots]
	Set $\Phi^+=\Phi\cap\{\text{positive vectors}\}$. Call the roots in $\Phi^+$ and call $\Phi^+$ a \textbf{positive subsystem.}
\end{defn}

\section{February 6, 2019}
Recall from last itme that we were letting $\Phi$ be a root system with positive and negative roots $\Phi^+$ and $\Phi^-$ such that they partition $\Phi$,
are disjoint, and are closed under positive linear combinations.

Next we want ot exhibit a basis for $\R^n$, the ambient vector space, consising of positive roots such that
every root in $\Phi^+$ is a \textbf{positive} combination of basis vectors.

To do this, we already know that $\Phi^+$ has subsets $S$ (e.g. the whole thing) such that every root in $\Phi^+$ is a positive combination of things in $S$.
Let $\Delta$ be a minimal such subset under inclusion. I am having a slow morning -- take a look at the book. :)

\brk

The next claim is that $\Delta$ is independent. Otherwise, we have the relation $\sum_{\alpha\in\Delta}r_\alpha \alpha=0$ with
at least one nonzero $r_\alpha$. Moving all the negative coefficients over to the other side, we get the equality of two
(possibly empty) sums. If either side is trivial, this is a contradiction since $0\notin\Phi^+$.

If both are nonempty, then you can use the positivity of the inner product of one side with some $\beta$,
which then implies that the inner product of the other side with $\beta$ is negative -- again a contradiction.

\begin{defn}
	We call such a $\Delta$ a \textbf{simple subsystem} of $\Phi$.
\end{defn}

\begin{rmk}
	Notice that $\Delta$ is uniquely determined -- it consists exactly of those $\alpha\in\Phi^+$ that are \textit{not} positice combinations of two or more positive roots.

	Pick some $\alpha\in\Delta$, a simle root. The corresponding reflection $s_\alpha$ is also called simple. Then for any $\beta\in\Phi^+$, 
	$s_\alpha\beta$ is positive unless $\beta=\alpha$, as it must involve some simple root with a positive coefficent, which stays the same upon applying $s_\alpha$.

	Now let $\Phi^+$ and $(\Phi^+)'$ be two choies of positice roots, corresponding to two choices of positive vecors in $\R^n$. Let $\Delta,\Delta'$ be the simple subsystems
	corresponding two these two systems.

	If $\Phi^+\ne(\Phi^+)'$, there is some \textbf{simple} $\alpha\in\Delta$ which is \textbf{negative} in $(\Phi^+)'$. Applying $S_\alpha$ to $\Phi^+$, we get another positive
	system which contains the same elements of $(\Phi^+)'$ that $\Phi^+$ did, plus now $-\alpha$.

	Doing this repeatedly, we can bump up the size of the intersection $\Phi^+\cap(\Phi^+)'$ at each step, eventually yielding
	two equal positive sytems.

	So any two positive systems are conjugate under a product of simple reflections with respect to any system. That product is 
	an orthogonal transformation, so the angles between all pairs of simple roots don't depend on the choice of simple roots.

	Finally, if $\beta$ is a positive root, but not simple, then since it is a positive linear combination of simple roots, 
	$(\beta,\alpha)>0$ for some simple $\alpha$. Then $s_\alpha\beta=\beta-c\alpha$ for some $c>0$ is another positive root.
	Continuing this process must end with a simple root, so any positive root is a product of simple reflections applies to a simple root (in fact, any root)

	So the \textbf{entire root system} is completely determined by a knowledge of the square length of the simple roots and the angles between them.
\end{rmk}

\begin{rmk}
	In the crystallographic case, we can say more! Here
	\[s_\alpha\beta=\beta=\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\alpha\]
	where the coefficient is in $\Z$! Here any positive root is a \textbf{positive integer combination} of simple roots.
\end{rmk}

\begin{rmk}
	Finally, what possible angles can occur between pairs of simple roots? If $\alpha,\beta$ are simple roots with simple reflections $s_\alpha,s_\beta$, then it must be that $s_\alpha s_\beta$
	must be an element of finite order lying in a finite group. Thus the angle must be a rational multiple of $2\pi$.

	In orderr to capture all positive roots that are combinations of $\alpha$ and $\beta$ that are \textbf{positive} combinations, the angle between $\alpha$ and $\beta$ msut be as wide as possible:
	$\pi-\frac{\pi}{m}$ for some $m$ (the order) if $\alpha\ne c\beta$.

	In the Crystallographic case, we get that $m=2,3,4,$ or $6$.
\end{rmk}
\subsection{Dynkin Diagrams}
Now we construct the \textbf{Dyinkin diagram} of our root system, if it is crystallographic, or its
\textbf{Coxeter graph} in general.

This is a graph in which the vertices correspond to simple roots $\alpha$. The vertices corresponding to $\alpha$ and $\beta$
are joined by an edge if $\alpha\not\perp\beta$. This splits into cases:
\begin{itemize}
	\item $m=3$, we take a single edge.
	\item $m=4$, take a double edge with arrow pointing toward the shorter root.
	\item $m=6$, we take a triple edge with the same condition as above.
\end{itemize}

In a Coxeter graph, we do the same except there is just one edge, labeled with $m$.

\begin{ex}
	Recall in the classical cases that a root is positive if its leftmost nonzero coordinate is positive. Then for $A_{n-1}$:
	\[\]
	I want to fill these all in later. For now, just including the notes:

	Notice that the diagrams for $B_2$ and $C_2$ are graph isomorphic (consisting of a single double edge!). This 
	tells us that, in fact, the Lie algebras are isomorphic. This relates one of the spin groups to one of the special linear groups although i forget which.
\end{ex}

\section{February 8, 2019}
Continuing from last time, we want to write down the simple roots that give rise to the Dynkin diagrams 
for the exceptional Lie algebras we saw (and I just realized I haven't copied in yet.)

\subsection{Exceptional Lie Algebra Dynkin Diagrams}
For $E_8$he roots are in the images I took. :)

For $E_7$ and $E_6$ you just remove the roots that we talked about last time.

For $H_3$ we take as the simple roots the edge midpoints of a regular dodecahedron or icosahedron.

For $H_4$, take the face centers of a 120-sided regular polytope in $\R^4$, called the hecatonicosahedroid. 

\subsection{Have we found them all?}
Now we want ot see that htis list of connected Dynking diagrams is complete! 
\begin{rmk}
	We say ``connected'' because if $\Phi$ and $\Phi'$ are two root systems in $\R^n$ and $\R^m$,
	then $\Phi\sqcup\Phi'$ is a root system in $\R^{n+m}$, the orthogonal direct sum. This gives two disjoint
	Dynkin diagrams (since they are orthogonal) so we restrict to the connected components.
\end{rmk}

We are actually going to classify the \textit{Coxeter graphs} corresponding to (crystallographic) Dynkin diagrams
starting with a simple subsystem of a crystallographic root system. We obtain a basis of $n$ unit vectors in $\R^n$ such that hte angle between any 
two of them is $\pi-\frac{\pi}{m}$ for $m\in\{2,3,4,6\}$, encoded by a coxeter graph where there is no edge if $m=2$, unlabelled if $m=3$ and unlabelled otherwise.

\subsection{Classifying Coxeter Graphs}
To classify these, we actually classify the graphs that are \textbf{not} of this type. That is, for each connected Coxeter graph
that we've seen so far, we are going to attach a slightly larger graph to it that \textit{doesn't occur} as a Coxeter graph of any root system
or, in fact, even a \textit{subgraph} of a Coxeter graph.

By section 10 of Humphreys, every connected Dynkin diagram and root system $\Phi$ corresponds to a unique lowest (highest) root.
Then every other root, if the lowest is subtracted, becomes a positive combination of simple roots and has nonpositive dot
product with the simple roots.

The resulting ``Dynkin diagram'' (it won't be) corresponding to the simple roots plus the lowest root 
comes from a \textbf{dependent} set of vectors -- in fact, one admitting a dependence relation with all
positive coefficients. It is not a Dynkin diagram since if it were, that combination of roots would have square length zero,
but this is impossible for any Dynkin diagram or subdiagram thereof. This is true \textit{even if some of the edge labels are increased.}

\subsubsection{What do we get when we do this?}
We call these the \textbf{extended} or \textbf{affine} Coxeter graphs.

For $\tilde A_{n-1}$ our lowest root is $e_n-e_1$, which attaches to both $e_1-e_2$ and $e_{n-1}-e_n$ and creates a cycle.

For $\tilde B_{n}$, we add $-e_1-e_2$ which links with the secondmost left root in $B_n$.

For $\tilde C_n$, we append $-2e_1$ to the leftmost root.

For $\tilde D_n$, we append $-e_1-e_2$ to the secondmost left root as in $\tilde B_n$.

\brk

For the exceptional algebras: $\tilde E_6$ gets $\frac{1}{2}(-1,\cdots,-1,1,1,-1)$. I got a picture of the rest.

The argument here can be found in Humphreys' \textit{Reflection Groups and Coxeter Groups} on page 37 that, once the forbidding configurations
above have been ruled out, the only connected diagrams left are the ones we've seen.

As a sketch, $B_n$ and $C_n$ are easy to see that the crystallographic property allows only two 
possibilities: that the rightmost simple root has square length \textbf{twice or half} the length of the other simple roots, but no other multiples.

In the type $E$ cases, we have one central vertex and three chains of lengths $p,q,$ and $r$ coming from it.
Let's consider the posibilities we get:
\begin{itemize}
	\item $\tilde E_6$: $(3,3,3)$
	\item $\tilde E_7$: $(2,4,4)$
	\item $\tilde E_8$: $(2,3,6)$
\end{itemize}
Notice that in all three cases the sum of the reciprocals of these values is 1! In fact, these are the only 
solutions to this this in the integers!

But then in the regular Dynkin diagrams:
\begin{itemize}
	\item $E_6$: $(3,3,3)$
	\item $E_7$: $(2,4,4)$
	\item $E_8$: $(2,3,6)$
\end{itemize}
Here we have the only solutions where the sum of the reciprocals are greater than 1 (with $p,q,r>1$). 
Well we actually have the case $(2,2,n)$, but these correspond to $D_{n+2}$.

In non-Euclidean geometry, this condition on $(p,q,r)$ arises all the time. The sum of reciprocals plays a role in the geometry of triangles.

\subsection{Finite subgroups of $SO(3,\R)$}
We actually played the $\frac{1}{p}+\frac{1}{q}+\frac{1}{r}>1$ game before in algebra!
\begin{itemize}
	\item $(2,2,n)$ is the group $D_n$
	\item $(2,3,3)$ is $T\cong A_4$
	\item $(2,3,4)$ is $O\cong S_4$
	\item $(2,3,5)$ is $I\cong A_5$, the icosahedral group.
\end{itemize}

The icosahedrom (or dodecahedron) pops up in the non-crystallographic Coxeter groups:
\begin{itemize}
	\item $H_3$ is all symmatries of an icosahedron or of a dodecahedron in $O(3,\R)$. This group is $A_5\times\Z_2$
	\item $H_4$ is a crazy picture, but the group is
	\[\frac{(A_5\times \Z_2)^2\ltimes \Z_2}{\langle \pm 1\rangle}\]
	which we call the Hecatonicosahedroidal group. :)
\end{itemize}

\section{February 13, 2019}
One final remark about the inequalities that came up: $\frac{1}{p}+\frac{1}{q}+\frac{1}{r}>1$ and $<1$ for $p,q,r\ge 2$.

\begin{rmk}
	if $\frac{1}{p}+\frac{1}{q}+\frac{1}{r}<1$, then the closest to 1 it can come is $\frac{1}{42}=\frac{1}{2}+\frac{1}{3}+\frac{1}{7}=1-\frac{1}{42}$.
	This actually comes up in enumerative algebraic geometry for exactly this reason. Of course, it is also the
	answer to the question of life, the universe, and everything as well. :)
\end{rmk}

We can also work out the coefficents of all simple roots in the highest root quite easily by induction:

Start with the extended Dynkin diagram: place the label 1 on the new vertex (highest root) and label
the others so that the corresponding combination of vectors has square length zero. This amounts to ensuring
that the sum of the labels of all vertices adjacent to agive one is twice the label of that vertex, where labels are counted doubly/triply
if they lie at the left end of a double/triple arrow.

In $\tilde A_{n-1}$, this forces labels all to be ones. In $\tilde B_n$, we get 1's at the ``forked'' end 
and twos elsewhere. $\tilde C_n$ has ones at the endpoints and twos elsewhere. $\tilde D_n$ has ones at the ends of both forks.
You can keep doing this for the exceptional ones but I will have to take a picture because they are hard to transcribe.

\brk

Now we go off ``in a completely different direction'': Let $\Phi$ e a crystallographic root system. Then there are two lattices attached to $\Phi\in\R^n$:
one is the \textbf{root lattice} spanned over $\Z$ by the roots -- denoted $R$. Another lattice is the \textbf{weight lattice} $P$
consisting of all $\lambda\in\R^n$ such that $\langle\lambda,\alpha\rangle=2(\lambda,\alpha)/(\alpha,\alpha)\in\Z$ for all $\alpha\in\Phi$.

Then $P$ and $R$ are free abelian groups of rank $n$. $P$ is generated by the \textbf{fundamental dominant weights} $\lambda_\alpha$
defined by $\langle\lambda_\alpha,\alpha\rangle = 1$ and $\langle\lambda_\alpha,\beta\rangle=0$
for $\alpha\ne\beta\in\Delta$.

Hence $P/R$ is a finite abeliang group, called the \textbf{fundamental group of $\Phi$}. Its importance is the following:
Complex semisimple Lie algebras are classified by their root systems. But what about complex semisimple Lie groups?
These are classified by their root system \textit{plus} a lattice lying between the root and weight lattices. Taking the root lattice 
leads to the adjoind group $\Int L$ (the smallest possible choice) if we take the weight lattice, then we get the simply-connected 
universal covering space of $\Int L$.

\begin{ex}
	For type $A_{n-1}$, $\Int \sl_n(\C)=PSL(n,\C)$. The simply-connected cover is $SL(n,\C)$ itself.
\end{ex}

\begin{rmk}
	When $k$ is not algebraically closed, we end up getting more forms to work with, but you can follow
	a similar technique to classifying the Lie groups. In positive characteristic things blow up pretty badly.
\end{rmk}

\brk

What did that have to do with the coefficients of the simple roots in the highest root? Well, in Humphreys exercise 13.13,
we see that every coset has a unique representative that is minimal in the sense that $\langle\lambda,\alpha\rangle=0$ or $1$ for all positive roots
$\alpha.$

It turns out that $\Phi^v$ (see Humphreys) is another root system called the \textbf{dual} to $\Phi$. Then if 
$\Delta$ is a simple subsystem for $\Phi$, then $\Delta v$ is a simple subsustem for $\Phi^v$. Then the condition
that $\langle\lambda,\alpha\rangle\in\Z$ is linear in $\alpha^v$.

\subsection{Takeaway}
We got a bit in the weeds for me, but the takeaway is that the fundamental groups are as follows:
\begin{itemize}
	\item $A_{n-1}$: $\Z/(n)$
	\item $B_n$: $\Z/(2)$
	\item $c_n$: $\Z/(2)$
	\item $D_n$: $\Z/(4)$ if $n$ is odd; $\Z_2\times\Z_2$ otherwise
	\item $E_6$: $\Z/(3)$
	\item $E_7$: $\Z/(2)$
	\item $E_8,F_4,G_2$: $\Z$
\end{itemize}

\begin{rmk}
	One more cool calculation (explained better in Humphreys' 1990 book): We can compute the order $|W|$:
	look at the orbit and stabilizer of the highest roots under the action of $W$: 

	The orbit consists of all roots with the same length as the heighest weight: the long roots.

	The stabilizer (discussed in Humphreys \S 10) is generated by all reflections fixing the highest root, corresponding to the roots \textbf{not}
	adjacent to the longest root in the extended diagram.
\end{rmk}

We have a final remark on root systems before returning to the land of Lie algebras:
\begin{rmk}
	Given a positive root $\beta$ in a root system $\Phi$ , we have seen that (if $\Delta$ is the corresponding simple subsystem)
	there is a product of simple reflections (attached to simple roots) sending $\beta$ to $\gamma$, a simple root.

	Thus there is a concatenated string
	\[\beta,\beta-\alpha,\cdots,\beta-\langle\beta,\alpha\rangle\alpha,\beta-\langle\beta,\alpha\rangle\alpha-\alpha',\cdots\]
	in which all terms of roots and the last root is simple. But then we can reverse this string to deduce that
	$\beta=\alpha_1+\alpha_2+\cdots+\alpha_m$ where all $\alpha_i$ are simple and every partial sum is a positive root.

	What we get from this, in terms of Lie algebras, is that one generator from each of the positive root spaces generates all the positive root spaces.
\end{rmk}
\end{document}