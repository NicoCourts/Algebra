\documentclass[12pt]{article}

\usepackage{setspace}

\usepackage{amsmath, amsfonts, amssymb, graphicx, color, fancyhdr, lipsum, scalerel, stackengine, mathrsfs, tikz-cd, mdframed, enumitem, framed, adjustbox, bm, upgreek, x	color}
\usepackage[framed,thmmarks]{ntheorem}
\usepackage[mathscr]{euscript}

%set up theorem/definition/etc envs
%Problems will be created using their own counter and style
\theoreminframepreskip{0pt}
\theoreminframepostskip{0pt}
\newframedtheorem{prob}{Problem}[section]
\newenvironment{hwprob}[1]
{\renewcommand{\theprob}{#1}%
 \addtocounter{thm}{-1}%
 \begin{prob}}
{\end{prob}}

%solution template
\theoremstyle{nonumberbreak}
\theoremindent0.5cm
\theorembodyfont{\upshape}
\theoremseparator{:}
\theoremsymbol{\ensuremath\spadesuit}
\newtheorem{sol}{Solution}

%Theorems, Lemmas, and Corollaries
\theoremstyle{changebreak}
\theoremseparator{}
\theoremsymbol{}
\theoremindent0.5cm
\theoremheaderfont{\color{violet}\bfseries} 

\newtheorem{thm}{Theorem}[subsection]
\theoremheaderfont{\bfseries}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

%Create a new env that references a theorem and creates a 'primed' version
%Note this can be used recursively to get double, triple, etc primes
\newenvironment{thmprime}[1]
  {\renewcommand{\thethm}{\ref{#1}$'$}%
   \addtocounter{thm}{-1}%
   \begin{thm}}
  {\end{thm}}

\setlength\fboxsep{15pt}

%Shade definitions
\theoremindent0cm
\theoremheaderfont{\normalfont\bfseries} 
\def\theoremframecommand{\colorbox[rgb]{.9,.8,1}}
\newshadedtheorem{defn}[thm]{Definition}

%Man, that's really good! Let's use the same thing for definitons.
\newenvironment{defprime}[1]
  {\renewcommand{\thethm}{\ref{#1}$'$}%
   \addtocounter{thm}{-1}%
   \begin{defn}}
  {\end{defn}}

%proofs
\theoremstyle{nonumberbreak}
\theoremindent0.5cm
\theoremheaderfont{\sc}
\theoremseparator{}
\theoremsymbol{\ensuremath\spadesuit}
\newtheorem{prf}{Proof}

%remarks
\theoremstyle{change}
\theoremindent0.5cm
\theoremheaderfont{\sc}
\theoremseparator{:}
\theoremsymbol{}
\newtheorem{rmk}[thm]{Remark}

%Replacement for the old geometry package
\usepackage{fullpage}

%Put page breaks before each part
\let\oldpart\part%
\renewcommand{\part}{\clearpage\oldpart}%

%Center each figure by default
\makeatletter
\g@addto@macro\@floatboxreset{\centering}
\makeatother

%header stuff
\setlength{\headsep}{24pt}  % space between header and text
\pagestyle{fancy}     % set pagestyle for document
\lhead{Notes on Lie Algebras} % put text in header (left side)
\rhead{Nico Courts} % put text in header (right side)
\cfoot{\itshape p. \thepage}
\setlength{\headheight}{15pt}
\allowdisplaybreaks

%Set of Integers
\newcommand*{\Z}{
\mathbb{Z}
}
%Set of Natural Numbers
\newcommand*{\N}{
\mathbb{N}
}
%Set of Real Numbers
\newcommand*{\R}{
\mathbb{R}
}
%Set of Complex Numbers
\newcommand*{\C}{
\mathbb{C}
}
%Rationals
\newcommand*{\Q}{
\mathbb{Q}
}

%Section break
\newcommand*{\brk}{
\rule{2in}{.1pt}
}

\DeclareMathOperator{\Aut}{Aut}

%raise that Chi!
\DeclareRobustCommand{\Chi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} 

%Image
\DeclareMathOperator{\im}{Im}

%Coker
\DeclareMathOperator{\coker}{coker}

%characteristic
\DeclareMathOperator{\ch}{char}

%rank
\DeclareMathOperator{\rank}{rank}

%identity map
\DeclareMathOperator{\id}{id}

%some nice shortcuts
\DeclareMathOperator{\calB}{\mathcal{B}}

%Lie algebra stuff
\DeclareMathOperator{\gl}{\mathfrak{gl}}
\let\sl\relax
\DeclareMathOperator{\sl}{\mathfrak{sl}}
\DeclareMathOperator{\so}{\mathfrak{so}}
\let\sp\relax
\DeclareMathOperator{\sp}{\mathfrak{sp}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Der}{Der}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Rad}{Rad}
\let\t\relax
\DeclareMathOperator{\t}{\mathfrak{t}}
\DeclareMathOperator{\n}{\mathfrak{n}}
\DeclareMathOperator{\diag}{diag}

%fix tilde
\let\tilde\relax
\newcommand*{\tilde}[1]{\widetilde{#1}}

% Enumerate will automatically use letters (e.g. part a,b,c,...)
\setenumerate[0]{label=(\alph*)}

\begin{document}
%make the title page
\title{Lie Algebras and Groups\vspace{-1ex}}
\author{A course by: Monty McGovern\\
Notes by: Nico Courts}
\date{Winter 2019}
\maketitle

\renewcommand{\abstractname}{Introduction}
\begin{abstract}
	These notes are my best attempt at following along with our \textit{Math 508 --
	Lie Algebras} course at UW. This is my first time trying to type my 
	notes on-the-fly in class so we'll see how well this goes. The course reference
	is Humphreys' \textit{Introduction to Lie Algebras and Representation Theory.}

	The course description follows:
	
	\brk

	This is the second course in the Algebraic Structures sequence. I will classify 
	finite-dimensional complex semisimple Lie algebras, also proving some structural 
	results on general Lie algebras along the way. Although one usually first 
	encounters Lie algebras in a manifolds course, the treatment (following the text) 
	will be entirely algebraic.
\end{abstract}

\section{January 7, 2019}
The homework is posted on Monty's website. :) 
\subsection{Lie algebras}

This course will be studying Lie algebras, but as opposed to their treatment in manifolds, 
we will be studying them from a purely algebraic point of view. The book (Humphreys)
actually never defines a Lie group.

\begin{defn}
	A \textbf{Lie Algebra} $L$ or $\mathfrak{g}$ over a field $k$ is a $k$-vector space (usually f.d.)
	along with a \textit{bracket operation} $[vw]:L\times L\to L$ such that $[\cdot\cdot]$ is 
	\begin{itemize}
		\item anticommutative,
		\item bilinear,
		\item $[x[yz]]=[[xy]z]+[y[xz]]$
	\end{itemize}
\end{defn}

\begin{rmk}
	The last principle above is actually equivalent to the \textit{Jacobi identity:}
	\[[x[yz]]+[y[xz]]+[z[xy]].\]
	This follows from bilinearity and anticommutativity of the bracket.
\end{rmk}
The most natural place for these to arise is as \textit{derivations} on an algebra!
\begin{defn}
	A \textbf{$k$-derivation} $d:A\to A$ on an algebra $A$ over $k$ is a $k$-linear map
	satisfying the Leibniz rule.
\end{defn}
\begin{rmk}
	Some key facts about derivations (for us):
	\begin{itemize}
		\item Given a fixed $a\in A$, the map $d_a$ sending $b\mapsto ab-ba$, the \textbf{commutator}
		$[ab]$ is a derivation.
		\item If $d,e$ are derivations, then so is $[de]=de-ed$, where $de$ is the \textit{composite}
		of $d$ and $e$ as opposed to the product.
	\end{itemize}
\end{rmk}

\subsection{Examples}

A main source of Lie algebras is (associative) algebras! \textit{Any associative $k$-algebra $A$}
becomes a Lie algebra over $k$, taking $[ab]=ab-ba.$ In particular, one obvious choice for $k$-algebra
is $M_n(k)=\gl_n(k)$, the (Lie) algebra of $n\times n$ matrices over $k$.

\textbf{Lie subalgebras} are what you'd expect (including closure under brackets). Notice
that if $L'\le L$, then they \textbf{must both be over the same field.}

If $L$ is a $k$-Lie algebra and $I\lhd L$ is an ideal of $L$, then the quotient space $L/I$ becomes a 
Lie algebra with $[x+I,y+I]=[xy]+I$ as the bracket.

A \textbf{Lie algebra homomorphism} is a map $\varphi:L\to L'$ such that $\varphi$ is $k$-linear
and $\varphi([xy])=[\varphi(x)\varphi(y)].$

We get the usual first isomorphism theorem $L/\ker\varphi\cong \varphi(L).$

\brk

Associative algebras are not the only source of Lie algebras, however! One example is 
$\sl(n,k)=\{n\times n\text{ matrices over } k\text{ with trace zero}\}$

Note that this is \textbf{not closed under product} since $\tr(AB)\ne\tr A\tr B$ but $\tr(AB)=\tr(BA)$
so $\tr(AB-BA)=\tr(AB)-\tr(BA)=0$.
\begin{defn}
	We call this algebra (or, in fact any subalgebra of $\gl(n,k)$) \textbf{linear}. Think 
	``Linear'' means ``of matrices.''
\end{defn}

We say that $\sl(n,k)$ has \textbf{type} $A_{n-1}$. Eventually we will see seven types
$A-G$ of semisimple Lie algebras. The shift in index will emerge later.

$\sl(n,k)$ is, in fact, a simple Lie algebra: for $k=\C$, $\sl(n,\C)$ has no ideals
apart from the trivial ones. 

\brk

Other non-associative examples include $k^n$ with a bilinear form $(\cdot,\cdot)$ which
is either symmetric or skew-symmetric and (in either case) is nondegenerate.
\begin{defn}
	$(\cdot,\cdot)$ is \textbf{nondegenerate} if the map $v\mapsto (v,\cdot)$ is injective. Equivalently
	there is no $v\in V$ such that $(v,w)=0$ for all $w\in V$.
\end{defn}

Given $V=k^n$ and a bilinear form on $V$, we can look at all $X\in \gl(n,k)=\gl(V)$ such that 
$(Xv,w)=(v,Xw)$. Then $X$ is \textbf{adjoint} with respect to the form. There is a similar definition for when
$X$ is \textbf{skew-adjoint.} One can check that 
$[XY]$ is skew-adjoint whenever both $X$ and $Y$ are.

\subsection{Generating (skew) symmetric forms}
It ends up that the dot product (which is a symmetric form) is misleadingly simple -- thus
we will look elsewhere.

If $M\in \gl(n,k)$ is symmetric, so that $M^t=M$, then $(v,w)=v^tMw$ is a symmetric. If 
instead $M$ is skew-symmetric, then the same definition yields a skew-symmetric form. 
This actually induces a one-to-one correspondence between matrices and forms.

In both cases, if $M$ is invertible, then the form will be nondegenerate. As a consequence, 
since skew-symmetric matrices are always singular in odd dimensions, we see that 
nondegenerate skew-symmetric forms (over $\ch k\ne 2$ where the two families of forms
coincide) exist only in even dimensions.

\subsection{A peek at classifications}
If we have a nondegenerate symmetric form where $n=2m$ is going to give us an algebra
of type $D_m$. If $n=2m+1$, then it is of type $B_m$. Both of these cases are called
\textbf{orthogonal.}

If instead we have a skew-symmetric form and $n=2m$, then this is of type $C_m$, and we 
call this algebra \textbf{symplectic.}

\brk

We will make a particular choice for our matrix $M$ and then study the resulting
Lie algebras in much more detail next time. The choices will be: 
\begin{itemize}
	\item For type $D_m$:
	\[\begin{pmatrix}
		0 & I_m\\
		I_m & 0
	\end{pmatrix}\]
	\item For type $C_m$:
	\[\begin{pmatrix}
		0 & -I_m\\
		I_m & 0
	\end{pmatrix}\]
	\item For type $B_m$:
	\[\begin{pmatrix}
		1 & 0 & 0\\
		0 & 0 & I_m\\
		0 & I_m & 0
	\end{pmatrix}\]
\end{itemize}

\section{January 9, 2019}
Today we will be looking deeply into the stucture of linear Lie algebras of types A-D.

\subsection{Linear Lie Algebras Revisited}
Recall that the \textbf{matrix unit} $e_{ij}$ is the matrix with 1 in the $(i,j)$ entry and 
zero elsewhere. And then $e_{ij}e_{kl}=\delta_{jk}e_{il}$ and furthermore
\[[e_{ij}e_{kl}]=\delta_{jk}e_{il}-\delta_{li}e_{kj}.\]
This is especially nice when $j=i$, called the \textbf{diagonal matrix unit}.

Then we look at type $A_{n-1}$ ($\sl(n,k)$). Let $D$ be the set of diagonal matrices in this algebra.
Notice the dimension is $n-1$ since then $n^{th}$ term on the diagonal is determined as the
negative of the sum of the other $n-1$ terms. Let $A=\operatorname{diag}(d_1,\dots,d_n)$. Then consider the eigenvalues
associated with $e_{ij}$: 
\[[Ae_{ij}]=(d_i-d_j)e_{ij}=(E_i-E_j)Ae_{ij}\]
where $E_i$ is the linear functional selecting the $i^{th}$ entry in $A$. Moreover, $D$ is abelian as a Lie algebra, so $D$
acts diagonally on $L=\sl(n,k)$ by commutation with eigenvalues $E_i-E_j$ and zero
for $1\le i,j\le n$ and $i\ne j$.

In the other classical cases B-D, there is always a matrix $M$ which defines the form $(v,w)=v^tMw$ 
as we saw yesterday. In all three cases, the Lie algebra exists consists of all skew-adjoint matrices
$X$ relative to the form. $B_m=\mathfrak{so}(2m+1,k)$ as well as $D_m=\mathfrak{so}(2m,k)$ and $C_m=\mathfrak{sp}(2m,k)$.

This condition translates to the form of matrices in the above Lie groups and the condition is always $Mx=-x^tM$ in all cases.

Type B:
\[\begin{pmatrix}
	0 & b_1 & b_2\\
	c_1 & m & n\\
	c_2 & p & q
\end{pmatrix}\]
where $c_1=-b_2^t$, $c_2=-b_1^t$, $q=-m^t$, $n^t=-n$ and $p^t=-p$.

Type C:
\[\begin{pmatrix}
	m& n\\ p & q
\end{pmatrix}\]
where $n^t=n, p^t=p,$ and $m^t=-q$.

Type D:
\[\begin{pmatrix}
	m& n\\ p & q
\end{pmatrix}\]
where $n^t=-n, p^t=-p,$ and $m^t=-q$.

Looking at the eigenvalues of elements of $D$ associated to vectors $e_{ij}-e_{m+i,m+j}$. Look at photos

Using a similar analysis, we can look at types $B$ and $D$. We define the functions $E_i$ similarly on the space of diagonalmatrices and gives a rise to the following collection of linear functions:
in $B_m:$ $\pm E_i$ and $\pm(E_i\pm E_j)$ and $D_m$ gives us $\pm(E_i\pm E_j)$.

This collection of functions in each case is called the \textbf{root system of the Lie algebra},
Then any complex simple finite-dimensional Lie algebra is classified by its root system.
The (perhaps surprising) fact is that this already encompasses all but finitely many of these
things up to ismorphism: the classical Lie algebras. Eventually we will learn more about the
\textbf{exceptional Lie groups.}

This section was a little hard to follow and the handling in Humphreys is easier to follow, 
but delays speaking about root systems and actually deriving the eigenfunctions (is that the right word?)
until significantly later. Monty seemed to think it was acceptable to delay the understanding of this a bit.

\subsection{Derivations and $\exp$}
Look an an arbitrary Lie algebra over a field $k$ where $\ch k=0$ (which we will mostly be assuming from here on)
. Let $\delta$ be a derivation of $L$, so that $[\delta x,y]+[x\delta y]=\delta[x,y]$. Assume that $\delta$ is nilpotent.

Then the ``power series'' (polynomial) is
\[\exp\delta=\sum_{i>0}\frac{\delta^i}{i!}\]
\begin{prob}
	This is a good exercise to go through: Check that
	\[[(\exp\delta)x,(\exp \delta)y)=[xy]\]
	for each $x,y\in L$
\end{prob}

\begin{rmk}
	This actually shows that $\exp\delta$ is an automorphism of $L$. Furthermore you'd find that
	\[(\exp\delta)(\exp(-\delta))=1.\]
\end{rmk}

What if $k=\R$ or $\C$? Then the power series (even when $\delta$ is not nilpotent!) always converges
and defines an automorphism as before.

\begin{lem}
	For all \textbf{complex} semisimple Lie algebras $L$ it turns out that the group
	generated by $\exp\ad x$ ($\ad x(y)=[xy]$) coincides with the group generated by all 
	nilpotent $\ad x$.
\end{lem}
\subsection{Adjoint group}
The last thing for today is to define the adjoint group:
\begin{defn}
	Let $L$ be a Lie algebra, then 
	\[\operatorname{Int}(L)=\exp\ad L\]
	is the \textbf{Adjoint group} of $L$. It is a subgroup (so we believe) of the Lie
	Group associated to $L$.
\end{defn}
\begin{rmk}
	Actually after talking to Monty, $\exp\ad L$ \textit{is} (essentially) the Lie group
	corresponding to $L$. Such a group is not unique, however.
\end{rmk}

Some examples of adjoint groups:
\begin{itemize}
	\item If $L=\sl(n,\C)$, then $\operatorname{Int}(L)=PSL(n,\C)=SL(n,\C)/\text{center}$
	\item If $L=\so(n,\C)$, then $\operatorname{Int}(L)=PSO(n,\C)$
	\item If $L=\sp(2n,\C)$ then $\operatorname{Int}(L)=PSp(2n,\C)$.
\end{itemize}

%%%% HW 1 - Due Jan 18 %%%%
\newpage

\section*{HW 1 -- Due January 18}
Do problems 1.9, 2.1, 3.8. 3.9, and 4.3.

\begin{hwprob}{1.9}
	When $\ch k=0$, show that each classical Lie algebra $A_l,B_l, C_l$ and $D_l$ satisfies $[LL]=L$. (This shows again that each algebra consists of trace zero matrices.)
\end{hwprob}
\begin{sol}
	\subsubsection*{Type $A_k$:}
	Recall that a basis for $A_l$ is the collection $\mathcal{B}=\{e_{ij}|i\ne j\}\cup\{e_{ii}-e_{i+1,i+1}|1\le i\le l\}$. Using linearity of the bracket,
	it then suffices to check that $\calB$ is contained in the algebra generated by $[\calB \calB]$. To see this, we use that
	\[[e_{ij}e_{kl}]=\delta_{jk}e_{il}-\delta_{li}e_{kj}.\]

	First, notice we have
	\[[e_{i,i+1}e_{i+1,i}]=e_{ii}-e_{i+1,i+1},\]
	so it suffices to show we also have the $e_{ij}$ for $i\ne j$.

\end{sol}

\begin{hwprob}{2.1}
	Prove that the set of all inner derivations $\ad x$ for $x\in L$ is an ideal of $\Der L$.
\end{hwprob}
\begin{sol}
	Let $I=\ad L$. $I\subseteq \Der L$ since $\ad x$ is a derivation. This is a subspace since $\ad 0=0$ and since the bracket is bilinear
	\[\ad x(ay+bz)=[x,ay+bz]=[x,ay]+[x,bz]=a[xy]+b[xz]=a\ad x(y)+b\ad x(z).\]

	Take any $x\in I$ and let $f\in\Der L$ be arbitrary. But then for $y,z\in L$
	\begin{align*}
		[\ad x,f](yz)&=(\ad x f-f\ad x)(yz)\\
		&=[x,f(yz)]-f[x,yz]\\
		&=[x,f(y)z+yf(z)]-f([x,y]z+y[x,z])\\
		&=[x,f(y)z]+[x,yf(z)]-f([x,y]z)-f(y[x,z])\\
		&=[x,f(y)]z+f(y)[x,z]+[xy]f(z)+y[x,f(z)]-f([x,y])z\\
		&\hspace{2in} -[x,y]f(z)-f(y)[x,z]-yf[x,z]\\
		&=([x,f(y)]-f[x,y])z+y([x,f(z)]-f[x,z])\\
		&=([\ad x, f](y))z+y([\ad x,f](z))
	\end{align*}
	so $[IL]\subseteq I$ whence $I$ is an ideal of $L$.
\end{sol}

\begin{hwprob}{3.8}
	Let $L$ be nilpotent. Prove that $L$ has an ideal of codimension 1.
\end{hwprob}
\begin{sol}
	Assume that $L$ is nilpotent. Then defining $L^0=L$, and by induction $L^i=[LL^{i-1}]$, this 
	means that $L^n=0$.
\end{sol}

\begin{hwprob}{3.9}
	Prove that every nilpotent Lie algebra L has an outer derivation (see 1.3) as follows: Write $L=K+Fx$
	for some ideal $K$ of codimension 1. Then $C_L(K)\ne 0$ (why?). Choose $n$ so that $C_L(K)\subseteq L^n$,
	$C_L(K)\not\subseteq L^{n+1}$ and let $z\in C_L(K)-L^{n+1}$. Then the linear map $\delta$ sending $K$ to zero
	and $x$ to $z$ is an outer derivation.
\end{hwprob}
\begin{sol}

\end{sol}

\begin{hwprob}{4.3}
	This exercise illustrates the failure of Lie's theorem when $F$ is allowed to have positive characteristic.
	Consider the $p\times p$ matrices
	\[x=\begin{pmatrix} 
		0 & 1 & 0 & \cdots & 0\\
		0 & 0 & 1 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & 1\\
		1 & 0 & 0 & \cdots & 0
	\end{pmatrix}\quad y=\diag(0,1,\dots, p-1)\]
	Check that $[xy]=x$, hence that $x$ and $y$ span a two dimensional solvable subalgebra $L$ of $\gl(p,F)$. Verify that $x$ and $y$ have no common eigenvector.
\end{hwprob}
\begin{sol}

\end{sol}
\newpage
%%%% End HW 1 %%%%

\section{January 11, 2019}
A few more things about $\Int L$ in classical cases:

Begin with $n\times n$ matrices $M$ over $\R$ or $\C$. Given any such $M$, we have
\[e^M=\sum_{i>0}\frac{M^i}{i!}\]
always converges (that is, the series for each entry converges). Futhermore $e^Me^{-M}=I$.
\begin{rmk}
	Notice that over \textit{any} field over characteristic zero whenever $M$ is nilpotent 
	this still holds.
\end{rmk}

Now if $M$ is skew-adjoint with respect to a bilinear form $(\cdot,\cdot)$, then 
\[(M^iv,w)=(-1)^i(v,M^iw)\]
whence
\[(e^Mv, w)=(v,e^{-M}w)\]
and so 
\[(e^Mv,e^Mw)=(v,w)\]
for all $v,w\in\R^n$ or $\C^n$.

But notice that then $e^M$ is an \textbf{isometry of} or \textbf{preserves} the form. Moreover, 
if we set $A=\ad M$, then $A$ is given as the difference of two \textit{commuting} linear maps, 
left and right multiplication by $M$.

\brk

Using the binomial theorem: $e^AN=e^MNe^{-M}$. This, in a nutshell, is why the formulas for
$\Int L$ all involved modding out by the center, since conjugation by a scalar matrix is trivial! :)

\subsection{Combining Ideals}
Now we return to the case of general Lie algebras over arbitrary fields (no longer assuming $\ch k=0$). 
We have seen that a subspace $I\le L$ is an ideal if $[IL]\subseteq I$.

If $I,J\lhd L$, then $I+J$ is an ideal, as well as
\[[IJ]=\{\sum_1^n [x_iy_i]|x_i\in I, y_i\in J\}.\]

We also are sometimes interested in the direct sum of ideals $\oplus L_i$ where $[L_iL_j]=0$ when $i\ne j$.

\subsection{Examples}
$0$ and $L$ are ideals as well as the \textbf{center} $Z(L)$.

\subsection{Characterizing Lie Algebras as Linear}
We now want to realize many Lie algebras as (isomorphic to) linear ones. For now, note that the adjoint map $\ad$
is a Lie algebra homomorphism by the Jacobi identity. Then $\ker\ad=Z(L)$, so when $Z=0$, then $L$ is isomorphic to 
a linear Lie algebra. 

\begin{defn}
	If $Z(L)$, and the map $\ad:L\to L$ is an isomorphism, we say $L$ \textbf{acts faithfully}
	on itself (via $\ad$).
\end{defn}

\begin{defn}
	If we have a homomorphism $L\to\gl(n,k)$, then we call $k^n$ an $L$-\textbf{module}.
	We define the action of $L$ on $K^n$ through the homomorphism.
\end{defn}

Notice that $L$ is always an $L$ module over itself.

One can check, using the realization of $\sl(n,k)$ we saw last time that (when $\ch k\nmid n$)
that $\sl(n,k)$ is simple. This is actually the Lie algebra analogue of the fact that $M_n(K)$ is simple
as a ring over any field $K$.

\begin{rmk}
	Notice that if $\ch k |n$$,$ then \textit{any} scalar matrix $aI\in\sl(n,k)$
	so $Z(\sl(n,k))$ is nontrivial.
\end{rmk}

\brk

In any event you can look up the proof for $\sl(2,k)$ in the book. The thing to glean here is that
\[[xy]=h,\quad [hx]=2x,\quad [hy]=-2y\]
for $h=e_{11}-e_{22}$, $x=e_{12}$ and $y=e_{21}$. This will keep popping up throughout the course.

\subsection{Ideals of Linear Lie Algebras}
Many subalgebras of $\sl(n,k)$ or $\gl(n,k)$ have many ideals (which happen to be ring-theoretic ideals).

\begin{defn}
	Some notation:
	\[\t(n,k)=\{\text{upper-triangular matrices}\}\] 
	also 
	\[\n(n,k)=\{\text{strictly-upper-triangular matrices}\}\]
\end{defn}

\subsection{Derived Series}
For any Lie algebra, we can define
\begin{defn}
	The \textbf{derived series} of a Lie algebra $L$ is defined to be
	\[L^{(0)}=L,\quad L^{(1)}=[LL],\quad L^{(n+1)}=[L^{(n)}L^{(n)}]\]
	which form an ascending chain of ideals.
\end{defn}

\begin{defn}
	If $L^{(n)}=0$ for some $n$, and thus for all sufficiently large $n$, we call $L$ \textbf{solvable.}
\end{defn}
\begin{rmk}
	Note that actually there is a cooresponding definition for groups using commutators $[G,G]$.
	This was defined first and ported to Lie algebras. It actually happened in the reverse direction for:
\end{rmk}

\begin{defn}
	The \textbf{central series} of $L$ is 
	\[L^0=L,\quad L^1=[LL],\quad L^2=[L,L^1],\dots\]
	and if $L^n=0$ for some $n$, then we call $L$ \textbf{nilpotent.}
\end{defn}
\begin{rmk}
	Equivalently, we can say that $L$ is nilpotent if \textit{any $n$-fold bracket} in $L$ is zero.
\end{rmk}
\begin{rmk}
	Notice that every nilpotent Lie group is solvable, but the converse fails: $t(n)$ is solvable by not nilpotent.
\end{rmk}

\subsection{Examples of solvable algebras}
One can easily check that every ideal and homomorphic image of a solvable algebra is solvable. Therefore if $I$ and $J$ are solvable ideals of $L$, then 
so is $(I+J)/J\cong I/(I\cap J)$ is solvable, as is $I+J$. Furthermore if $I$ and $L/I$ are solvable, then $L$ is.

\begin{defn}
	$\Rad L$, the \textbf{radical of $L$}, is the unique largest solvable ideal of $L$.
\end{defn}
\begin{rmk}
	Any finite dimensional algebra has a radical (taken to be the sum of all solvable ideals in $L$).
\end{rmk}

Using this definition, we are able to give two different characterizations:
\begin{defn}[Semisimple Lie Algebra]\label{def-semisimple}
	$L$ is semisimple if $\Rad L=0$.
\end{defn}
\begin{defprime}{def-semisimple}[Semisimple Lie Algebra]
	$L$ is semisimple if it is a direct sum of simple Lie algebras.
\end{defprime}
This equivalence will not become clear until we learn about the Killing form, but at least it jives
with our experience with representation theory and semisimple modules.

\section{January 14, 2019}
I missed this day for an event. I will try to catch up if I have time.

\section{January 16, 2019}
We are continuing from last time when we were showing that as solvable Lie subalgebra of $\gl(n,k)$
(when $\ch k=0$ and $\bar k=k$) necessarily admits a vector $v\in k^n\setminus 0$ that is
a common eigenvector:
\[xv=\lambda(x)v\]
for some \textbf{linear} function $\lambda:L\to k$.

We argues (as in the parllel case of a Lie subalgebra of $\gl(n,k)$ consisting of nilpotent matrices
by induction on dimension and found the same result (except it is zero). In this case when $\dim L=1$,
since $k$ is algebraically closed, $L$ is spanned by asingle matrix $x$ and any eigenvector of $x$ does the trick.

Now assume that the result holds for solvable Lie algebras of dimension $<d$. Then since $L$ is sovable, we know
$[LL]\subsetneq L$. Let $I$ be any subspace containing $[LL]$ and properly contained in $L$ of codimension one.
Then $I$ is an ideal since $[LI]\subseteq[LL]\subseteq I$, so by the inductive hypothesis $I$ has a common eigenvector, 
so that there is a linear function $\lambda$ such that the \textbf{weight space}
\[V_\lambda=\{v\in k^n:xv=\lambda(x)v,\,\forall x\in I\}\]
is nonzero.

We know that $L=I+kx$ for some $x\in L$, so we must show that $x$ preserves $V_\lambda$: then any eigenvector
of $x$ on $V_\lambda$ will do the job. Given $v\in V_\lambda$, we must show that 
\[yxv=xyv+[yx]v=\lambda(y)xv+\lambda([yx])v\]
so we must show that $\lambda([xy])=0$.

Given $v\in V_\lambda$, look at hte powers $v,xv, x^2v,\dots$. Let $m$ be the least power such that the
$v,xv,\dots,x^mv$ are linearly independent and let $W_i$ be the span of the first $i$ of these.

Then one checks by induction for an $y\in I$ that $yx^iv=\lambda(y)x^iv\pmod{v,xv,\dots,x^iv}$
It follows that $y$ was in the subspace $W_m$ (oh no missed some stuff).

So the trave of $y$ on $W_m$ is $m\lambda(y)$. Now for any $y\in I$, the matrix $[xy]$ acts on $W_m$ ascendingthe commutator of two matrices having trace zero, whence $m\lambda([xy])=0$,
so since $\ch k=0$, $\lambda([xy])=0$, as desired.

\brk

As a consequence, given a solvable Lie subalgebra of $\gl(n,k)$ here is a chain of subspaces
\[0=V_0\le V_1\le\cdots\le V_n=k^n\]
or a \textbf{flag} such that $LV_i\subseteq V_i$ for all $i$: $L$ acts by upper triangular 
matrices with respect to a suitable basis.

Another way of putting this, using module language, is that given any \textbf{finite dimensional} module over
a solvable Lie algebra (over an algebraically closed characteristic zero field) has a one-dimensional submodules $N$.

Equivalently, the only finite-dimensional irreducible modules over a solvable Lie algebra are one-dimensional. We 
will use this later in the context of semisimple Lie algebras with large solvable subalgebra.

\begin{rmk}
	For the record, Lie's theorem \textit{definitely fails} when $k$ has positive characteristic.
	We see one in our homework, and another example comes from $\sl(2,k)$ where $\ch k=2.$

	We know that $\sl(2,k)$ has a basis: $(\begin{smallmatrix}
		1&0\\0&-1
	\end{smallmatrix})=I_2$, $x=e_{12}$, $y=e_{21}$. Here $[hx]=0=[hy]$ and $[xy]=h$. It is shown in the book
	and is easy to see that when $\ch k\ne 2$, $\sl(2,k)$ is simple, but here $\sl(2,k)$ is nilpotent and its
	bracket relations become $[xy]=z$, $[xz]=[yz]=0$.

	It ends up we can define (actually over any field) the \textbf{Heisenberg} Lie algebra, spanned
	by $x=e_{12},y=e_{23},z=e_{13}$ or more simply the $3\times 3$ strictly-upper-triangular matrices.

	The matrices in $\sl(2,k)$ for any $k$ have no common eigenvector, so our proof of Lie's theorem must
	fail somewhere? We indeed have an ideal $I$ of coimension one spanned by $h$ and $x$ and there is acommon eigenvector for $I$, 
	namely $e_1$. Then $I$ acts diagonally on $ke_1$ via the linear function sending $h$ to 1, $x$ to $1$. 
	Here $y$ plays the role of $x$ in the proof, but is outside the ideal. That is,
	\[\lambda[yx]=\lambda(h)\ne 0.\]
\end{rmk}

Returning to the good case of $k$, we can now apply our result on a linear solvable Lie algebras to arbitrary ones:
uGIven any solvable Lie algebra $L$, there is a flag $0=L_0\le L_1\le\cdots\le L_n=L$ of ideas of $L$
so that $[LL_i]\subseteq L_i$ Also $[LL]$ is nilpotent, since it acts on $L$ by strictly upper triangular matrices.

Both results fail for general solvable Lie algebras when the basefield is not nice. If a solvable Lie algebra $L$ 
\textbf{does} have a chain of ideals $0=L_0\le L_1\le\cdots\le L_n=L$ where $\dim L_i=i$, then we call $L$ \textbf{completely solvable.}

\subsection{Returning to Linear Algebra}
At this point we will take some time to return to an important idea from Linear algebra, namely the \textbf{Jordan canonical form}
of a square matrix. Recall Jordan blocks and similarity when eigenvalues lie in the field.

If $M$ is already in this form, then we can write $M=M_s+M_n$ where $M_s$ consists of the diagonal entries of each
block (zeroes elsewhere) and $M_n$ the off-diagonal entries. Then $M_s$ is diagonal (or for general $M$
diagonalizable), or \textbf{semisimple} and $M_n$ is nilpotent and $[M_sM_n]=0$ (they commute).

\begin{defn}
	$M=M_s+M_n$ is called the \textbf{Jordan decomposition of $M$.} We call $M_s$ and $M_n$ the \textbf{semisimple} 
	and \textbf{nilpotent} parts of $M$.
\end{defn}
\begin{rmk}
	Each are uniquely determined for $M$ and (and this is not generally proved in the first year 
	algebra sequence) each are equal to polynomials in $M$ with no constant term.x`
\end{rmk}

\end{document}
