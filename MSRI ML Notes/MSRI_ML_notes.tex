\documentclass[12pt]{article}

\usepackage{setspace}

\usepackage{amsmath, amsfonts, amssymb, graphicx, color, fancyhdr, lipsum, scalerel, stackengine, mathrsfs, tikz-cd, mdframed, enumitem, framed, adjustbox, bm, upgreek, xcolor, hyperref}
\usepackage[framed,thmmarks]{ntheorem}

%Replacement for the old geometry package
\usepackage{fullpage}

%Input my definitions
\input{./mydefs.tex}

%Shade definitions
\theoremindent0cm
\theoremheaderfont{\normalfont\bfseries} 
\def\theoremframecommand{\colorbox[rgb]{0.9,1,.8}}
\newshadedtheorem{defn}[thm]{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% Customize Below %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%header stuff
\setlength{\headsep}{24pt}  % space between header and text
\pagestyle{fancy}     % set pagestyle for document
\lhead{``Mathematics of Machine Learning'' Notes} % put text in header (left side)
\rhead{Nico Courts} % put text in header (right side)
\cfoot{\itshape p. \thepage}
\setlength{\headheight}{15pt}
\allowdisplaybreaks

% Document-Specific Macros
\DeclareMathOperator{\err}{err}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\grad}{\nabla}

\begin{document}
%make the title page
\title{MSRI's \textit{Mathematics of Machine Learning} Summer School\vspace{-1ex}}
\author{Nico Courts}
\date{July 29-Aug 9, 2019}
\maketitle

\renewcommand{\abstractname}{Introduction}
\begin{abstract}
	These notes were the ones I took while attending MSRI's ``Mathematics of Machine Learning'' summer school
	at the University of Washington during the two weeks above.
\end{abstract}

\section{Statistical Learning}
This series of lectures was given by Robert Shapire from Microsoft Research.

The topic of the talks here are somewhere at hte nexus of supervised and statistical learning. The big idea concerning what we want o do is 
to learn how to do better in the future based on experience from the past. The hope is that these methods can be fully automated. This is a proper subset of AI, 
but definitely a core area of how we do it.

We are looking for easy to use, flexible algorithms that give us useful results. Ideally they would be interpretable, but 
this is usually a secondary result.

He believes that ML can help us to understand how learning happens in non-machine entities (cool!). For instance, one can consider the question of 
nature vs nuture and to ask the question ``what is simplicity and why is it useful?''

\subsection{The Problem}
In this course, we will be focusing on a single kind of (simple) learning problem: learning from example. Given a set of examples, 
we are looking for an algorithm that can classify instances given objects.

For instance, character recognition. We have a set of handwritten characters and the letter it depicts. Then we want to feed this 
into a learning algorithm that gives us a prediction rule, that given a handwritten character, outputs the predicted character.

We were then given a couple of examples to try. The particulars weren't important, but the takeaway here is 
\begin{itemize}
	\item We needed enough data to say something meaningful.
	\item We looked for a rule that fit the observations. We want the rule to be consistent or to contain few mistakes.
	\item We were looking for rules that were ``simple.'' Importantly, our notion of simplicity changed when we converted one set of numbers to binary.
\end{itemize}
The first condition is always the case: ``more is more.'' But the latter two fit into a tradeoff between fit and simplicity.

The main questions we will be trying to answer:
\begin{itemize}
	\item How much data do we need?
	\item How do we define simplicity?
	\item How much complexity do we need to represent our problem?
	\item What is the tradeoff mentioned above?
\end{itemize}

\subsection{Studying the Problem Mathematically}
We have to come up with a formal model for the problem so that we can do mathematics with it. The learning model should answer some basic questions:
\textit{What is the goal of learning?} and \textit{How is the learning happening?}.

We begin the description of the model:
\begin{defn}
	An \textbf{instance} (or sometimes informally \textbf{example}) $x$ is an object in a space called the \textbf{instance space} or \textbf{domain} $X$.
\end{defn}
\begin{rmk}
	To each instance we assign a \textbf{label} or \textbf{class}. In this course we will be simplifying to only consider two classes, denoted $0$ and $1$ or $+$ and $-$.
\end{rmk}
\begin{rmk}
	We work under an assumption that there is an underlying function 
	\[c:X\to\{0,1\}\]
	called the (target) \textbf{concept} that we are trying to learn. Later we will relax this assumption.
\end{rmk}
\begin{defn}
	We define the \textbf{hypothesis} to be (similar to the concept) a map 
	\[h:X\to\{0,1\}\]
	that holds our current beliefs.
\end{defn}
We write $\calC$ to denote the \textbf{concept class}, the class of all possible functions we are exploring (we assume we know a class to which $c$ belongs). We will further 
assume that our data were generated \textit{independently} and at random. It is limiting to make an assumption about the distribution from which these were drawn. So we will just say there is one.

\begin{defn}
	We define 
	\[\operatorname{err}_D(h)=\Pr_{x\sim D}[h(x)\ne c(x)]\]
	to be the (generalization) \textbf{error}.
\end{defn}

Then we obviously ask that $\operatorname{err}_D(h)<\varepsilon$ for some small $\varepsilon$, but furthermore we want that 
this happens with high probability:
\[\Pr[\operatorname{err}_D(h)\le\varepsilon]\ge 1-\delta\]
and in this case, we say that that the rule is probably approximately correct  (PAC).
\begin{defn}
	The class $\calC$ is \textbf{PAC-learnable} by $\calH$ (a hypothesis space) if there is an algorithm $A$ such that
	for all $c\in\calC$ and for all distributions $D$ and for all positive $\varepsilon$ and $\delta,$ then $A$ takes ``polynomial many''
	random variables $X_i\sim D$ which outputs $h\in\calH$ such that
	\[\Pr[\operatorname{err}_D(h)\le\varepsilon]\ge 1-\delta\]
\end{defn}
\begin{rmk}
	This is not an easy problem, per se, but it is one of the most simple classes of algorithms to work with.
\end{rmk}
\begin{ex}
	Let $X=\bbR$ and let $\calC$ be of the form $c_r$ for $r\in\bbR$ where 
	\[c(x)=\left\{\begin{array}{lr} 1, & x\ge r\\
	 0 & \text{otherwise}\end{array}\right.\]
	 so $\calC$ is the \textit{set of positive half lines}. Then if we have some data, we can set our hypothesis 
	 $h=c_b$ where $b$ is the leftmost positive value. Then if $c=c_r$, then $\operatorname{err}_D(h)$ has to do with the interval $[r,b]$. Let $b-r=\varepsilon$.

	 Now restrict attention to $X=[0,1]$ and notice that if we got a training point within $\varepsilon$ of $c$ (to the right), then we would have gotten a smaller difference. Thus
	 \begin{align*}
		 \Pr[\operatorname{err}_D(h)<\varepsilon] &\le \Pr[\text{no $x_i$ are in $[b,b+\varepsilon]$}]\\
		 &=\prod_1^m\Pr[x_i\in R]\\
		 &=(1-\varepsilon)^m\\
		 &\le e^{-\varepsilon m}
	 \end{align*}
	 using the fact that $1+x\le e^x$ for all $x$.
\end{ex}

\subsection{Sufficient conditions for learning}
Now we consider the case when our hypothesis space $\calH$ is finite.
\begin{thm}
	Let $A$ be an algorithm that finds a hypothesis $h_A\in\calH$ which is consistent with $m$ random training examples (as before)
	where
	\[m\ge \frac{1}{\varepsilon}(\ln|\calH|+\ln\frac{1}{\delta})\]
	then $\Pr[\operatorname{err}_D(h_A)>\varepsilon]\le \delta$.
\end{thm}
\begin{rmk}
	Note that we have completely done away with trying to get a handle on the concept class here.
\end{rmk}
\begin{rmk}
	Equivalently, with probability of $1-\delta$, 
	\[\operatorname{err}_D(h_A)\le\frac{\ln|\calH|+\ln|\frac{1}{\delta}|}{m}\]
\end{rmk}
The idea to take away here is that $\ln|\calH|$ is in some way measuring ``description length'' of a hypothesis: you have to describe how to distinguish the correct one from the wrong ones. Think popping these 
in a binary tree. Then the $m$ bound gives you some sort of way to manage the complexity of the algorithm.

\begin{thm}
	Assume we have $m\ge \frac{1}{\varepsilon}(\ln|\calH|+\ln|\frac{1}{\delta}|)$ examples. Then with probability $1\ge 1-\delta$, for all $h\in\calH$ if $h$ is consistent then $\operatorname{err}_D(h)\le \varepsilon$ (here we often write ``$h$ is $\varepsilon$-good.'')
\end{thm}
\begin{prf}
	We prove the converse: the probability of there being an $h\in\calH$ that is consistent and $\varepsilon$-bad is less than $\delta.$
	Notice that whether $h$ is consistent with the training set is a random variable (since it depends on choice of training set). Whether it is $\varepsilon$-bad is not random. So 
	if $\calB$ is the set of $\varepsilon$-bad hypotheses, then we really want 
	\[\Pr[\exists h\in\calB: h\text{ is consistent}]\le \sum_{g\in\calB}\Pr[h\text{ is consistent}]\]
using the union bound.

Now fix $g\in\calB$ and we want to compute the probability of it being consistent. That is, $h(x_i)=c(x_i)$ for all $i$. Since the samples are independent, this probability is 
\[\prod_1^m\Pr[h(x_i)=c(x_i)]\le (1-\varepsilon)^n\]

Therefore the original probability is 
\[|\calB|(1-\varepsilon)^m\le|\calH|e^{-\varepsilon m}\le \delta\]
\end{prf}

\subsection{When does this work when we have infinite hypothesis classes?}
We saw in the first example that infinite classes can still be PAC-learnable, but where is the line drawn?

To see this, consider classifications of finite subsets. But any choice of threshold between any two points on $\bbR$ will give the same classification! So really we only have 
something like 5 different hypothesis (in the form of a threshold function at least). Compare that with $2^m$ behaviors one could consider in general!

More formally, if we are given a set $S=\langle x_1,\dots,x_m\rangle$ of instances, then we can consider the collection of all $\langle h(x_1),\dots, h(x_m)\rangle$ for all $h\in\calH$. We call this set 
$\Pi_\calH(S)$ and define 
\[\Pi_\calH(m)=\max_{|S|=m}|\Pi_\calH(S)|\]
and call this the \textbf{growth function.} We can use this function to get some sort of handle on the complexity of the problem, 
and in fact it plays a similar role in an analogous theorem to the one we saw earlier:
\begin{prop}
	Give $m$ training examples, with probability $1-\delta$ for all $h\in\calH$ if $h$ is consistent then 
	\[\operatorname{err}_D(h)\le\calO\left(\frac{\ln\Pi_\calH(2m)+\ln\frac{1}{\delta}}{m}\right)\]
\end{prop}
\begin{rmk}
	The proof of this result can't quite continue in the same way as before because we need to select a training set \textit{before we can know the hypothesis space}, 
	so the dependence gets in the way. There is a fancier proof but we won't do it here (although there will be another proof later that is similar).
\end{rmk}
\begin{rmk}
	Notice that if $\Pi_\calH(m)$ is polynomial, this reduces nicely and as $m$ gets larger, the error drops. Thus learning is possible.
\end{rmk}

\subsubsection{More on the growth function}
There are at least some cases when $\Pi_\calH(m)\in\calO(m^d)$ for some constant $d$. In the worst case, $\Pi_\calH(m)=2^m$. In fact, these are the only two cases that can happen! Furthermore,
it ends up that these two cases correspond exactly to when learning is possible! 

To see why, we need a new definition:
\begin{defn}
	A sample $S$ of size $m$ is \textbf{shattered} by $\calH$ if all possible behaviors/labelings are possible: that is, $\Pi_\calH(S)=2^m$.
\end{defn}
\begin{ex}
	Given $\calH$ to be the set of all closed intervals $[a,b]\subseteq\bbR$, any set of two points in $\bbR$ is shattered by $\calH$.
	But you cannot shatter a set of three points in $\bbR$.
\end{ex}
\begin{defn}
	The \textbf{Vapnik-Chervinenkis (or VC) dimension} is
	\[\operatorname{VCdim}(\calH)=\max\{|S|: S\text{ is shattered by }\calH\}\]
\end{defn}
\begin{rmk}
	So in the exercise above, $\operatorname{VCdim}(\text{intervals})=2$.
\end{rmk}
Another set of hypotheses are the linear threshold functions in $\bbR^n$, and the VC dimension here is $n+1$. If you fix your planes to be subspaces, the VC dimension is $n$.

\begin{rmk}
	A good heuristic measure for VC dimension is the number of parameters. There are pathological examples, but they are truly that.
\end{rmk}

We have a nice little lemma:
\begin{lem}[Sauer]
	If $d=\operatorname{VCdim}(\calH)$, then 
	\[\Pi_\calH(m)\le\sum_0^d\binom{m}{i}.\]
\end{lem}
\begin{rmk}
	A ``nicer'' upper bound leverages
	\[\sum\binom{m}{i}\le\left(\frac{em}{d}\right)^d\]
	whenever $m\ge d\ge 1$.
\end{rmk}

Plugging this into the bound on the error of a hypothesis, we get that (forgetting the relatively small log term) that the VC dimension $d$ gives us
a complexity measure---that is, it gives a bound on the error of a consistent hypothesis.

\subsection{Non-consistent Hypotheses}
The results so far have been great, but has always assumed that there is a consistent hypothesis to find. But what if our data is noisy? Then 
learning the concept is no longer about finding the concept that matches \textit{all} the given data.

Now we consider data $(x,y)\sim D$ to be a pair jointly sampled from some distribution $D$. We do basically the same 
thing to compute error:
\[\err_D(h)=\Pr_{(x,y)\sim D}[h(x)\ne y].\]
We are still trying to minimize the generalization error $\err(h)$ over $\calH$ given some sample $(x_i,y_i)$ sampled from $D$.
\begin{defn}
	Write the \textbf{empirical error} as 
	\[\widehat\err(h)=\frac{1}{m}\sum_1^m\delta_{h(x_i)\ne y_i}\]
\end{defn}
And then we can follow the route called \textit{empirical risk minimization} (ERM) that finds an empirical solution:
\[\hat h=\argmin_{h\in\calH}\widehat\err(h).\]

We want to get to the situation in which, with probability greater than $1-\delta$, for all $h\in\calH$,
\[|\err(h)=\widehat\err(h)|\le\varepsilon.\]
This result is a kind of \textbf{uniform convergence result} which is the kind of thing we are after. Obviously this is useful since it allows us 
to bridge the gap between imperfect information and the true concept.

Now $\delta_{h(x_i)\ne y_i}=1$ with probability $\err(h)$ and zero otherwise.

\begin{ex}
	A quick aside: Consider IID variables $Z_1,\dots,Z_m$ with $Z_i\in[0,1]$. Let $p=\bbE[Z_i]$ and then 
	let $\hat p=\frac{1}{m}\sum_i Z_i$ be the empirical mean.

	Then there is a result called \textbf{Hoeffding's inequality}, which claims
	\[\Pr[\hat p\ge p+\varepsilon]\le e^{-2e\varepsilon^2m}.\]

	Notice that what this says is that the empirical average converges to its expected value. So we can think of this 
	as a function converging
	\[f(Z_1,\dots,Z_m)\to\bbE[f(Z_1,\dots,Z_m)].\]
\end{ex}
This idea leads us to:
\begin{lem}[McDiarmid's Inequality]
	Suppose $f(z_1,\dots,z_m)$ is real valued such that chainging $z_i$ changes $f$ by at most $c_i$, and furthermore the $z_i$ are independent, but not necessarily identically distributed.
	Then you get a nice bound on the error (I didn't get it in time but I am sure it is online.)
\end{lem}

So if we let $Z_i=\delta_{h(x_i)\ne y_i}$ and let $p=\bbE Z_i=\err(h)$ and $\hat p=\widehat\err(h)$, then for any particular $h\in\calH$,
\[\Pr[|\err(h)-\widehat\err(h)|\ge \varepsilon]\le 2e^{-2\varepsilon^2 m}.\]
So when $\calH$ is finite, we can use the union bound to say 
\[\Pr[\exists h\in\calH: |\err(h)-\widehat\err(h)|\ge \varepsilon]\le 2|\calH|e^{-2\varepsilon^2 m}\]
then setting the RHS equal to $\delta$,  we get that with probability $1-\delta$, for all $h\in\calH$,
\[|\err(h)-\widehat\err(h)|\le\calO\left(\sqrt{\frac{\ln|\calH|+\ln\frac{1}{\delta}}{m}}\right)\]
or in another (slightly weaker but sometimes more suggestive) form:
\[\err(h)\le\widehat\err(h)+\calO\left(\sqrt{\frac{\ln|\calH|+\ln\frac{1}{\delta}}{m}}\right)\]
\begin{rmk}
	Notice that this really (finally) ties together how several aspects impact learning: it relates the true error to the sample size, the complexity class of the concept, 
	and the consistency of the hypothesis with the sample data.
\end{rmk}

\begin{rmk}
	Notice that as the complexity of your hypotheses increase, we expect the empirical error to decrease, but if we assume that 
	the above is truly a good bound for $\err(h)$, we would expect that the true error will begin by dropping, but eventually the growth term from $\ln|\calH|$
	will overtake things and cause the error to rise again.

	This is one way to think about the concept of \textbf{overfitting}: the complexity of your hypothesis functions can incorporate more and more pathological functions that are less likely to be realistic.
\end{rmk}

\subsection{Another Complexity Measure}
Suppose we have a labelled sample $S$ consisting of $(x_i,y_i)$, where we can now assume the label space is $\pm 1$.
Then if we have a hypothesis, we can compute the training error $\widehat\err(h)$:
\[\widehat\err(h)=\frac{1}{m}\sum_i\delta_{h(x_i)\ne y_i}=\frac{1}{m}\sum_i \frac{1-y_ih(x_i)}{2}=\frac{1}{2}-\frac{1}{2}\left(\frac{1}{m}\sum_i y_ih(x_i)\right)\]
and the term in parentheses is another quantity we can consider which is just a constant away from training error.

So we can consider computing 
\[\bbE_{\mathbf\sigma}[\max_{h\in\calH}\sum_i\sigma_ih(x_i)]\]
where $\sigma_i$ are $\pm 1$ equally likely. In the case that $|\calH|=1$, we get that this value is zero and when $\calH$ shatters $S$ (the set of all $m$-tuples of $\pm 1$), 
the value is 1. These serve as the two extremes, obviously.

We would like to work with this towards describing convergence (and rates thereof) towards the generalization error.
To do this, we need to broaden the context slightly. Let $f:Z\to\bbR$ (standing in for the hypothesis), a space $\calF$ ($\calH$), and define the \textbf{(empirical) Rademacher complexity} to be 
\[\hat R_\scrF(S)=\bbE_\sigma\left[\sup_{f\in\scrF}\frac{1}{m}\sum_i\sigma_if(z_i)\right]\]
and the expected Rademacher complexity is 
\[R_\scrF(m)=\bbE_S[\hat R_\scrF(S)]\]

Then we want to show that for all $f\in\scrF$ that $\hat\bbE_S[f]\to\bbE[f]$. Specifically,
\begin{thm}
	Let $\scrF$ be a family of functions $f:Z\to[0,1]$ and let $S$ be a collection of $z_i$ where $z_i\sim D$. Then with probability $1-\delta$,
	and for all $f\in\scrF$,
	\[\bbE[f]\le\hat\bbE_S[f]+2\cdot\Psi+\calO\left(\sqrt{\frac{\ln(1/\delta)}{m}}\right)\]
	where $\Psi$ is either $\hat R_\scrF(S)$ or $R_\scrF(m)$.
\end{thm}
\begin{prf}
	Let $\Phi(S)=\sup_{f\in\scrF}(\bbE[f]=\bbE_S[f])$. Then first we show (via McDiarmid) that 
	\[\Phi(S)\le \bbE_S[]\Phi(S)]+\calO\left(\sqrt{\frac{\ln 1/\delta}{m}}\right).\]
	So then we have reduced the problem with finding some bound for $\bbE_S[\Phi(S)]$.

	Then let $S'$ to be a hypothetical second sample (called the ``ghost sample''). Compute 
	\[\bbE_S[\Phi(S)]=\bbE_S[\sup_f(\bbE[f]-\hat\bbE_S[f])]\le \bbE_{S,S'}[\sup(\hat\bbE_{S'}[f]-\hat\bbE_S[f])]\]
	There is something to prove in the last line there but he didn't show us. He said there was a slide somewhere. :)

	For the next step, go through the two samples $S$ and $S'$ one index at a time and for each $i$ with equal probability either swap $z_i$ and $z_i'$ or do nothing.
	Call the resulting samples $T$ and $T'$. But then the following two expressions have the same distribution:
	\[\hat\bbE_{S'}[f]-\hat\bbE_S[f]\quad\text{and}\quad \hat\bbE_{T'}[f]-\hat\bbE_T[f]\]
	and so if we let $\sigma_i$ denote the random variable for whether we swapped at the $i^{th}$ step ($+1$ if we swapped $z_i$ and $z_i'$ and $-1$ otherwise),
	the latter is 
	\[\frac{1}{m}\sum_i\sigma_if(f(z_i')-f(z_i))\]
	so we get 
	\[\bbE_{S,S'}[\sup_f(\hat\bbE_{S'}-\hat\bbE_S[f])]=\bbE_{S,S'}\left[\sup_f\left(\frac{1}{m}\sum_i\sigma_if(f(z_i')-f(z_i))\right)\right]\]

	Finally we can show (another slide) that this value is at most $2R_\scrF(m)$ and then use McDiarmid's once more to show that the expected Rademacher complexity is close enough to that of the empirical RC.
\end{prf}

Now the whole point was to show that $\err(h)\approx \widehat\err(h)$. One can show the empirical Rademacher complexity of the class of loss functions comprised of indicator functions $l(x,y)=\delta_{h(x)\ne y}$ is twice that of $\calH$ itself.
You can easily plug this in to get $\bbE[l_h]$. Then you can get what you want.

Now if $|\calH|<\infty$, (careful, he made a notation error earlier so I am switching subscripts and arguments in Rademacher complexities--they are still the same)
\[\hat R_S(\calH)\le\sqrt{\frac{2\ln|\calH|}{m}}.\]

If the space is infinite, then we choose a representative hypothesis for each behavior seen in applying all hypotheses to $S$. Call this space $\calH'$ and notice that $|\calH'|=|\Pi_\calH(S)|.$
Then one can show that the empirical Rademacher complexity of $\calH'$ is the same as that of $\calH$! 

\subsection{Weak Learning}
At the end of the last lecture we discussed the ideal of weak vs. strong learning. In weak learning, we only require that there is an algorithm yielding a hypothesis $h$ such that $\err_R(h)\le\frac{1}{2}-\gamma$ for some $\gamma>0$.
But surprise!
\begin{thm}
	A class is strongly learnable if and only fif it is weakly learnable.
\end{thm}
Today we will discuss why. We want to convert any algorithm that weakly learns a class into one that strongly learns the class. The idea here is that a class $\calC$ is either completely (PAC) learnable or else 
there is no algorithm that learns the concept any better than random chance.

\subsubsection{The Ada Boost Algorithm}
We will assume we are given a weak learning algorithm $A$ and data $(x_1,y_1),\dots,(x_m,y_m)\sim\scrD$. We want to find a hypothesis $H$
whose error $\err_\scrD(H)$ is arbitrarily small.
\begin{defn}
	A theorem that can take a weak learning algorithm and convert it into an $\varepsilon$-good hypothesis is called a \textbf{boosting algorithm.}
\end{defn}

The basic idea is what one would expect; run $A$ a total of $T$ times, extracting weak hypotheses $h_1,\dots,h_T$ and somehow combine these in a way to get a strong hypothesis $H$. There is a problem with just feeding the same data in 
to $A$ over and over (e.g. $A$ may be deterministic). So instead we need to find a way of creating distributions $D_1,\dots, D_T$ that ``focus in'' on the bad/difficult examples that the weak hypotheses fail on.

\begin{rmk}
	Notice that the definition of learnability means that there has to be an algorithm that works \textbf{for all true distributions}. So for instance, the fact that 
	we can learn something about the weather, but can't get it $\varepsilon$-good. This is because our algorithms only work well (enough) on a particular distribution.
\end{rmk}

Let $\varepsilon_t=\err_{D_t}(h_t)=\frac{1}{2}-\gamma_t$ and notice $\gamma_t\ge\gamma>0$. Then define the following distributions: $D_1$ will be the uniform distribution, so the weight on the $i^{th}$ training 
datum: $D(i)=\frac{1}{m}$. Then define the rest iteratively:
\[D_{t+1}(i)=\frac{D_t(i)}{Z_t}\cdot \left\{\begin{array}{lr}e^{-\alpha_t},& h_t(x_i)=y_i\\
e^{\alpha t}, & \text{otherwise}\end{array}\right.=\frac{D_t(i)e^{-\alpha_ty_ih_t(x_i)}}{Z_t}\]
where $Z_t$ is a normalization constant and 
\[\alpha_t=\frac{1}{2}\ln\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)>0\]

Then we output $H$ which is defined as 
\[H(x)=\operatorname{sign}\left(\sum_1^T\alpha_t h_t(x)\right).\]
That is, we let a weighted sum (according to the error on the provided distribution) tells us what to select.

\begin{thm}
	\[\widehat\err(h)\le \prod_1^T2\sqrt{\varepsilon_t(1-\varepsilon_t)}=\prod\sqrt{1-4\gamma_t^2}\le\exp\left(-2\sum\gamma_t^2\right)\]
	So if $\gamma_t>\gamma$, we get 
	\[\widehat\err(h)\le e^{-2\gamma^2T}\]
\end{thm}
\begin{prf}
	He went too fast. Slides are somewhere supposedly.
\end{prf}

So we get a bound on training error, great! Let's turn it into something about generalization error:
if $h_t\in\calH$ and $d=\operatorname{VCdim}(\calH)$, then with probability $1-\delta$ (see problem session)
\[\err(H)\le\widehat\err(H)+\tilde\calO\left(\sqrt{\frac{Td+\ln 1/\delta}{m}}\right)\]
where $\tilde\calO$ means that we ignore log factors.

Then something that may be surprising is that as $T$ grows, we may expect that eventually we will overfit our training data, but in practice you can often run this very far out and just keeping better results!
in fact, your output $H$ will continue to get better even after it achieves zero error on the training set (since the underlying $h_t$ are still incorrect).

But this points to the issue: the test error doesn't tell the whole story. Instead we are interested in measuring the \textit{confidence} the algorithm has in the combined classifier. So how do we do that?
Recall that to determine $H$ we held an ``election'' for the best output. What we are interested in here is the \textbf{margin} (the difference between the fraction of votes for and against an output):
we call the margin the \textit{weighted fraction of the $h_t$'s correct minus the weighted fraction of the $h_t$'s incorrect.}

Doing some math, you get 
\[\text{margin}=y\cdot\frac{\sum \alpha_t h_t(x)}{\sum\alpha_t}\]

Since we are about out of time, we are sprinting to the finish line: Doing an analysis, we can show that this algorithm tends to push the margins towards the positive. Then you prove that 
this phenomenon tends to create better generalization performance. Note that we can actually remove the dependence on $T$ by doing the following:
\[\err(H)\le S+\tilde\calO\left(\frac{d/\theta^2+\ln 1.\delta}{m}\right)\]
where $S$ is the fraction of training examples with margin $\le\theta$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convex Optimization}
This series was given by Sebastien Bubeck from Microsoft Research.

\subsection{Fundamentals}
Let $K\subseteq \bbR^n$ be a convex set: that is one such that the straight line segment between two points in $K$ is contained entirely in $K$.
\begin{defn}
	The map $f:K\to\bbR$ is called \textbf{a convex function} if 
	\[f((1-\gamma)x+\gamma y)\le (1-\gamma)f(x)+\gamma f(y).\]
\end{defn}
\begin{rmk}
	Then if $f$ is differentiable, this implies
	\[\frac{f(x+\gamma(y-x))-f(x)}{\gamma}\le f(y)-f(x)\]
	and as $\gamma\to 0$, the LHS tends to $\nabla f(x)\cdot (y-x)$. Thus
	\[f(y)\ge f(x)+\nabla f(x)\cdot(y-x).\]
\end{rmk}

Then the goal of this course is to ``find'' $\operatorname{argmin}_{x\in K}f(x)$. A question we ask ourselves: how are $f$ and $K$ specified? Mostly in these lectures we will be 
focusing on \textit{simple} $K$ and functions $f$ such that $\nabla f(x)$ can be computed (for any $x$).

\subsection{Examples in machine learning}
\subsubsection{Regression}
Here we have a dataset: $(a_i,y_i)\in\bbR^n\times\bbR$ are the elements. Then we want to find a rule that assigns $y$ to $\mathbf a$. We focus on \textit{linear} rules.
That is, we are looking for a linear space in $\bbR^{n+1}$ that approximates the ``true'' values.

Tentatively we let our function be $a\mapsto x\cdot a$, parameterized by $x\in\bbR^n$. Then for each $x$ we can evaluate how the tentative function
fits our data. More specifically:
\[\frac{1}{m}\sum_1^m l(x\cdot a_i,y_i)\]
where $l$ is some loss function.

There are several loss functions one might consider:

\noindent\textbf{Least Squares}: 
\[l(u,v)=(u-v)^2\] 
The upshot here, however, is that the evaluation function above in this case is convex!
The idea here is that we are applying a linear function to a convex function. The other nice thing is that if you make a modeling assumption about the 
distribution underlying the label distribution, nice things happen.

Specifically, assume that $y\sim\calN(a\cdot \underline x,\sigma^2)$ where $\underline x$ is the \textit{true value}.
Notice we haven't put any assumption on the draw distribution, just the relationship between the (fixed) $a_i$ and $y$ given the truth $x$. One can compute 
\[\frac{1}{2\pi\sigma^2}\exp(-\frac{1}{2\sigma}(y_i-a_i\cdot x)^2).\]

Thus the likelihood of the entire data set is the product:
\[\frac{1}{(2\pi\sigma^2)^{m/2}}\exp(-\frac{1}{2\sigma^2}\sum_1^m(y_i-a_i\cdot x)^2)\]
and so we recover the least squares loss function as the objective we want to minimize. That is, the maximum likelihood estimator minimizes least squares fit.

\subsubsection{Classification}
Now restrict $y\in\{\pm 1\}$. A natural loss is 
\[l(u,v)=\delta_{\operatorname{sign}(u)\ne\operatorname{sign}(v)}.\]
This is a problem for this class since it is non-convex.

So instead we define \textbf{support vector machines} using the loss 
\[l(u,v)=\max(0,1-uv)\]
which is convex 

Another example is the \textbf{logistic loss}
\[l(u,v)=\log(1-e^{-uv})\]
which you can see is a smooth upper bound on the 0-1 loss we started with. Minimizing this function is equivalent to minimizing the MLE for the \textit{logistic model:}

Denote $p(a)=\bbP(+1|a)$. Then we assume that the ``scale'' of our probability is given by a linear function:
\[\log\left(\frac{p(a)}{1-p(a)}\right)=\underline x\cdot a\Leftrightarrow p(a)=\frac{1}{1+e^{-\underline{x}\cdot a}}\]
and then the \textbf{logistic loss} is the negative log-likelihood of the logistic model.

\subsubsection{Graphical Models}
Given $a_1,\dots,a_m\in\bbR^n$, we want to infer the correlation structure of the variables (how are they all related).

Let's start from a (Gaussian) model: we assume we are drawing IID samples from $\calN(\mu,\underline\Sigma)$. A fact: if $(\Sigma^{-1})_{ij}=0$, then $x_i$ and $x_j$ are independent, conditioned on the rest.
Thus the goal is to estimate $\Sigma^{-1}$. So if the truth is $\Sigma$, the density of $a_1,\dots,a_n$ is 
\[\frac{1}{\sqrt{(2\pi)^n\det(\Sigma)}}\exp(-\frac{1}{2}(a_i-\mu)^T\Sigma^{-1}(a_i-\mu))\]
and by taking the negative log likelihood:
\[c+\frac{m}{2}\log\det\Sigma+\frac{1}{2}\sum_1^m(a_i-\mu)^T\Sigma^{-1}(a_i-\mu)=C+\frac{m}{2}\left(-\log\det\Sigma^{-1}+\operatorname{tr}(\Sigma^{-1}S)\right)\]
where $c$ is some constant and $S$ is the sample covariance matrix. Then to estimate $\Sigma^{-1}$ via MLE, we want to solve 
\[\operatorname{argmin}_{X>0}-\log\det(X)+\operatorname{tr}(XS)\]
where this is convex in $X$ (this is not obvious but we will see it in the problem session).

If you know that the true matrix is sparse, you may add a penalty term $\lambda \|X\|_1$. Similarly in regression you may add either $\|x\|_1$ or $\|x\|_2^2$.
In either case, these add curvature to your objective function. This may speed up the optimization process.

\subsubsection{Unsupervised Learning}
Finally we want to talk about \textit{clustering}. Notice that we have been slowly changing our dataset. First we dropped labels and now we are going to represent our dataset as a graph $G=(V,E)$ of interactions.
We would like to infer some partitioning of the vertices.

As a modelling assumption, we use the stochastic block model. There is a hidden partitioning $\sigma\in \{-1,1\}^|V|.$ $G$ is generated from the distribution
\[(i,j)\in E\text{ with probability }\left\{\begin{array}{lr}p & \sigma_i=\sigma j\\ q, \text{otherwise}\end{array}\right. \]
where we assume $q<p$. Furthermore we assume all connections are independent.

The the likelihood of $x\in\{-1,1\}^n$ is given as follows: let $A=(a_{ij})$ be the adjacency matrix of $G$. Then the likelihood is 
\[\prod_{ij}\left[\frac{1+x_ix_j}{2}\left(pa_{ij}+(1-p)(1-a_{ij})+\frac{1-x_ix_j}{2}(qa_{ij}(1-q)(1-a_{ij}))\right)\right]\]
this is non-convex! But we can look at a convex upper bound and we're good! Next time we'll talk about gradient descent.

\subsection{Gradient Descent}
This is the most simple algorithm we will study (also one of the most used ones). It is an iterative method where we define 
\[x_{t+1}=x_t-\eta\nabla f(x_t)\]
which dates back to the mid 1800's (likely by Cauchy). Notice that this is similar to Newton's method, but the latter requires inverting an $n\times n$ matrix 
and this was an attempt to avoid that difficult computation.

Why does this work? By Taylor's theorem, $f(y)\approx f(x)+\nabla f(x)\cdot (y-x)$. The idea here is that the negative $\nabla f(x)$ direction is in the direction of the minimum, so we just follow that.

Now if we have a convex function, where $f(x)-f(y)\le\nabla f(x)\cdot (x-y)$, we can set $y=x^\ast$ and define 
\[\delta(x):=f(x)=f(x^\ast)\le\nabla f(x)\cdot (x^\ast-x)=-\nabla f(x)\cdot(x^\ast-x)\]
then the idea is that progress in the $x^\ast-x$ direction (that is towards the target) is bounded below by $\delta(x)$.
That is to say: \textit{the rate of decrease of the instance to OPT is lower bounded by the suboptimal gap.}

There is a small problem, namely that we also take a (hopefully small) step away from the direct-line path from $x$ to $x^\ast$. To fix this, we
use continuous time analysis:
\[\frac{d}{dt}x(t)=-\nabla f(x(t))\]
which we call the \textbf{gradient flow.} Then 
\[\frac{d}{dt}\frac{1}{2}\|x(t)-x^\ast\|^2=\left(\frac{d}{dt}(x(t)-x^\ast)\right)\cdot(x(t)=x^\ast)=-\nabla f(x(t))\cdot (x(t)-x^\ast)\le -\delta(x(t))\]
and integrating over time:
\[\int_0^\tau \frac{1}{2}\|x(t)-x^\ast\|^2 \mathrm{d}t\le\int_0^\tau \delta(x(t))\mathrm{d}t\]
and using the FTOC and dropping the $\tau$ term (and normalizing):
\[\frac{1}{\tau}\int_0^\tau\delta(x(t))\mathrm dt\le \frac{1}{2\tau}\|x(0)-x^\ast\|^2:=\frac{r_0^2}{2\tau}\]
and so finally
\[\delta\left(\frac{1}{\tau}\int_0^\tau x(t) \mathrm{d}t\right)\le\frac{r_0^2}{2\tau}.\]

Of course this isn't a discrete process, so it's useless for coding! So we want to run gradient descent with step size $\eta$ for $T$ steps and approximate this integral with a finite sum from $1$ to $T$.
Then if we further impose that $f$ is Lipshitz ($\|\nabla f(x)\|\le L$) then the error in this approximation is at worst $\eta L^2$.
So if we are trying to measure the total error of our discretized algorithm from the true value, we get an error bound 
\[\frac{r_0^2}{T\eta}+\eta L^2\le \frac{r_0L}{\sqrt{T}}\]
for a ``good'' choice of $\eta=\frac{r_0}{L\sqrt{T}}$.
\subsubsection{Discrete Time Analysis}
\begin{thm}
	Let $g_1,\dots,g_T$ be arbitrary vectors in $\bbR^n$ and that $x_{t+1}=x_t-\eta g_t$. Then for any $x\in\bbR^n$,
	\[\sum_1^Tg_t(x_t-x)\\le \frac{\|x_1-x\|^2}{2\eta}+\eta\sum_1^T\|g_T\|^2\]
\end{thm}
\begin{cor}
	If $g_t=\nabla f(x_t)$ ($f$ convex) and $\|nabla f(x_t)\|\le L$, then
	\[f\left(\frac{1}{T}\sum_1^Tx_t\right)-f(x^\ast)\le\frac{1}{T}\sum_1^T(f(x_t)-f(x^\ast))\le\frac{1}{T}\sum \nabla f(x_t)(x_t-x^\ast)\le\frac{r_n^2}{2\eta T}+\eta L^2\] 
\end{cor}
The proof here is really just one line of computation but I missed it. Interestingly, the proof gives us \textit{equality} rather than inequality. I suppose that it's nice to use it as if it were a bound.

\subsubsection{Projective Gradient Descent}
A generalization is optimization with some constraint. So if we have a bounded convex set $K$, we just follow each step in gradient descent with a projection onto the closest point in $K$ (should we leave it).
This is only better for us since 
\begin{lem}
	$\|P_K(x)-z\|\le\| x-z\}$ for any $z\in K$.
\end{lem}
\begin{rmk}
	Notice that we are assuming here that both $K$ and $f$ are convex. Without convexity we definitely don't have this.
\end{rmk}

\subsection{Stochastic Gradient Descent}
We begin with a corollary of the theorem we just proved for standard GD:
\begin{cor}
	Say that $g_t$ is a random variable such that $\bbE[g_t|x_t]=\nabla x_t$ and that $\bbE[\|g_t\|^2]<B^2$.
	Then 
	\[\bbE[f\left(\frac{1}{T}\sum_1^T x_t\right)-f(x^\ast)]\le\frac{r_1^2}{2\eta}+\frac{\eta}{2}TB^2\le\frac{r_1B}{\sqrt{T}}\]
	given an optimal choice of $\eta$.
\end{cor}

So then we can use this result to define SGD, Stochastic Gradient Descent. Let $f(x)=\bbE_{(a,y)}l(x,(a,y))$.
The idea here is that we don't have the actual gradient of the loss function, but we have access (under the statistical learning framework) to a continuous stream of IID data.
So we define the \textbf{stochastic gradient}:
\[g_t=\nabla_x l(x_t,(a,t,y_t))\]
that is, compute the $x$ gradient of $l(x,(a_t,y_t))$ and evaluate at $x_t$. Then basically this pos right into the above corollary.

\begin{rmk}
	here the notation varies from statistical learning, so think of $x$ as our hypothesis (the thing we're optimizing over) and $(a,y)$ denotes a sample from the dataset.
\end{rmk}

Note that here you only get one pass on the data (although here you can directly control the generalization error as opposed to bounding it with testing error in the other class).

Instead, we can do multi-pass SGD where at each step we sample uniformly data points from your dataset (possibly having much more steps than there are data points) but then your function converges to an unbiased estimator for the test error rather than the generalization error.

\subsection{Optimality}
Recall that we had two kind of bounds on expectation: $\frac{r_0L}{\sqrt{T}}$ and $\frac{r_0B}{\sqrt{T}}$ where $r_0=\|x_0-x^\ast\|_2$,
$L=\sup_x\|\nabla f(x)\|$ and $B\ge \bbE[\|g_t\|^2]$ where $g_t$ is an unbiased estimator for $\nabla f(x_t)$, which we think of as a random oracle.

\subsubsection{Basic minimax principle}
We want to show that for all (possible randomized) algorithms such that there exists a function/oracle such that the error rate of gradient descent is optimal.
It suffice to find a \textbf{random} function such that any deterministic algorithm on this function has error rate bigger than $\frac{\square}{\sqrt{T}}$ in expectation.

This is a consequence of the fact that $\min\max\ge\max\min$ (these are read in the opposite order than you'd think).

\subsubsection{Random oracle lower bound} 
Let $K=[-1,1]$ and $f(x)=\xi x$ where $\xi=\pm\varepsilon$ randomly with equal probability.
Then our random oracle is $g_t\sim \calN(\xi,1)$ (equivalently $g_t=\xi+Z_t$ for $Z_i$ IID $\calN(0,1)$). Intuitively, after $T$ queries the best estimator for $\xi$ is the 
empirical mean $\frac{1}{T}\sum_1^T g_t\sim\calN(\xi,\frac{1}{T})=\xi+\frac{1}{\sqrt{T}}Z$ where $Z$ is $\calN(0,1)$.

So if $\varepsilon<\frac{1}{\sqrt{T}}$, there is a constant probability to incorrectly estimate the sign of $\xi$. Formally,
\begin{lem}
	For any $\Psi:\bbR^T\to [-1,1],$
	\[\bbP(\operatorname{sign}(\Psi(g_1,\dots,g_T))=\operatorname{sign}(\xi))\le\frac{1}{2}+\sqrt{T\varepsilon^2}\]
\end{lem}

\subsubsection{Deterministic oracle model} 
let $v_1,\dots,v_{T+1}\in\bbR^n$ be a random orthonormal set. Let $f(x)=\max_{i\in[T+1]}v_i\cdot x$. We also let $K=\{x\in\bbR^n:\|x\|_2=1\}$. 
Then $\nabla f(x)=v_i$ for $i=\argmax_{j} v_j\cdot x$ (except for a set of measure zero, where we can use subgradients).

After $T$ queries, one of the $v_i$ remains unknown, say $T+1$. So for any $x, v_1,\dots, v_T$,
\[\bbE_f[f(x)|v_1,\dots,v_T]\ge\bbE_f[v_{T+1}\cdot x]|v_1,\dots,v_T]=0\]
And so $f^\ast=\min_{x\in B}f(x)\le-\frac{1}{\sqrt{T+1}}$. 
\begin{rmk}
	It is very important that $T<n$, since we are looking for an orthonormal set! But the bound is not dependent on the dimension! So although we got a similar lower bound on $T$, 
	we still may be able to do better when we have lower dimension.
\end{rmk}

To see this, we need 
\begin{prop}[Gr\"unbaum's Inequality]
Let $K$ be a convex body. For any half-space $H$ that contains the center of gravity of $K$. Then 
\[\operatorname{vol}(K]\cap M)\ge\frac{1}{e}\operatorname{vol}(K).\]
\end{prop}

How do we get half spaces by making gradient queries? Well by convexity, $f(x)\ge f(x_t)+\nabla f(x_t)(x-x_t)\ge f(x_t)$ for any $x\in H_t=\{x:\nabla(x_t)\cdot(x-x_t)\ge 0\}$.

Then we have the \textbf{center of gravity algorithm}: let $K_0=K$ and for any $T\ge 0$, $x_t=cg(K_t)$ and set $K_{t+1}=K_t\setminus H_t$. Then by Gr\"unbaum, $\operatorname{vol}(K_t)\le(1-\frac{1}{e})\operatorname{vol}(K_{t-1})\le(1-\frac{1}{e})^t\operatorname{vol}(K)$
\begin{thm}
	Say $f:K\to [-1,1]$. After $\calO(n\log(1/\varepsilon))$ steps, $f(x_t)-f(x^\ast)\le\varepsilon$.
\end{thm}
\begin{prf}
	After this many steps, $\operatorname{vol}(K_t)\le\varepsilon^n\operatorname{vol}(K)$. We claim $\exists \Omega\subseteq K$ such that $\forall x\in\Omega$, $f(x)-f(x^\ast)\le\varepsilon$ and $\operatorname{vol}(\Omega)\ge \varepsilon^n$
\end{prf}

\subsubsection{Beyond \texorpdfstring{$\frac{1}{\sqrt{T}}$}{1/sqrt(T)} in high dimensions}
The best way to get around the lower bounds we've cooked up for ourselves is to put some further restrictions on our functions:
\begin{defn}
	A function $f$ is $\alpha$\textbf{-strongly convex} if $f(x)=g(x)+\frac{\alpha}{2}\|x\|_2^2$, where $g$ is convex.
\end{defn}
\begin{rmk}
	This property is equivalent with the fact that 
	\[f(x)-f(y)\le \nabla f(x)\cdot (x-y)-\frac{\alpha}{2}\|x-y\|_2^2.\]
	So we have that $f$ curves away from its tangent at that point like a parabola with coefficient $\alpha$.
\end{rmk}

Let's do the continuous-time analysis of the gradient flow $\frac{d}{dt}x(t)=-\nabla f(x(t))$. Let $r^2(t)=\|x(t)-x^\ast\|^2$ and $\delta(t)=f(x(t))-f(x^\ast)$.
Then the calculation we get is 
\begin{align*}
	\frac{d}{dt} r^2(t)=-\nabla f(x(t))\cdot(x(t)-x^\ast)\\
	&\le -\delta(t)-\frac{\alpha}{2}\|x(t)-x^\ast\|^2_2.
\end{align*}
Thus $\frac{d}{dt}r^2(t)\le -\alpha r^2(t)$ and so $r^2(t)\le r^2(0)e^{-\alpha t}$. All this to say that gradient flow gets to OPT in time $\calO(\frac{1}{\alpha}\log(r_0^2/\varepsilon))$. By comparison with out strong convexity, 
we had that 
\[\frac{1}{\tau}\int_0^\tau\delta(t)\mathrm{d}t\le \frac{r^2_0(0)}{\tau}\]
and so our time is $\calO(r_0(0)/\varepsilon)$.

By switching from gradient flow to gradient descent, we pick up a discretization error! let $\eta L^2$ be the error from gradient flow at $\tau=T\eta$.
Then through some analysis, we get a $\tilde\calO(L^2/\alpha T)$ rate.

\brk

Next we can talk about \textbf{smoothness}.
\begin{defn}
	A function $f$ is $\beta$-smooth if $\nabla f$ is $\beta$-Lipshitz.
\end{defn}
Intuitively, we are looking to improve the discretization error. Before we could overshoot our maximum easily because the gradient could change sharply.
Now, we are forcing it to tend (loosely) to zero as you get closer to the optimum. Notice
\[\|\nabla f(x+\eta\nabla f(x))-\nabla f(x)\|\le\beta\eta\|\nabla f(x)\|\]
which suggests that $\eta\propto\frac{1}{\beta}$ is fine for the discretization. In this case we get convergence like $r^2(0)\beta/T$ (roughly $1/\varepsilon$)
and if we combine both $\alpha$-strong convexity and $\beta$-smoothness, you get a parameter 
\[\kappa=\frac{\beta}{\alpha}\]
called the \textbf{Condition number} and we get growth like $\kappa\log(1/\varepsilon)$

\subsubsection{A formal inequality}
\begin{prop}
	Let $x^+=x-\frac{1}{\beta}\nabla f(x)$. Then 
	\[f(x^+)-f(x)\le\frac{-1}{2\beta}\|\nabla f(x)\|_2^2.\]
\end{prop}
\begin{prf}
	We begin by claiming 
	\[f(x)-f(y)\le\nabla f(y)\cdot(x-y)+\frac{\beta}{2}\|x-y\|_2^2\]
	which implies that 
	\[f(x^+)-f(x)\le\grad f(x)\cdot(x^+-x)+\frac{\beta}{2}\|x^+-x\|_2^2\]
	and unfolding this 
	\[f(x^+)-f(x)\le-\frac{1}{2\beta}\|\nabla f(x)\|^2.\]
\end{prf}
\begin{defn}
	A point $x$ is called \textbf{$\varepsilon$-stationary point of $f$} if $\|\grad f(x)\|\le \varepsilon$.
\end{defn}
\begin{rmk}
	The above inequality shows that gradient descent on a bounded, smooth, non-convex function finds an $\varepsilon$-stationary point in $1/\varepsilon^2$ steps.
\end{rmk}
\begin{ex}
	Let $f(x)=\frac{1}{2m}\|Ax-b\|^2$ where $A$ is $m\times m$. Then the Hessian is 
	\[\grad^2 f(x)=\frac{1}{m}A^TA=\frac{1}{m}\sum_1^ma_ia_i^T\]
	where the $a_i$ should be thought of as feature vectors. Then the above quantity is the sample covariance matrix of the features.

	In this case, $\alpha=\lambda_{min}(\frac{1}{m}A^TA$ and $\beta=\lambda_{max}(\frac{1}{m}A^TA)$, so if $\|a_i\|_2\le 1$ then $\beta\le 1$.
\end{ex}
\begin{rmk}
	It is natural to ask how we can get a better value of $\beta.$ One way is to convolve your function $f$ with some noise $z$: $\hat f(x)=f(x+z)$.

	Sometimes $f$ is nonsmooth, but can be written as $f=g+r$ where $g$ is smooth and $r$ is ``simple''. Typically one takes $r(x)=\|x\|_1$.
\end{rmk}
\subsection{Acceleration}
It turns out that $1/\sqrt{\varepsilon}$ (for smooth) and $\sqrt{\kappa}\log(1/\varepsilon)$ for smooth and strongly convex functions are achievable!

What could be better than gradient descent? Well here's a crazy idea:
\[x_{t+1}=\argmin_{\operatorname{span}(\grad f(x_0),\dots,\grad f(x_t))}[f(x)]\]
where $[f(x)]=\sum_{s=1}^t\lambda_s\grad f(x_s)$. THen if $f$ is quadratic, it turns out that $\lambda_s$ has a closed form formula!

\subsubsection{Nemiraski's Acceleration (1982)}
Let 
\[x_{t+1}=\argmin_{x\in P_t} f(x)\]
where $P_t=\operatorname{span}(x_t^+,\sum_1^t \lambda_s\nabla f(x_s))$.

Then doing some analysis, notice
\[\sum_1^T\lambda_s\delta_s\le\sum_1^T\lambda_s\grad f(x_s)\cdot(x_s-x^\ast)=\sum_1^T\lambda_s\grad f(x_s)\cdot(-x^\ast)\]
where the last equality follows since $x_s\in P_{s-1}$ implies that $\grad f(x_s)\in P_{s-2}^\perp$. Thus 
\[\|x^\ast\|\cdot\sqrt{\sum_1^t\lambda_s^2\|\grad f(x_s)\|^2}\]
since $\grad f(x_{T+1})\perp \sum_1^T\lambda_s\grad f(x_s)$. But then by smoothness, 
\[\frac{1}{2beta}\|\grad f(x_s)\|^2\le \delta_s-\delta_{s+1}\]
and so 
\[\sum_1^R\lambda_s\delta_s\le\|x^\ast\|\cdot\sqrt{2\beta\sum_1^T\lambda_s^2(\delta_s-\delta_{s+1})}.\]

Now pick some $\lambda_s$ such that $2\beta(\lambda_s^2-\lambda_{s-1}^2)=\lambda_s$ and after some juggling things, 
\[\delta_T\le\frac{\beta\|x^\ast\|^2}{T^2}\]
and so we get an algorithm that runs in time like $1/\sqrt{\varepsilon}$.

\subsubsection{Nesterov's AGD (1983)}
The key breakthrough in this algorithm is that all that matters about the past is the momentum.
\begin{defn}
	The \textbf{momentum} if an iterative algorithm is 
	\[d_t:=\gamma_t(x_t-x_{t-1}).\]
\end{defn}

The began with Polyak's \textit{heavy ball method.} Let $x_{t+1}=x_t^++d_t$. This method alone doesn't accelerate (can be seen on simple quadratic functions).
Instead, Nestrov's uses
\[x_{t+1}=(x_t+d_t)^+.\]

Now for some of the analysis. Unfortuately this is notoriously ``magical'', so hold onto your pants. Denote by $g_t=-\frac{1}{\beta}\grad f(x_t+d_t)$. Thus $x_{t+1}=x_t+d_t+g_t.$
Then 
\[\delta_{t+1}-\delta_t=f(x_t+d_t)^+)-f(x_t)\le f(x_t+d_t)=\frac{1}{2\beta}\|\grad f(x_t+d_t)\|_2^2-f(x_t)\]
by the smoothness of $f$. Then by convexity,
\[\delta_{t+1}-\delta_t\le \grad f(x_t+d_t)\cdot d_t-\frac{1}{2\beta}\|\grad f(x_t+d_t)\|_2^2=-\beta g_t\cdot d_t-\frac{\beta}{2}\|g_t\|_2^2.\]

But notice that 
\[\delta_{t+1}=f((x_t+d_t)^+)-f(x^\ast)\le=\beta g_t\cdot(x_t+d_t-x^\ast)-\frac{\beta}{2}\|g_t\|_2^2.\]
And the plan is to take a combination of these two statements to get a weighted sum of the $\delta_i$ on the left and a telescopic sum on the right. It wasn't that bad. Basically just put in some parameters and then choose them to be the ``right ones''.
THe punchline is approximate convergence like $\delta_{T+1}\le\frac{\beta\|x_1-x^\ast\|^2}{T^2}.$

\subsection{Some variance reduction techniques}
\subsubsection{The finite sum problem}
Let's say we have a function $f(x)=\frac{1}{m}\sum_i f_i(x)$ where $f_i$ is $\beta$-smooth (note that in particular this implies $f$ is $\beta$-smooth) and $f$ is $\alpha$-strongly convex.
Then stochastic gradient descent does $x_{t+1}=x_t-\eta\grad f_{i_t}(x_t)$ which runs like $1/\alpha\varepsilon$ (the number of ``data operations'').

In regular gradient descent, $x_{t+1}=x_t-\frac{\eta}{m}\sum_1^m\grad f_i(x_t)$, which runs with $\kappa\log(1/\varepsilon)$ data operations.
The differnce here is that SGD has a lot of variance in its estimators, so to be ``safe'' you have to take smaller steps. On the other hand, GD needs much more data but can run with better accuracy.
So the idea here is to somehow control the variance.

The key idea is that we, from time, to time, compute a full gradient at some point $y$.
\[g_t:=\grad f_{i_t}(x_t)-\grad f_{i_t}+\grad f(y)\]
We can see that $g_t$ is un unbiased estimator of $\grad f_{i_t}(x_t)$ with the added bonus that the variance $\bbE[\|g_t\|^2]$ gets smaller when close to the optimum.
\begin{thm}
	During an epoch $T$, let $x_1\leftarrow y$ and for $t\ge 2$, $x_{t+1}=x_t-\eta g_t$. Then set $y$ to be $\frac{1}{T}\sum_1^Tx_t$ and recompute $\grad f(y)$.
	Then by using $\eta=\frac{1}{10\beta}$,
	\[f(\text{new } y)-f(x^\ast)\le 0.9 (f(y)-f(x^\ast)).\]
\end{thm}

\subsection{Further Topics}
\begin{itemize}
	\item We talked earlier about decomposing functions into smooth and non-smooth (simple) parts. 
	\item Mini-batch SGD: write $x_{t+1}=x_t-\frac{\eta}{|B|}\sum_B\grad f_i(x)$ where $B$ is a batch. Then we get similar results for convergence up to $b=\sqrt{m}$.
	\item Bandit convex optimization
	\item Interior point method: tells you how to handle complicated $K.$
	\item Constant step size least squares: no $1/\alpha$ in the bound.
	\item Mirror descent: the equation $x-\grad f(x)$ doesn't actually make sense (one is a column, one is a row). Instead we pick a function $\phi$ and look at 
	\[\grad\phi^{-1}(\grad\phi(x)-\grad f(x)).\]
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Bandits}
This series was given by Kevin Jamieson, a professor in the CSE department here at UW.

\subsection{Introduction}
The core idea here (motivated very well by his description of the WSJ front page (ads, content, etc) and google maps (that does bandit-like testing to find new good routes) that balances
the exploration (data-gathering) phase and the application phase and the transiton between them. It more resembles a conversation than a one-time transaction.

The name comes from slot machines, obviously. :) We are thinking of these problems as having a slot machine with $n$ ``arms'', which can be pulled to yield some data.
So the input to the algorithm is the number $n$ and for $t=1,2,\dots$, the algorithm pulls some arm $I_t\in[n]$ and nature reveals a corresponding reward $r_t\sim P_{I_t}$ where 
$\bbE[r_t|I_t]=\theta_{I_t}^\ast$.

\subsubsection{Regret Minimization}
We define the \textbf{regret} as follows:
\begin{defn}
	After $T$ timesteps, the regret is 
	\[R_T=\max_i\theta_i^\ast T-\bbE\left[\sum_1^T r_t\right]\]
\end{defn}
Then the goal is to achieve ``sublinear regret'', that is get that $R_T\in\mathbf{o}(T)$.
\begin{lem}
	\begin{align*}
		R_T&=\max_i\theta_i^\ast T-\bbE[\sum_1^T\sum_1^n r_t\delta_{I_t=i}]\\
		&=\max\theta_i^\ast T - \sum_1^n\bbE[\sum+1^T\theta_i^\ast\delta_{I_t=i}]\\
		&=\max\theta_i^\ast T-\sum_1^n\theta_i^\ast\bbE[T_i]\\
		&=\sum_2^n(\theta_1^\ast-\theta_i^\ast)\bbE[T_i]\\
		&=\sum_2^n\Delta_i\bbE[T_i].
	\end{align*}
\end{lem}

\subsection{Another game}
This leads us to the concept of \textbf{best-arm identification}: we fix some $\delta\in(0,1)$ and we identify $\argmax_i\theta_i^\ast$ with probability $1=\delta$ with as few as possible total pulls.
This time we don't put an upper bound on the time you're given, and instead your job is to identify what the best strategy is.

\begin{defn}
	A random variable $X$ is \textbf{$R$-sub-Gaussian} if 
	\[\bbE[\exp(\lambda(x))]\le\exp(\lambda R^2/2)\]
\end{defn}
\begin{rmk}
	Then by Hoeffding, if $X\in[a,b]$  and letting $R^2=\frac{(b-a)^2}{8}$ and notice when $X\sim \calN(0,1)$, we get $R^2=1$.
\end{rmk}

Then we can develop the Chernoff bound: Suppose $X_i$ are IID and $1$-sub-Gaussian. Then 
\begin{align*}
	\bbP(\sum_1^n X_i >\varepsilon)&=\bbP(\exp(\lambda\sum X_i) >\exp(\lambda\varepsilon))\\
	&\le e^{-\lambda\varepsilon}\bbE[\exp(\lambda\sum X_i)]\\
	&= e^{-\lambda\varepsilon}\prod\bbE[e^{\lambda X_i}]\\
	&\le e^{-\lambda\varepsilon}(e^{\lambda^2/2})^n\\
	&\le e^{-n\varepsilon^2/2}
\end{align*}
where the last step ``comes from calculus''. So by letting $\delta=e^{-n\varepsilon^2/2}$, one can solve for epsilon
and get that with probability $1-\delta$, $\frac{1}{n}\sum X_i\le\sqrt{\frac{2\ln 1/\delta}{n}}$.

If $n=2$, say $\theta_1^\ast\theta_2^\ast$ and $\Delta=\theta_1^\ast-\theta_2^\ast$. If both arms are pulled $\tau$ times, let the event
\[\calE_i=\left\{\left|\theta_i^\ast-\frac{1}{\tau}\sum_{t=1}^\tau X_{i,t}\right|\le\sqrt{\frac{2\ln (4/\delta)}{\tau}}\right\}\]
for $i=1,2$ (these are the ``good events''), then $\bbP(\calE_i^C)\le\delta/2$, so 
\[\bbP(\calE_1^C\cup\calE_2^C)\le\sum\bbP(\calE_i^C)\le\delta.\]
Then if $\tau=8\Delta^{-2}\ln(4/\delta)$, we can say 
\[\hat\theta_2-\hat\theta_2=\hat\theta^1-\theta_1^\ast-\hat\theta_2+\theta_2^\ast+\theta_1^\ast-\theta_2^\ast>0\]
with probability $1-\delta$. This of course means that we have accurately determined which arm to pull.

Now consider the regret:
\[R_T=\theta_1^\ast t=\bbE[\theta_1^\ast T_1+\theta_2^\ast T_2]=(\theta_1^\ast-\theta_2^\ast)\bbE[T_2]=\Delta\bbE[T_2]\]
from which we can compute 
\[\Delta\bbE[T_2]=\Delta\bbE[T_2\delta_{\calE_1\cap\calE_2}+T_2\delta_{\calE_1^C\cup \calE_2^C}]\le\Delta\tau+\Delta T\bbP[\calE_1^C\cup\calE_2^C]\]
and so if we set $\delta=\frac{1}{T}$,
\[R_T\le 8\Delta^{-1}\log(8T)+\Delta\]
But notice that if we just always pick the bad arm, the worst regret we can get is $R_T\le \Delta T$. So 
\[R_T\le1+\min{8\Delta^{-1}\log(8T),\Delta T}\le 1+\sqrt{8T\ln(8T)}\]

If you were to graph the minimum function we had there, you'd notice that the regret tends towards zero near zero and towards infinity. The idea here is that if there is only a small difference between the optimal and suboptimal choice, 
then guessing randomly will only incur a tiny regret. Likewise, if the means are very far apart, you will quickly detect this through testing. There is a spike at about $1/\sqrt{T}$ that is the place where both ideas miss the mark.

\subsection{Extending to more Arms}
We are going to start by writing down an algorithm and then trying to analyze it:

\subsubsection{Algorithm}
We input $n$ arms and confidence $\delta$. Initialize $\hat X_1=\{1,\dots,n\}.$ Then for $l=1,2,\dots$ we let $\calE_l=2^{-l}$ and 
\[\tau_l=\left\lceil 2\calE_l^{-2}\ln\frac{4 l^2 n}{\delta}\right\rceil\]
Then pull each arm in $\hat X_l$ $\tau_l$ times and compute $\hat\theta_{i,l}$. Then define 
\[\hat X_{l+1}=\hat X_l\setminus\left\{i\in \hat X_l:\max_{j\in\hat X_l}\hat\theta_{j,l}-\hat\theta_{i,l}>2\varepsilon_l\right\}\]

\subsubsection{Analysis}
Define $\calE_{i,l}=\{|\hat\theta_{i,l}-\theta_i^\ast|\le\varepsilon_l\}$ and let $\calE=\cap_{i,l}\calE_{i,l}$. But then 
\[\bbP(\calE_{i,l}^C)\le 2\bbP(\hat\theta_{i,l}-\theta_i^\ast>\varepsilon_l)\le 2\exp{-\tau_l\varepsilon^2_l/2)}\le\frac{\delta}{2l^2n}\]
and so $\bbP(\calE^c)\le\delta.$

\begin{defn}
	Let $\Delta_i=|\theta_i^\ast-\theta_1^\ast|$ for all $i>1$ and define $\Delta_1=\Delta_2$ (the minimum (nonzero) gap), where we assume for notational simplicity that $1\ge\theta_1^\ast>\theta_2^\ast\ge\cdots\ge\theta_n^\ast\ge 0$.
\end{defn}
\begin{lem}
	With probability greater or equal to $1-\delta$, we have $1\in\hat X_l$ and 
	\[\max_{i\in\hat X_l}\Delta_i\le 8\varepsilon_l\]
	for all $l$.
\end{lem}
\begin{rmk}
	The key thing to notice in the proof here is the fact that for any $j\in\hat X_l$,
	\[\hat\theta_{j,l}-\hat\theta_{1,l}=(\hat\theta_{j,l}-\theta_j^\ast)-(\hat\theta_{1,l}-\theta_1^\ast)-\Delta_j\le 2\varepsilon_l-\Delta_j\le 2\varepsilon_l\]
\end{rmk}

Now let's talk about regret:
\[R_T=\bbE\left[\sum_1^n\Delta_i T_i\right]\]
and so for any $\nu>0$,
\[\sum_1^n\Delta_iT_i\le \nu T+\sum_{l:8\varepsilon_l\ge \nu}8\varepsilon_l\tau_l|\hat X_l|\le \nu T+\sum_{l-1}^{\lceil\lg\max(\Delta,\nu)^{-1}\rceil}8\varepsilon_l\tau_l\delta_{\Delta_i\le 8\varepsilon_l}\]
and then doing some more massaging, we get 
\[\sum\Delta_iT_i\le \nu T+\sum_1^n c(\max(\Delta_i,\nu)^{-1})\log\left(\frac{n\log(\max(\Delta_i,\nu)^{-1})}{\delta}\right)\]

\begin{rmk}
	Notice there are some serious limitations here. In particular, it requires prior knowledge of $T$, which we don't always have.
\end{rmk}

The previous algorithm we described (which one?) we will denote AE1, and we ended up with regret 
\[R_T\le\sqrt{cnT\log(nT)}\]
and 
\[R_T\le c\sum_2^n\Delta_i^{-1}\log(T)\]

One can show that with probability $\ge 1-\delta$, AE1 give us that $\hat X_l=\{1\}$ after at most 
\[\tau=\sum_2^n c\Delta_i^{-2}\log\left(\frac{n\log(\Delta_i^{-2})}{\delta}\right)\]
pulls.

\subsection{Theoretical Bounds}
Then the question one can ask is whether the bounds given are \textit{tight}. A information-theoretic result we will see in the problem session.
\begin{prop}
	Suppose $n$ samples are taken from $\calN(\mu,1)$ where $\mu\in\{0,\Delta\}$. To identify if $\mu=0$ or $\Delta$ with probability $1-\delta$,
	one requires $n\ge 2\Delta^{-2}\log(1/4\delta).$
\end{prop}

This really hints how difficult the multi-arm bandit problem can be. 
\begin{thm}
	Any $\delta$-PAC on $\{P:P_i=\calN(\theta_i,1),\theta\in[0,1]^n\}$ satisfies 
	\[\bbE[\tau|\theta^\ast]\ge 2\log(1/2\delta)\sum_2^n\Delta_i^{-2}\]
\end{thm}
\begin{rmk}
	This is a result on expectation rather than a high-probability bound, obviously, but under these conditions one can turn this into a lower-bound in the high-probability case 
	as well (after losing a big constant factor). Going in the other direction is tricky, because there are bad cases in which this algorithm never terminates or outputs the right thing, so the 
	expectation goes up a bit.
\end{rmk}
\begin{rmk}
	If we take the partial sums $S_T=\sum_{t=1}^T Z_t$ of your random variables, you get the \textit{law of the iterated logarithm}, which 
	says 
	\[\limsup_T\frac{S_T}{\sqrt{2T\log\log T}}=1\]
	so the $\log\log$ factor in the bound we found is actually tight.
\end{rmk}
We also get:
\begin{thm}\label{thm-UCB}
	Any algorithm satisfying $\bbE[T_i]\le\mathbf o(T^\alpha)$ for all $i\ne 1$ (suboptimal samples) any $\alpha\in(0,1)$ suffers 
	\[\lim_{T\to\infty}\frac{R_T}{\log(T)}=\sum_2^n\frac{2}{\Delta_i}\]
\end{thm}
and a worst-case result:
\begin{thm}
	For any $n$ and $T$, there exists an instance ($\theta^\ast$) such that 
	\[R_T\ge\sqrt{(n-1)T}/27.\]
\end{thm}
\subsection{Upper Confidence Bound}
An algorithm that achieves the bound in thm~\ref{thm-UCB} is as follows. For $t=1,2,\dots$,
pull $\argmax_i\hat\theta_{c,T_i(t)}+\sqrt{\frac{2\log(t)}{T_i(t)}}$ where $T_i(t)$ is the number of pulls up to $t$.

This leads to a minimization of regret, but notice that this algorithm is suboptimal on the problem of identifying the best arm. But Kevin 
created an alteration called ``Lil' UCB'' that proceeds similarly but adds in an extra $\frac{1}{\delta}$ factor (as well as some constants) to achieve optimal best arm identification.

These algorithms have this phenomenon where, as time goes on, the likelihood of being selected gradually increases, but then each pull reduces it somewhat. Also notice that this algorithm 
is very much not optimal for an overall ranking since it will stop pulling arms that have a bad run. 

\subsection{Linear Bandits}
Notice that so far we still have a dependence on $n$, but if you are running (e.g.) a music recommendation software, you likely have millions of data points to sample.
Furthermore, songs have tons of features to compare, so we would like to be able to exploit the fact that a person is likely to enjoy music that is ``close'' under some sort of metric (using genre, BPM, etc).

So we develop a \textbf{linear bandits:} For $t=1,2,\dots$, we pull arm $I_t$ and nature reveals $\langle x_{I_t},\theta^\ast\rangle+\xi_t$. 
Here we say $\xi_t\sim\calN(0,1)$. We take as known $X=\{x_1,\dots,x_n\}\subset \bbR^d$ and $\theta^\ast\in\bbR^d$ is unknown.
We define the best arm to be $x^\ast=\argmax_{x\in X}\langle x,\theta^\ast\rangle$.

Say we observe some data $\{(x_i,y_i)\}_1^\tau$ where $y_i=\langle x_i,\theta^\ast\rangle+\xi_i$. Then setting 
\[\hat\theta=\argmin_{\theta\in\bbR^d}\sum_1^\tau(\langle x_i,\theta\rangle-y_i)^2\]
and then if $X=(x_i^T)^T$ and $Y=(y_i)=X\theta^\ast+\xi$, then 
\[\hat\theta=(X^TX)^{-1}X^TY.\]

Then unfolding $Y$, we get 
\[\hat\theta=\theta^\ast+(X^TX)^{-1}X^T\xi\]
thus $\bbE[\hat\theta]=\theta^\ast$. Computing the standard deviation:
\[\bbE[(\hat\theta-\theta^\ast)(\hat\theta-\theta^\ast)]=(X^TX)^{-1}\]
so $\hat\theta-\theta^\ast\sim\calN(0,(X^TX)^{-1})$.

Importantly, notice that the variance of $\hat\theta-\theta^\ast$ depends only on $X$ so we can ``plan ahead'' to control how good we want our estimator $\hat\theta$ to be!

Now notice that for any $z\in\bbR^d$, we have $\langle z,\hat\theta-\theta^\ast\rangle\sim\calN(0,z^T(X^TX)^{-1}z)$. Then if we just call $\sigma^2=z^T(X^TX)^{-1}z$, then 
\[\bbP\left(\langle z,\hat\theta-\theta^\ast\rangle>\sqrt{2\sigma^2\log(1/\delta)}\right)\le\delta.\]
Writing this another way, with probability $1-\delta,$
\[|\langle z,\hat\theta-\theta^\ast\rangle|\le\sqrt{z^T(X^TX)^{-1}z}\cdot\sqrt{2\log(2/\delta)}=\|z\|_{(X^TX)^{-1}}\sqrt{2\log(2/\delta)}\]
where, as a matter of notation, we write $x^TAx=\|x\|_A^2$.

Somehow the idea here is that you may have data lining up along a certain line, in which case your confidence interval along the line in question goes down.

\brk

For all $X$, there exists $\lambda\in\Delta_x$ such that 
\[\tau\sum_{x\in X}\lambda_x xx^T=X^TX\]
and so if we set $A_\lambda=\sum \lambda_xxx^T$, we get a bunch of optimization problems:
\begin{itemize}
	\item E-optimal
	\[\min_\lambda\max_{\|u\|_2\le 1}u^TA^{-1}_\lambda u\]
	\item A-optimal
	\[\min_\lambda\operatorname{Tr}(A_\lambda^{-1})\]
	\item G-optimal
	\[\min_\lambda\max_{x\in X}x^TA^{-1}_\lambda x\]
	\item D-optimal
	\[\max_{\lambda}\log\det(A_\lambda)\]
\end{itemize}
\begin{thm}[Kiefer-Wolfowitz]
	For any $X\in\bbR^d$ (that spans), there is $\lambda^\ast\in\Delta_X$ such that 
	\begin{enumerate}
		\item $\max_\lambda g_D(\lambda)=g_D(\lambda^\ast)$
		\item $\min_\lambda f_G(\lambda)=f_G(\lambda^\ast)$
		\item $\operatorname{support}(\lambda^\ast)\le\frac{(d+1)d}{2}$
		\item $f_G(\lambda^\ast)=g_D(\lambda^\ast)=d$
	\end{enumerate}
	where $g_D(\lambda)=\log\det(A_\lambda)$ and $f_G(\lambda)=\max_{x\in X}x^TA_\lambda^{-1}x.$
\end{thm}

\begin{prop}
	If $\lambda^\ast$ is the $\frac{(d+1)d}2$-sparse $G$-optimal solution and $x$ is pulled $\lceil\lambda_x\tau\rceil$ and we compute $\hat\theta$ (using at most $\frac{(d+1)d}{2}+\tau$ pulls)
	then with probability $\ge 1-\delta$, for any fixed $x\in X$,
	\[\langle x,\hat\theta-\theta^\ast\rangle\le\sqrt{\frac{2d\log(1/\delta)}{\tau}}\]
\end{prop}

\subsection{AE revisited}
Recall AE1: for $t=1,\dots$, $\hat\lambda_l$ is $G$-optimal for $\hat X_l$ and $\varepsilon_l=2^{-l}$ and $\tau_l=2d\varepsilon_l^{-2}\log\left(\frac{4l^2|x|}{\delta}\right).$
Then pull arm $x$ $\lceil\tau_l\hat\lambda_l\rceil$ times and construct $\hat\theta_l$. Then let 
\[\hat X_{l+1}=\hat X_l\setminus\{x\in\hat X_l:\max_{x'\in X}\langle x'-x,\hat\theta_l\rangle>2\varepsilon_l\}\]
then $\bbP(\cup_l\cup_{x\in X}\calE_{x,l}^c)\le\delta$ where $\calE_{x,l}$ is the event that $\langle x,\hat\theta_l-\theta^\ast\rangle\le\varepsilon_l.$

Going through basically the entire argument from before,
\[R_T\le c\sqrt{dT\log(|X|T)}.\]
\begin{rmk}
	Notice that part of what we wanted in all this was to get rid of dependence on the sides of in the inputs, $|X|$. The upshot is, in many (all?) cases, you can just a kinds of $\varepsilon$-cover of your space, which means we can replace it with a factor of $d$.
\end{rmk}

\subsection{Contextual Bandits}
Another game we can play: for $t=1,\dots$ Nature reveals context $c_t\sim D$ (IID). Then the player chooses $x_t\in X$ and nature reveals the reward (with noise)
\[r_t=v(v_t,x_t)+\xi_t\]
where $X$ is known and we know $\bbE[\xi]=0.$

\subsubsection{Linear Contextual Bandits}
One can actually capture this in the linear bandit framework. Assume there exists a feater map $\Phi:X\times X\to \bbR^d$ and $v(c,x)=(\Phi(c,x),\theta^\ast)$ for some unknown $\theta^\ast$.
For example, if $x\in\bbR^p$ and $c\in\bbR^q$, we could have $\Phi(c,x)=\operatorname{vec}(xc^T)\in\bbR^{pq}$.
\begin{rmk}
	Often one uses as $\Phi$ a deep network. One can train a deep network and then take out the last layer and use it to assign features to your data.
\end{rmk}
\subsubsection{General Policies}
A policy is a map $\pi:\calC\to X$, that is it assigns an action to each observed context. Then the \textbf{value} $V(\pi)$ is 
\[V(\pi)=\bbE_{c\sim D}[v(c,\pi(c))+\xi].\]
To define regret, we say 
\[R_T=T\cdot v(\pi^\ast)-\bbE\left[\sum_1^T v(\pi_t)\right]\]
where $\pi^\ast=\argmax_{\pi\in\Pi}v(\pi)$.
\subsubsection{Inverse Propensity Scores}
We define an estimator for $v(\pi)$ no matter what $\pi$ we have. Suppose we have a distribution $\mu(x|c)\in\bbS^{|X|}$. Then we play action $x_t$ distributed as $\mu(x|c_t)$ for all $t$.
Build the estimator $\hat v(c_t,x)=r_t\frac{\delta_{x_t=x}}{\mu(x|c_t)}.$ Then we define the estimator 
\[\hat v_t(\pi)=\frac{1}{t}\sum_{s=1}^t\hat v(c_s,\pi(c_s)).\]

Notice that 
\begin{align*}
	\bbE[\hat v_t(c_t,x)|c_t] &= \sum_{x'\in X}\mu(x'|c_t)\bbE\left[\left.r_t\frac{\delta_{x_t=x}}{\mu(x|c_t)}\right| c_t,x_t=x'\right]\\
	&=\sum_{x'\in X}\mu(x'|c)v(c_t,x')\frac{\delta_{x_t=x}}{\mu(x|c_t)}\\
	&=v(c_t,x)
\end{align*}
So we get that $\hat v(c_t|x)$ is unbiased, but could suffer from very large variance. For instance it is zero unless $x=x_t$!
Then one can show $\bbE[\hat v_t(\pi)]=v(\pi)$.
\begin{rmk}
	To follow this process, you need to record as data $\{(c_t,x_t,r_t,\mu(c_t,x_t))\}$. Most important is the distribution $\mu$! This is a common mistake in practice but is useless without this data.
\end{rmk}

To compute the variance:
\[\bbE[(\hat v_t(c_x,x)-\bbE[\cdot])^2|c_t]\le\frac{1}{\mu(x,c_t)}.\]

Now let $Q\in \bbS^{\pi}$ be a distribution over policies. Then we are going to let 
\[\mu(x|c)=\sum_{\pi\in\Pi:\pi(c)=x}Q(\pi).\]
Then at round $l$, we choose 
\[\hat Q_l=\argmin_{Q\in\bbS^{|\hat\Pi_l|}}\max_{\pi\in\hat\Pi_l}\bbE[(\hat v_t(\pi)-v(\pi))^2]\]
Using this scheme, we can get a regret bound that looks like 
\[R_T\le\sqrt{T|x|\log(|\pi|T)}\]
\subsection{Introduction to Adversarial Bandits}
In this scheme, an adversary chooses $x\in[0,1]^{n\times T}$. Then for $t=1,2,\dots$, the player chooses $I_t\in[n]$, and receives reward $x_{I_t,t}$.
Then we set the regret to be 
\[R_T=\max_{i\in[n]}\sum_1^Tx_{i,t}-\bbE\left[\sum_1^t x_{I_t,t}\right]\]
The twist here is that your adversary will be able to choose $x$ \textit{with full knowledge of your algorithm}. Notice that any deterministic algorithm can be tricked into achieving linear regret. So we need something stochastic.

Let $\hat X_{i,t}=\frac{\delta_{I_t=i}}{P_{i,t}}x_{I_t,t}$ where $I_t\sim P_t\in\bbS^n$. Then $\bbE[\hat X_{i,t}]=x_{i,t}$.

The algorithm we need will be able to deal with the high variance of this estimator. It is called EXP3 (exponential something for exploration and exploitation). Playing this game, you can get 
\[R_T\le\sqrt{nt\log n}.\]

Some textbooks: Seb has one. You can find a preprint on the arXiv. Also check out \href{banditalgs.org}{banditalgs.org}.

\section{Deep Learning}
This talk was given by Joan Bruna from NYU's Courant Institute.
\subsection{The Puzzle}
We are primarily going to be interested in what is called \textit{supervised learning}. Notice that this specifically omits some hot ideas that are currently being developed.
Throughout the talks, we will be considering $\calX$, a \textit{high-dimensional} space. For instance, if we were working in the domain of computer vision, $\calX$ may be the space of all $d\times d$ images,
giving us a huge space, potentially.

The next ingredient we have is the \textbf{data distribution} $\nu$ on $\calX$. Importantly, $U$ is \textit{unknown to us}, so we are not allowed to directly access any of its properties. Next, we have a \textbf{target function}
$f^\ast:\calX\to \bbR$ which we are trying to learn. We also need a loss functional $L(f)=\bbE_\nu[l(f(x),f^\ast(x))].$
\begin{rmk}
	In this class, we will almost always restrict to the case when $L$ is given by 
	\[\|f-f^\ast\|^2_{L^2(\calX,\nu)}\]
\end{rmk}

Then our goal is to predict $f^\ast$ from a finite IID sample $\{(x_i,f(x_i)\}$, where $x_i\sim \nu$.
Notice that this is basically the same as a problem we have all done at some point: fit a curve to a finite number of points. But now the dimensionality has exploded.
\subsubsection{Empirical Risk Minimization}
Consider a set $\calF\subseteq\{f:\calX\to\bbR\}$, which we call the \textbf{hypothesis class}. This space of functions comes along with a notion of \textit{complexity}.
We have a value $\gamma(f)$ for each $f\in\calF$, which we call the \textbf{complexity of $f$.} One should think of this like a norm. The upshot here is this gives us a way to ``organize'' $\calF$ in a nice way.

A natural way to proceed is to begin with the norm zero piece and gradually increase to consider more candidate functions.
\begin{rmk}
	Careful! Notice that assuming that $\gamma$ is honestly a norm means that such ``balls'' are already convex! In general this will not be true.
\end{rmk}
\begin{defn}
	The \textbf{empirical risk} of a function $f$ is 
	\[\hat L(f)=\frac{1}{n}\sum_1^n|f(x_i)-f^\ast(x_i)|^2.\]
\end{defn}

Now if one wants to minimize $\hat L$, there are a couple different ways to do this. The first idea is consider the \textbf{constrained form}:
\[\min_{f\in\calF^\delta}\hat L(f)\]
where $\calF^\delta=\{f\in\calF:\lambda(f)\le\delta\}$. This requires a bit more prior knowledge than we generally have, so we sometimes use instead a \textbf{penalized form}:
\[\min_{f\in\calF}\hat L(f)+\lambda\cdot\gamma(f).\]
Another idea is called the \textbf{interpolant}:
\[\min_{f\in\calF} \gamma(f) \text{  such that  }\hat L(f)=0\]

\subsubsection{The Fundamental Theorem of Machine Learning}
Let us assume for this formulation that we are focusing on the constrained form. This was found in the seminal paper from 2008 by Bolton and Bousquet.
\begin{thm}[FtoML]
	Assume that we have $\hat f\in\calF^\delta$ such that $\hat L(\hat f)\le \varepsilon+\inf_{f\in\calF^\delta}\hat L(f).$ Then 
	\[L(\hat f)-\inf_{f\in \calF}L(f)\le \inf_{f\in\calF^\delta}L(f)-\inf_{f\in\calF}L(f)+2\sup_{f\in\calF^\delta}|L(f)-\hat L(f)|+\varepsilon.\]
\end{thm}
\begin{rmk}
	Here the first two terms on the right constitute an approximation error, the third term is a statistical error, and the $\varepsilon$ comes from our optimization error. 

	Notice that we have a free parameter $\delta$. If $\delta$ is small, then our approximation error gets very bad and the statistical error drops (we are working on a ``smaller'' set in terms of complexity). On the other hand, if $\delta$ is very big, the statistical error blows up.
\end{rmk}
\subsection{The Big Questions}
We have a series of questions we want to answer with this theory:
\begin{itemize}
	\item \textbf{[Approx]} How can we design ``good'' spaces $\calF$ to approximate $f^\ast$ in higher dimensions?
	\item \textbf{[Optimization]} How to design algorithms to solve the problem of empirical risk management in general (gradient descent).
\end{itemize}

\subsubsection{The Curse of Dimensionality}
To begin to convince ourselves that dimensionality is troubling, consider the question: How many samples do I need to estimate $f^\ast$ 
depending on its regularity assumptions?

If we assume that $f^\ast$ is \textit{linear}, then $f^\ast(x)=\langle x,\theta^\ast\rangle$ for some $\theta^\ast\in\bbR^d$. Then $\calF$ is the set of all such functions and we need $d$ samples to get enough equations to solve the corresponding linear equations.

Now assume that $f^\ast$ is ``locally linear'', that is \textit{Lipschitz.} That is, there exists a (minimal) $\beta$ such that 
\[|f^\ast(x)-f^\ast(y)|\le\beta\|x-y\|.\]
Then $\calF$ is the space of all Lipschitz functions. Then $\gamma(f)$ is determined by the Lipschitz constant as well as $|f|_\infty$. Now for all $\varepsilon>0$, we want to find $f\in\calF$ such that $\|f-f^\ast\|\le\varepsilon$ 
from $n$ IID samples. Then in this case, the sample complexity blows up! To get an $\varepsilon$-error, we need $n\sim \varepsilon^{-d}$ samples!

Let's build the upper bound here: given a set of points $(x_i,f^\ast(x_i))$, we want to find 
\[\hat f=\argmin_f\operatorname{Lip}(f)\text{  such that  }f(x_i)=f^\ast(x_i)\]
where $\operatorname{Lip}(f)=\min_\beta |f(x)-f(y)|\le\beta\|x-y\|$. To do this, we can, for any $x$, pick the closest data point $x_k$ to $x$ and notice 
\[|\hat f(x)-f^\ast(x)|\le|\hat f(x)-\hat f(x_k)|+|\hat f(x_k)-f^\ast(x_k)|+|f^\ast(x_k)-f^\ast(x)|\le2\operatorname{Lip}(f^\ast)\|x-x_k\|.\]

But now 
\[\bbE_{x\sim\nu}|\hat f(x)-f^\ast(x)|^2\le\bbE_{x\sim\nu}\|x-x_k\|^2\approx\varepsilon^2.\]

\begin{defn}
	The \textbf{maximum discrepancy} of a function class $\calF$ is 
	\[\tilde R_n(\calF)=\sup_{f\in\calF}\frac{1}{2n}\sum_1^{n/2}f(x_i)-\sum_{n/2+1}^n f(x_i))\]
\end{defn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Reinforcement Learning}
This series of lectures was given by Emma Brunskill from Stanford's CS department.

\subsection{Introduction}
There is a wide set of backgrounds in this class will start broad and get more focused towards the end. One can think of reinforcement learning as being at the intersection of supervised learning and bandits (and beyond). In a sentence, 
reinforcement learning is ``learning to make good sequences of decisions under uncertainty.''

\subsubsection{Examples of things you can do}
The main applications arise in a couple different domains: learning to play games (e.g. Atari games), teaching robots tasks, and in healthcare.
There are a plethora of applications we will hear about more.

\subsubsection{Tasks}
There are four main aspects:
\begin{itemize}
	\item Optimization
	\item Generalization
	\item Exploration 
	\item Delayed consequences
\end{itemize}

In general \textbf{AI learning} uses all of these except exploration. Although RL was used in solving go, it wasn't needed.

\textbf{Deep learning} only exploits optimization and generalization. There is also a sense of \textbf{imitation learning}, which also omits exploration.

We will be following the following (rough) plan:
\begin{enumerate}
	\item Markov decision processes
	\item Q-learning
	\item RL and generalization
	\item Exploration in sequential domains 
	\item Batch/counterfactual reinforcement learning
\end{enumerate}

\subsection{Markov Decision Processes}
The big idea here is that two entities: the agent and the world. The agent takes an action and the world offers a reward. In the case of bandits, this is where the scenario stops,
but here in RL, we continue the feedback loop. Our goal here is to maximize the expected discounted sum of rewards.

Some ideas that demonstrate the delayed consequences are as follows: imagine teaching a robot to empty the dishwasher---it only receives a positive reward once its task is complete. 

Now one of the hardest aspects here is coming up with a \textbf{reward function}. That is, a real-valued map $r(s,a)$ given a state and action. Why is this hard?
Well, imagine you are trying to teach students math and the fact of the matter is that teaching addition is easier than subtraction, and we design our reward to be +1 if the student is correct and -1 if incorrect.
Then the machine is naturally going to give only additon questions (and infinitely many of them). This is an example of \textit{reward hacking.}

A problem follows \textbf{Markov dynamics} if 
\[\bbP(s_t|s_{t-1},a_{t-1},s_{t-2},a_{t-2},\cdots)=\bbP(s_t|s_{t-1},a_{t-1}).\]
\begin{defn}
	A \textbf{Markov decision process} is a set $\langle S,A,R,T,\gamma\rangle$ where 
	\begin{itemize}
		\item $S$ is a set of states,
		\item $A$ is a discrete action space,
		\item $R$ is a set of rules of the form $r(s,a)$ or $r(s)$ or $r(s,a,s')$,
		\item $T$ is a collection of distributions $\bbP(s'|s,a)$ and $\gamma\in(0,1)$ is a parameter.
	\end{itemize}

	Then we are trying to learn a \textbf{policy} $\pi:S\to A$. Note this could also be stochastic.
\end{defn}

There are a couple ways to do this: first, is the \textbf{model-based approach}. Here we directly estimate and use $R$ and $T$. Another is \textbf{value-based:}
here we consider the value function 
\[V^\pi=\bbE_{s'\sim pi}\left[\sum_0^\infty\gamma^tr_t|s_{t=0}=s_0\right].\]
Finally we can perform a \textbf{policy-based approach} by computing $\argmax_{\pi\in\Pi}V^\pi$.

Another parameter we have is $H$, the \textbf{time horizon} on the process. This tells us the number of steps that we may take before the system is reset to $s_0$.

Then this process will return
\[G^\pi=r_{0}+\gamma r_{1}+\gamma^2r_2+\cdots+\gamma^{H-1}r_H.\]
Notice that in general this is going to be a random variable! That;'s why $V^\pi(s_0)=\bbE[G^\pi|s_0]$.

\subsubsection{Estimation}
Now say we know all the parameters of our MDP. Then we can use $S$ as a lookup table to approximate $V^\pi$ by ``running simulations.'' That is, we essentially 
have a black box that outputs specific measurements of $G$. We don't need the Markov assumption here since we aren't touching the distributions directly.

Using the Markov assumption yields additional structure, however. Consider that 
\[V^\pi(s)= r(s,\pi(s))+\gamma\sum_{s'}p(s|s,\pi(s))V^\pi(s')\]
giving us our immediate and discounted future rewards.

Now let $V^\ast=\max_{\pi}V^\pi$ and $\pi^\ast=\argmax_\pi V^\pi$. But then using the above equation,
\[V^\ast(s)=\max_a r(s,a)+\gamma_{s'}p(s'|s,a)V^\ast(s')\]
which induces a kind of dynamical programming structure on the problem. This is called the \textbf{Bellman equation.}

Then we proceed by a process called \textbf{value iteration}: let $V_0(s)=0$ for all $s$. Then for all $k$, set 
\[V_k(s)=\max_a r(s,a)+\gamma\sum_{s'}p(s'|s,a)V_{k-1}(s')\]
Doing this is called a ``Bellman backup.'' Then we break once you converge: $\|V_k-V_{k-1}\|_\infty\le\varepsilon$.

\begin{lem}
	Assuming all rewards are valued in $[0,1]$, the Bellman backup is a contraction operator as long as the state and action spaces are discrete (i.e. the above process converges).
\end{lem}
\begin{prf}
	Throughout we will be considering $\|V-V'\|_\infty=\max_s|V(s)-V'(s)|$. So we compute 
	\begin{align*}
		\|BV_k-BV_j\|&=\|\max_a(r(s,a)+\gamma\sum_{s'}p(s'|s,a)V_{k-1}(s'))-\max_{a'}(r(s,a')\\
		&\qquad\qquad\qquad\quad +\gamma\sum_{s'}p(s'|a',s)V_{j-1}s)\|\\
		&\le\|\max_a \gamma\sum p(s'|s,a)\|V_k-V_j\|\\
		&\le \gamma\|V_k-V_j\|
	\end{align*}
	so as long as $\gamma<1$, our functions have gotten closer.
\end{prf}
\begin{rmk}
	Notice this also implies that $V^\ast$ must be unique.
\end{rmk}

\subsubsection{MDP Policy Iteration}
Value iteration is great, but we have to consider the set of functions $A^S$, exponential in the set of states. Instead we can do \textbf{Policy Iteration}:
Set $i=0$ and initialize $\pi_0(s)$ randomly for all states $s$. Then while $i=0$, or $\|\pi_i-\pi_{i-1}\|_1\>0$, set $V^{\pi_i}$, the MDP
value function policy evaluation of $\pi_i$ and let $\pi_{i+1}$ be the policy improvement.

Note that we can find an analytic solution whenever you have a finite set of states and actions, otherwise you have to do the dynamic programming-type argument using the Bellman operator.
\begin{defn}
	The $Q$ value of a policy is 
	\[Q^\pi(s,a)=r(s,a)+\gamma+\sum_{s'} p(s'|s,a)V^\pi(S')\]
\end{defn}
so now that we have computed $V^{\pi_i}$ using analytical or dynamic programming methods, this is a relatively simple step. Then we just pick
\[\pi_{i+1}(s)=\argmax_aQ^{\pi_i}(s,a).\]

\begin{rmk}
	It ends up that $V^{\pi_{i+1}}\ge V^{\pi_i}$ with strict inequality whenever $\pi_i$ is suboptimal. Notice that this is not trivial! We are getting better optimization over all time, 
	but since we are switching policies greedily at each step, there is something to prove.
\end{rmk}

\subsection{Reinforcement Learning and Q-learning}
Now we are performing reinforcement learning in a contex where $R$ and $T$ are unknown, but we are still interested in finding $V^\ast$ and $\pi^\ast$.

\subsubsection{Policy Evaluation}
Here we are given $\pi$ and want to compute $V^\pi$ and $Q^\pi$.

If \textbf{our horizon $H$ is finite}, then we can do \textbf{Monte Carlo Evaluation}. If we let (again)
\[G^i=r_1+\gamma r^2+\cdots\]
where $s,r\sim \pi$ starting with $s_0$. Then the value is 
\[V^\pi(s_0)=\bbE_{s\sim \pi}[G|s_0]\]
where the above does not require the Markov assumption and doesn't use the ``bootstrapping'' of MDP value iteration (where make incremental improvements to an imperfect hypothesis).
Then the average of all runs acts as our estimator. This is unbiased but has high variance (although it converges by the law of large numbers).

\subsubsection{Temporal Difference Learning}
This is called ``model free'' in the sense that we are not directly estimating the dynamics and reward models. This combined ideas in Monte carlo and dynamic programming.
It is episodic (finite horizon) or infinite $H$. Then each tuple $(s,a,r,s')$ can update $\hat V^\pi$ (instead of having to wait for an entire run).

The MC estimator updates at each $s_0$:
\[\hat V(s_0)+\alpha(G^i-\hat V^\ast(s_0))\]
where $\alpha\in(0,1)$ is a learning rate. Then the idea we do here is 
\[\hat V_{k+1}(s)=r(s,\pi(s))+\gamma\hat V_k(s')\]
Notice that this uses bootstrapping  of $\hat V$ as well as doing a sampling MC to get the data to update with.
This new estimator will be biased, but wil have lower variance of what we did before.

So overall the TD learning we are doing looks like what follows:
\[\hat V^\pi(s)=\hat V^\pi(s)+\alpha\left[r+\gamma\hat V^\pi(s')-\hat V^\pi(s)\right]\]
where $r+\gamma\hat V^\pi(s')$ is called the \textbf{TD target} and this quantity minus $\hat V^\pi(s)$ is called the \textbf{TD error.}

For mean estimation, let $X\in[0,1]$ is a random variable and let 
\begin{thm}
	Assume that $\alpha_n\in[0,1]$ and $\sum \alpha_i=\infty$ and $\sum \alpha_i^2<\infty$, and let $X_0,x_1,\dots$ be iid samples of $X$. THen define $\mu_n$ for each $n\in\bbN$ such that 
	\[\mu_0=X_0,\quad \mu_{m+1}=(1-\alpha_m)\mu_m+\alpha_mx_m\]
	then $\mu_m$ almost surely approaches $\bbE[X]$.
\end{thm}
\begin{rmk}
	I can't really read and my probability isn't strong enough to figure out what she meant.
\end{rmk}

\subsubsection{Control}
Compute $\pi^\ast$ and $V^\ast$.

\subsection{Reinforcement Learning and Generalization}

\subsection{Exploration in Sequential Domains}

\subsection{Batch/Counterfactual RL}

\end{document}